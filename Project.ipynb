{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogcazglff3Uq",
        "outputId": "d3bf4e3e-758c-42e0-e257-51e59cd3ea18",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4_1EpCXgRk-"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler() \n",
        "#scaler.fit(X_train)\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/MyDrive/ColabNotebooks/dataset/Actor_01/03-01-01-01-01-01-01.wav')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "1x2xEKTZjiVz",
        "outputId": "bc50b996-79cf-441c-ebda-53f6a8f94e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f0e586a1c90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcfERSSIf3Uu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "dd4aae49-af0c-44b0-8066-efb57d4f40ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f70e3fb4a443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaiser_fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#mfccs extracted per file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_open_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/ColabNotebooks/dataset'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) #mfccs extracted per file\n",
        "        file = file[6:8] #extract features\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpKjDAZf3Uv"
      },
      "outputs": [],
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)\n",
        "\n",
        "#X = mfccs\n",
        "#y = labels;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLXdKnBff3Uv",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559a1eea-b4a6-45d0-a64b-67a6e39d4b6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2913, 40), (2913,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVeHi5covfSK"
      },
      "outputs": [],
      "source": [
        "#preprocessing, basically preprocessing means that the datas are formatted and processed in a way that they will become usefull for the machine learning model,\n",
        "#also part of the preprocessing consists in \"garbage in, garbage out\" a good sample of data is better than having bad datas even if the model it's the best that we can find\n",
        "#in this case we don't need to garbage out anyhting.\n",
        "#The process of transforming numerical features to use the same scale is known as feature scaling. It’s an important data preprocessing step for most distance-based machine learning algorithms because it can have a significant impact on the performance of your algorithm.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler() \n",
        "scaler.fit(X)\n",
        "#Standardize features, scaling to unit variance\n",
        "X = scaler.transform(X) #preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfJ2ILj0f3Ux"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #splitting for training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohHCIDBhp2L5"
      },
      "source": [
        "# K-MEANS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JluWN1o_0ZsR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CY0-vJOm4EG",
        "outputId": "6e4cfa51-73a7-4f4a-e51f-de46daf8607a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.09      0.20      0.12        56\n",
            "           2       0.22      0.31      0.26       130\n",
            "           3       0.26      0.12      0.16       137\n",
            "           4       0.14      0.16      0.15       121\n",
            "           5       0.24      0.24      0.24       125\n",
            "           6       0.09      0.12      0.11       120\n",
            "           7       0.14      0.09      0.11       136\n",
            "           8       0.00      0.00      0.00       137\n",
            "\n",
            "    accuracy                           0.15       962\n",
            "   macro avg       0.13      0.14      0.13       962\n",
            "weighted avg       0.15      0.15      0.14       962\n",
            "\n",
            "[[ 0  0  0  0  0  0  0  0  0]\n",
            " [ 0 11 16  0 12  0  9  8  0]\n",
            " [ 0 30 40  0 23  0 22 15  0]\n",
            " [14 15 19 16 15 20 29  9  0]\n",
            " [ 5 14 34  8 19 10 11 20  0]\n",
            " [28 14  6  6 11 30 23  7  0]\n",
            " [17  2 19 27  9 25 15  6  0]\n",
            " [ 5 25 26  1 28 13 26 12  0]\n",
            " [26 16 18  3 17 25 25  7  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#provare a fare clusteing es k-means, l'ha consigliato il prof.\n",
        "#There should be 8 classes in this dataset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "KM = KMeans(\n",
        "    init = \"random\",\n",
        "    n_clusters = 8,\n",
        "    n_init = 8,\n",
        "    )\n",
        "KM.fit(X_train)\n",
        "y_predicted = KM.predict(X_test)\n",
        "\n",
        "y_test_intConversion = np.array([int(x) for x in y_test])\n",
        "\n",
        "print(classification_report(y_test_intConversion, y_predicted))\n",
        "print(confusion_matrix(y_test_intConversion, y_predicted))\n",
        "\n",
        "#print(y_predicted[:20])\n",
        "#print(y_test_intConversion[:20])\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
        "\n",
        "#The warning means that the classifier cannot predict some labels e.g.:0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HxLs-lyzrNv"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaqpW6F4xic3",
        "outputId": "3631f687-8c91-46c0-b787-f4c6ef02f1f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors= 5) #value for k = 5\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svwbEXodx9tJ",
        "outputId": "9174d8a5-1ee7-4e86-e2f7-804943afbba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.42      0.86      0.56        56\n",
            "          02       0.73      0.72      0.72       130\n",
            "          03       0.67      0.68      0.68       137\n",
            "          04       0.68      0.54      0.60       121\n",
            "          05       0.71      0.69      0.70       125\n",
            "          06       0.70      0.64      0.67       120\n",
            "          07       0.87      0.63      0.73       136\n",
            "          08       0.72      0.82      0.76       137\n",
            "\n",
            "    accuracy                           0.69       962\n",
            "   macro avg       0.69      0.70      0.68       962\n",
            "weighted avg       0.71      0.69      0.69       962\n",
            "\n",
            "[[ 48   7   1   0   0   0   0   0]\n",
            " [ 30  93   2   3   0   0   0   2]\n",
            " [  6   1  93   5   5  14   2  11]\n",
            " [ 14  13   9  65   2   6   3   9]\n",
            " [  2   6   7   2  86   2   5  15]\n",
            " [  3   4  13  14   9  77   0   0]\n",
            " [  7   3   6   5  16   6  86   7]\n",
            " [  5   0   7   2   3   5   3 112]]\n"
          ]
        }
      ],
      "source": [
        "#make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-expDVaf3Uv"
      },
      "source": [
        "# Decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wziztZubf3Ux"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk_FCWbZf3Uy"
      },
      "outputs": [],
      "source": [
        "dtree = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13zoYq9Rf3Uy",
        "outputId": "519d4bd4-5cf5-497e-c503-921dfbd7afc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ntvAlVZf3Uz"
      },
      "outputs": [],
      "source": [
        "predictions = dtree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9ONfvrif3Uz",
        "outputId": "462e5a91-7d62-4425-ccbc-fbda772bf17e",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      0.82      0.77        56\n",
            "           2       0.83      0.75      0.79       130\n",
            "           3       0.85      0.82      0.84       137\n",
            "           4       0.76      0.82      0.79       121\n",
            "           5       0.88      0.81      0.84       125\n",
            "           6       0.88      0.85      0.86       120\n",
            "           7       0.73      0.72      0.73       136\n",
            "           8       0.76      0.84      0.80       137\n",
            "\n",
            "    accuracy                           0.80       962\n",
            "   macro avg       0.80      0.80      0.80       962\n",
            "weighted avg       0.81      0.80      0.80       962\n",
            "\n",
            "[[ 46   0   2   6   0   0   2   0]\n",
            " [  8  98   4   8   0   2   2   8]\n",
            " [  2   4 113   2   0   2   6   8]\n",
            " [  0   8   0  99   0   0  10   4]\n",
            " [  2   2   4   2 101   4   8   2]\n",
            " [  0   2   4   4   6 102   2   0]\n",
            " [  2   0   4  10   8   0  98  14]\n",
            " [  4   4   2   0   0   6   6 115]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "text_representation = tree.export_text(dtree)\n",
        "print(text_representation)\n",
        "#text rappresentation of the tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8FS1bZPaDb-",
        "outputId": "58273ed4-a99c-4a1a-c871-4a2f913ffc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|--- feature_0 <= -0.39\n",
            "|   |--- feature_0 <= -0.86\n",
            "|   |   |--- feature_37 <= 0.83\n",
            "|   |   |   |--- feature_9 <= 1.25\n",
            "|   |   |   |   |--- feature_8 <= 0.79\n",
            "|   |   |   |   |   |--- feature_27 <= -0.24\n",
            "|   |   |   |   |   |   |--- feature_10 <= 0.90\n",
            "|   |   |   |   |   |   |   |--- feature_32 <= -0.16\n",
            "|   |   |   |   |   |   |   |   |--- feature_8 <= 0.67\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.64\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.69\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.69\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 >  0.64\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_27 <= -1.09\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_27 >  -1.09\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |--- feature_8 >  0.67\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_28 <= -0.94\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_28 >  -0.94\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_32 >  -0.16\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 <= -0.08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 <= 2.05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_34 <= 0.64\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_34 >  0.64\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 >  2.05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 >  -0.08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_27 <= -1.18\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_27 >  -1.18\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_10 >  0.90\n",
            "|   |   |   |   |   |   |   |--- feature_34 <= -0.50\n",
            "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.54\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 1.19\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 >  1.19\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |--- feature_1 >  0.54\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_34 >  -0.50\n",
            "|   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |--- feature_27 >  -0.24\n",
            "|   |   |   |   |   |   |--- feature_6 <= -0.95\n",
            "|   |   |   |   |   |   |   |--- feature_19 <= -1.03\n",
            "|   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |--- feature_19 >  -1.03\n",
            "|   |   |   |   |   |   |   |   |--- feature_9 <= -0.80\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_9 >  -0.80\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_6 >  -0.95\n",
            "|   |   |   |   |   |   |   |--- feature_28 <= 0.40\n",
            "|   |   |   |   |   |   |   |   |--- feature_17 <= -1.37\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_17 >  -1.37\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_26 <= 0.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= -0.59\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  -0.59\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_26 >  0.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= -0.09\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  -0.09\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |--- feature_28 >  0.40\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 <= 0.92\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_12 <= -1.37\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_12 >  -1.37\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 >  0.92\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_25 <= 1.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_37 <= -0.25\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_37 >  -0.25\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_25 >  1.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |--- feature_8 >  0.79\n",
            "|   |   |   |   |   |--- feature_29 <= -0.55\n",
            "|   |   |   |   |   |   |--- feature_11 <= 0.89\n",
            "|   |   |   |   |   |   |   |--- feature_19 <= 0.70\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 <= 1.73\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_37 <= -0.57\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_37 >  -0.57\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 >  1.73\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_19 >  0.70\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 <= -0.75\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 >  -0.75\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |--- feature_11 >  0.89\n",
            "|   |   |   |   |   |   |   |--- feature_6 <= 1.04\n",
            "|   |   |   |   |   |   |   |   |--- feature_13 <= 0.15\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_39 <= -1.14\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_39 >  -1.14\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_13 >  0.15\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 <= -0.59\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_18 <= -0.20\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_18 >  -0.20\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 >  -0.59\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |--- feature_6 >  1.04\n",
            "|   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |--- feature_29 >  -0.55\n",
            "|   |   |   |   |   |   |--- feature_10 <= -0.41\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_10 >  -0.41\n",
            "|   |   |   |   |   |   |   |--- feature_24 <= -0.42\n",
            "|   |   |   |   |   |   |   |   |--- feature_26 <= -0.31\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |--- feature_26 >  -0.31\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 <= -0.20\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 >  -0.20\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |--- feature_24 >  -0.42\n",
            "|   |   |   |   |   |   |   |   |--- feature_26 <= 0.17\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 <= 1.78\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_24 <= -0.40\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_24 >  -0.40\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 >  1.78\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_12 <= 0.62\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_12 >  0.62\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |--- feature_26 >  0.17\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_36 <= 0.43\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 <= 0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 >  0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_36 >  0.43\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |--- feature_9 >  1.25\n",
            "|   |   |   |   |--- feature_30 <= 0.09\n",
            "|   |   |   |   |   |--- feature_25 <= -0.49\n",
            "|   |   |   |   |   |   |--- feature_3 <= 1.59\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_3 >  1.59\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |--- feature_25 >  -0.49\n",
            "|   |   |   |   |   |   |--- feature_29 <= 0.05\n",
            "|   |   |   |   |   |   |   |--- feature_24 <= -0.80\n",
            "|   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |--- feature_24 >  -0.80\n",
            "|   |   |   |   |   |   |   |   |--- feature_11 <= 0.00\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |--- feature_11 >  0.00\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 <= -1.66\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 >  -1.66\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.96\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.96\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_29 >  0.05\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |--- feature_30 >  0.09\n",
            "|   |   |   |   |   |--- feature_16 <= 1.07\n",
            "|   |   |   |   |   |   |--- feature_34 <= 0.16\n",
            "|   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |--- feature_34 >  0.16\n",
            "|   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |--- feature_16 >  1.07\n",
            "|   |   |   |   |   |   |--- feature_5 <= 1.93\n",
            "|   |   |   |   |   |   |   |--- feature_34 <= -0.11\n",
            "|   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |--- feature_34 >  -0.11\n",
            "|   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |--- feature_5 >  1.93\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |--- feature_37 >  0.83\n",
            "|   |   |   |--- feature_11 <= 0.04\n",
            "|   |   |   |   |--- feature_31 <= -0.75\n",
            "|   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |--- feature_31 >  -0.75\n",
            "|   |   |   |   |   |--- feature_27 <= 0.35\n",
            "|   |   |   |   |   |   |--- feature_20 <= -1.26\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_20 >  -1.26\n",
            "|   |   |   |   |   |   |   |--- feature_31 <= -0.02\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 <= -0.22\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 >  -0.22\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_31 >  -0.02\n",
            "|   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |--- feature_27 >  0.35\n",
            "|   |   |   |   |   |   |--- feature_30 <= 0.32\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_30 >  0.32\n",
            "|   |   |   |   |   |   |   |--- feature_34 <= 0.25\n",
            "|   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_34 >  0.25\n",
            "|   |   |   |   |   |   |   |   |--- feature_28 <= 0.96\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |--- feature_28 >  0.96\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |--- feature_11 >  0.04\n",
            "|   |   |   |   |--- feature_0 <= -0.95\n",
            "|   |   |   |   |   |--- feature_8 <= 1.10\n",
            "|   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |--- feature_8 >  1.10\n",
            "|   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |--- feature_0 >  -0.95\n",
            "|   |   |   |   |   |--- feature_30 <= 1.82\n",
            "|   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |--- feature_30 >  1.82\n",
            "|   |   |   |   |   |   |--- class: 08\n",
            "|   |--- feature_0 >  -0.86\n",
            "|   |   |--- feature_39 <= 0.66\n",
            "|   |   |   |--- feature_22 <= -0.06\n",
            "|   |   |   |   |--- feature_21 <= 0.38\n",
            "|   |   |   |   |   |--- feature_32 <= 0.19\n",
            "|   |   |   |   |   |   |--- feature_10 <= 1.03\n",
            "|   |   |   |   |   |   |   |--- feature_16 <= -0.92\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 <= 0.46\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_18 <= -1.17\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= -0.85\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  -0.85\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_18 >  -1.17\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 >  0.46\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_6 <= -0.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_39 <= -0.91\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_39 >  -0.91\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_6 >  -0.04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_16 >  -0.92\n",
            "|   |   |   |   |   |   |   |   |--- feature_6 <= 0.76\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_20 <= -0.15\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 <= -0.75\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 >  -0.75\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_20 >  -0.15\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_36 <= -0.58\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_36 >  -0.58\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |--- feature_6 >  0.76\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |--- feature_10 >  1.03\n",
            "|   |   |   |   |   |   |   |--- feature_33 <= -0.78\n",
            "|   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_33 >  -0.78\n",
            "|   |   |   |   |   |   |   |   |--- feature_32 <= -0.08\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_28 <= -0.38\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 <= 1.59\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 >  1.59\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_28 >  -0.38\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |--- feature_32 >  -0.08\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |--- feature_32 >  0.19\n",
            "|   |   |   |   |   |   |--- feature_38 <= 0.21\n",
            "|   |   |   |   |   |   |   |--- feature_30 <= 0.87\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 <= 0.74\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 >  0.74\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_30 >  0.87\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 <= -0.99\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 >  -0.99\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |--- feature_38 >  0.21\n",
            "|   |   |   |   |   |   |   |--- feature_21 <= -0.20\n",
            "|   |   |   |   |   |   |   |   |--- feature_16 <= 0.18\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_16 >  0.18\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |--- feature_21 >  -0.20\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |--- feature_21 >  0.38\n",
            "|   |   |   |   |   |--- feature_34 <= -0.53\n",
            "|   |   |   |   |   |   |--- feature_19 <= 0.98\n",
            "|   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |--- feature_19 >  0.98\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |--- feature_34 >  -0.53\n",
            "|   |   |   |   |   |   |--- feature_6 <= 1.45\n",
            "|   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |--- feature_6 >  1.45\n",
            "|   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |--- feature_22 >  -0.06\n",
            "|   |   |   |   |--- feature_10 <= -0.71\n",
            "|   |   |   |   |   |--- feature_17 <= -1.03\n",
            "|   |   |   |   |   |   |--- feature_28 <= 1.52\n",
            "|   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |--- feature_28 >  1.52\n",
            "|   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |--- feature_17 >  -1.03\n",
            "|   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |--- feature_10 >  -0.71\n",
            "|   |   |   |   |   |--- feature_27 <= -0.40\n",
            "|   |   |   |   |   |   |--- feature_17 <= -0.19\n",
            "|   |   |   |   |   |   |   |--- feature_33 <= -1.12\n",
            "|   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |--- feature_33 >  -1.12\n",
            "|   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_17 >  -0.19\n",
            "|   |   |   |   |   |   |   |--- feature_39 <= -1.35\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_39 >  -1.35\n",
            "|   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |--- feature_27 >  -0.40\n",
            "|   |   |   |   |   |   |--- feature_39 <= -0.40\n",
            "|   |   |   |   |   |   |   |--- feature_30 <= -0.35\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 <= -0.05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_0 <= -0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_0 >  -0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 >  -0.05\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_30 >  -0.35\n",
            "|   |   |   |   |   |   |   |   |--- feature_25 <= -0.13\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |--- feature_25 >  -0.13\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.19\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.98\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.98\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.19\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |--- feature_39 >  -0.40\n",
            "|   |   |   |   |   |   |   |--- feature_34 <= -0.71\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_34 >  -0.71\n",
            "|   |   |   |   |   |   |   |   |--- feature_18 <= -0.84\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_18 >  -0.84\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 <= 1.41\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_12 <= 1.55\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_12 >  1.55\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 >  1.41\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |--- feature_39 >  0.66\n",
            "|   |   |   |--- feature_33 <= -0.39\n",
            "|   |   |   |   |--- feature_36 <= 0.76\n",
            "|   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |--- feature_36 >  0.76\n",
            "|   |   |   |   |   |--- class: 04\n",
            "|   |   |   |--- feature_33 >  -0.39\n",
            "|   |   |   |   |--- feature_25 <= 0.07\n",
            "|   |   |   |   |   |--- feature_23 <= -1.07\n",
            "|   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |--- feature_23 >  -1.07\n",
            "|   |   |   |   |   |   |--- feature_11 <= 0.21\n",
            "|   |   |   |   |   |   |   |--- feature_5 <= -0.17\n",
            "|   |   |   |   |   |   |   |   |--- feature_31 <= -0.56\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |--- feature_31 >  -0.56\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |--- feature_5 >  -0.17\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 <= -0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_17 <= -0.89\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_17 >  -0.89\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |   |--- feature_12 >  -0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |--- feature_11 >  0.21\n",
            "|   |   |   |   |   |   |   |--- feature_14 <= 0.52\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 <= 0.79\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |--- feature_10 >  0.79\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |   |--- feature_14 >  0.52\n",
            "|   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |--- feature_25 >  0.07\n",
            "|   |   |   |   |   |--- feature_22 <= -0.49\n",
            "|   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |--- feature_22 >  -0.49\n",
            "|   |   |   |   |   |   |--- feature_36 <= 0.91\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_36 >  0.91\n",
            "|   |   |   |   |   |   |   |--- feature_12 <= 0.48\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_12 >  0.48\n",
            "|   |   |   |   |   |   |   |   |--- class: 06\n",
            "|--- feature_0 >  -0.39\n",
            "|   |--- feature_0 <= 1.36\n",
            "|   |   |--- feature_20 <= 2.12\n",
            "|   |   |   |--- feature_4 <= 0.64\n",
            "|   |   |   |   |--- feature_14 <= 1.13\n",
            "|   |   |   |   |   |--- feature_24 <= -0.51\n",
            "|   |   |   |   |   |   |--- feature_4 <= -0.34\n",
            "|   |   |   |   |   |   |   |--- feature_22 <= -0.51\n",
            "|   |   |   |   |   |   |   |   |--- feature_17 <= 0.22\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_32 <= 1.57\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 <= -1.84\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 >  -1.84\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_32 >  1.57\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_29 <= 1.73\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_29 >  1.73\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |--- feature_17 >  0.22\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_23 <= -0.49\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_23 >  -0.49\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |--- feature_22 >  -0.51\n",
            "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_30 <= 1.12\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_31 <= 0.77\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_31 >  0.77\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_30 >  1.12\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |--- feature_3 >  0.04\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |--- feature_4 >  -0.34\n",
            "|   |   |   |   |   |   |   |--- feature_6 <= 0.18\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_30 <= 0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_36 <= 0.30\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_36 >  0.30\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_30 >  0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_27 <= -0.12\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_27 >  -0.12\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 >  0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_31 <= 0.92\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.37\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.37\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_31 >  0.92\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_6 >  0.18\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 <= 1.05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 <= -1.15\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_28 <= -0.63\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_28 >  -0.63\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_11 >  -1.15\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_32 <= 1.50\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_32 >  1.50\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 >  1.05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_19 <= 0.29\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.34\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.34\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_19 >  0.29\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |--- feature_24 >  -0.51\n",
            "|   |   |   |   |   |   |--- feature_26 <= 1.85\n",
            "|   |   |   |   |   |   |   |--- feature_2 <= 1.05\n",
            "|   |   |   |   |   |   |   |   |--- feature_15 <= -0.55\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 <= 0.87\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 <= -0.76\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_6 >  -0.76\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_34 >  0.87\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |--- feature_15 >  -0.55\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 1.24\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 <= 1.63\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_22 >  1.63\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 >  1.24\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= -0.35\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  -0.35\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_2 >  1.05\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.31\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 1.60\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= -0.28\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  -0.28\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_10 >  1.60\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |--- feature_4 >  0.31\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |--- feature_26 >  1.85\n",
            "|   |   |   |   |   |   |   |--- feature_37 <= -0.51\n",
            "|   |   |   |   |   |   |   |   |--- feature_35 <= -0.25\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |--- feature_35 >  -0.25\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |--- feature_37 >  -0.51\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 <= 2.94\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_13 <= -0.84\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.88\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_9 >  0.88\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_13 >  -0.84\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 1.34\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  1.34\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |--- feature_24 >  2.94\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |--- feature_14 >  1.13\n",
            "|   |   |   |   |   |--- feature_35 <= -0.23\n",
            "|   |   |   |   |   |   |--- feature_26 <= 0.17\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_26 >  0.17\n",
            "|   |   |   |   |   |   |   |--- feature_28 <= 0.38\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_28 >  0.38\n",
            "|   |   |   |   |   |   |   |   |--- feature_11 <= 0.15\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |--- feature_11 >  0.15\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |--- feature_35 >  -0.23\n",
            "|   |   |   |   |   |   |--- feature_23 <= 1.18\n",
            "|   |   |   |   |   |   |   |--- feature_26 <= -1.38\n",
            "|   |   |   |   |   |   |   |   |--- feature_0 <= -0.06\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |--- feature_0 >  -0.06\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |--- feature_26 >  -1.38\n",
            "|   |   |   |   |   |   |   |   |--- feature_39 <= -1.24\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |--- feature_39 >  -1.24\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_8 <= 1.42\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_8 >  1.42\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |--- feature_23 >  1.18\n",
            "|   |   |   |   |   |   |   |--- feature_7 <= -2.26\n",
            "|   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |--- feature_7 >  -2.26\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |--- feature_4 >  0.64\n",
            "|   |   |   |   |--- feature_34 <= -1.26\n",
            "|   |   |   |   |   |--- feature_39 <= -0.05\n",
            "|   |   |   |   |   |   |--- feature_4 <= 1.26\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_4 >  1.26\n",
            "|   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |--- feature_39 >  -0.05\n",
            "|   |   |   |   |   |   |--- feature_22 <= -0.41\n",
            "|   |   |   |   |   |   |   |--- feature_23 <= -1.54\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |--- feature_23 >  -1.54\n",
            "|   |   |   |   |   |   |   |   |--- class: 01\n",
            "|   |   |   |   |   |   |--- feature_22 >  -0.41\n",
            "|   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |--- feature_34 >  -1.26\n",
            "|   |   |   |   |   |--- feature_7 <= -0.61\n",
            "|   |   |   |   |   |   |--- feature_30 <= 2.04\n",
            "|   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |--- feature_30 >  2.04\n",
            "|   |   |   |   |   |   |   |--- feature_35 <= 0.28\n",
            "|   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_35 >  0.28\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |--- feature_7 >  -0.61\n",
            "|   |   |   |   |   |   |--- feature_2 <= -0.05\n",
            "|   |   |   |   |   |   |   |--- feature_8 <= -2.31\n",
            "|   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |--- feature_8 >  -2.31\n",
            "|   |   |   |   |   |   |   |   |--- feature_9 <= 1.13\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_29 <= -0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_21 <= 0.01\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_21 >  0.01\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_29 >  -0.61\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 1.49\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  1.49\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |--- feature_9 >  1.13\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 <= 0.46\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.54\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.54\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_33 >  0.46\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_2 >  -0.05\n",
            "|   |   |   |   |   |   |   |--- feature_0 <= 0.60\n",
            "|   |   |   |   |   |   |   |   |--- feature_22 <= -1.05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_26 <= -1.21\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_33 <= 1.25\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_33 >  1.25\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_26 >  -1.21\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |   |--- feature_22 >  -1.05\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 1.38\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_21 <= 0.94\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_21 >  0.94\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_3 >  1.38\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_18 <= 0.06\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_18 >  0.06\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 02\n",
            "|   |   |   |   |   |   |   |--- feature_0 >  0.60\n",
            "|   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |--- feature_20 >  2.12\n",
            "|   |   |   |--- feature_5 <= -1.36\n",
            "|   |   |   |   |--- feature_38 <= 1.17\n",
            "|   |   |   |   |   |--- feature_19 <= 4.14\n",
            "|   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |--- feature_19 >  4.14\n",
            "|   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |--- feature_38 >  1.17\n",
            "|   |   |   |   |   |--- feature_22 <= 1.29\n",
            "|   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |--- feature_22 >  1.29\n",
            "|   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |--- feature_5 >  -1.36\n",
            "|   |   |   |   |--- feature_7 <= 0.07\n",
            "|   |   |   |   |   |--- feature_14 <= -1.93\n",
            "|   |   |   |   |   |   |--- feature_27 <= -0.00\n",
            "|   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |--- feature_27 >  -0.00\n",
            "|   |   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |--- feature_14 >  -1.93\n",
            "|   |   |   |   |   |   |--- feature_29 <= 1.45\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |--- feature_29 >  1.45\n",
            "|   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |--- feature_7 >  0.07\n",
            "|   |   |   |   |   |--- feature_20 <= 2.68\n",
            "|   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |--- feature_20 >  2.68\n",
            "|   |   |   |   |   |   |--- class: 03\n",
            "|   |--- feature_0 >  1.36\n",
            "|   |   |--- feature_6 <= -1.34\n",
            "|   |   |   |--- feature_16 <= -0.66\n",
            "|   |   |   |   |--- feature_32 <= -0.35\n",
            "|   |   |   |   |   |--- feature_15 <= -0.41\n",
            "|   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |--- feature_15 >  -0.41\n",
            "|   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |--- feature_32 >  -0.35\n",
            "|   |   |   |   |   |--- class: 03\n",
            "|   |   |   |--- feature_16 >  -0.66\n",
            "|   |   |   |   |--- feature_34 <= 0.82\n",
            "|   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |--- feature_34 >  0.82\n",
            "|   |   |   |   |   |--- feature_2 <= -2.48\n",
            "|   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |--- feature_2 >  -2.48\n",
            "|   |   |   |   |   |   |--- class: 06\n",
            "|   |   |--- feature_6 >  -1.34\n",
            "|   |   |   |--- feature_5 <= 0.49\n",
            "|   |   |   |   |--- feature_16 <= 2.29\n",
            "|   |   |   |   |   |--- feature_13 <= 2.74\n",
            "|   |   |   |   |   |   |--- feature_7 <= -2.01\n",
            "|   |   |   |   |   |   |   |--- feature_36 <= -0.47\n",
            "|   |   |   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |   |   |   |--- feature_36 >  -0.47\n",
            "|   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |--- feature_7 >  -2.01\n",
            "|   |   |   |   |   |   |   |--- feature_3 <= -2.61\n",
            "|   |   |   |   |   |   |   |   |--- feature_19 <= -0.04\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |   |   |   |--- feature_19 >  -0.04\n",
            "|   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |--- feature_3 >  -2.61\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 <= -0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_13 <= -1.65\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_39 <= -0.30\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_39 >  -0.30\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_13 >  -1.65\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |   |   |--- feature_2 >  -0.20\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 1.32\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_31 <= -0.38\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 08\n",
            "|   |   |   |   |   |   |   |   |   |   |--- feature_31 >  -0.38\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |   |   |   |   |--- feature_7 >  1.32\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |--- feature_13 >  2.74\n",
            "|   |   |   |   |   |   |--- feature_17 <= -0.78\n",
            "|   |   |   |   |   |   |   |--- class: 05\n",
            "|   |   |   |   |   |   |--- feature_17 >  -0.78\n",
            "|   |   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |--- feature_16 >  2.29\n",
            "|   |   |   |   |   |--- feature_23 <= 0.30\n",
            "|   |   |   |   |   |   |--- class: 03\n",
            "|   |   |   |   |   |--- feature_23 >  0.30\n",
            "|   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |--- feature_5 >  0.49\n",
            "|   |   |   |   |--- feature_26 <= 0.32\n",
            "|   |   |   |   |   |--- feature_8 <= -0.92\n",
            "|   |   |   |   |   |   |--- class: 04\n",
            "|   |   |   |   |   |--- feature_8 >  -0.92\n",
            "|   |   |   |   |   |   |--- class: 07\n",
            "|   |   |   |   |--- feature_26 >  0.32\n",
            "|   |   |   |   |   |--- feature_2 <= 0.12\n",
            "|   |   |   |   |   |   |--- class: 06\n",
            "|   |   |   |   |   |--- feature_2 >  0.12\n",
            "|   |   |   |   |   |   |--- class: 03\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDdFKKx5f3U0"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaNL9ZZjf3U0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Rqv8rHf3U0"
      },
      "outputs": [],
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfAc5si8f3U1",
        "outputId": "2c3a8f10-ed52-48ff-ccd4-3e19b44668d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "                       min_samples_leaf=3, min_samples_split=20,\n",
              "                       n_estimators=22000, random_state=5)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "rforest.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utEm9IvSf3U1"
      },
      "outputs": [],
      "source": [
        "predictions = rforest.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDlgf7e_f3U1",
        "outputId": "8ed55a0d-78b3-4290-ec62-33a4687502cc",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.89      0.55      0.68        56\n",
            "           2       0.70      0.94      0.80       130\n",
            "           3       0.82      0.74      0.78       137\n",
            "           4       0.75      0.70      0.73       121\n",
            "           5       0.88      0.80      0.84       125\n",
            "           6       0.78      0.79      0.79       120\n",
            "           7       0.80      0.68      0.74       136\n",
            "           8       0.71      0.85      0.77       137\n",
            "\n",
            "    accuracy                           0.77       962\n",
            "   macro avg       0.79      0.76      0.77       962\n",
            "weighted avg       0.78      0.77      0.77       962\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test,predictions))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOo0paKj19Sq"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters tuning with gridSearch\n",
        "\n",
        "#Define parameters to try out\n",
        "parameters = {\n",
        "'criterion' : ['gini', 'entropy'],\n",
        "'max_leaf_nodes' : [100],\n",
        "'max_features': ['log2'],\n",
        "'max_depth': [10],\n",
        "'min_samples_split': [15],\n",
        "'min_samples_leaf': [5],\n",
        "'n_estimators': [22000],\n",
        "'random_state' : [5]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "exz-sua031V9",
        "outputId": "9b382222-06ba-4981-ec19-6ae639b71754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "RandomForestClassifier(criterion='entropy', max_depth=10, max_features='log2',\n",
            "                       max_leaf_nodes=100, min_samples_leaf=5,\n",
            "                       min_samples_split=15, n_estimators=22000,\n",
            "                       random_state=5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nFitting 5 folds for each of 2 candidates, totalling 10 fits\\nRandomForestClassifier(criterion='entropy', max_depth=10, max_features='log2',\\n                       max_leaf_nodes=100, min_samples_leaf=5,\\n                       min_samples_split=15, n_estimators=22000,\\n                       random_state=5)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#a hyperparameter is a parameter whose value is used to control the learning process.\n",
        "#Tuning thos parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search_dt = GridSearchCV(estimator = rforest,\n",
        "param_grid = parameters,\n",
        "scoring = 'accuracy',\n",
        "cv = 5,\n",
        "verbose = 1)\n",
        "\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "print(grid_search_dt.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model computed:\n",
        "rforest = RandomForestClassifier(criterion=\"entropy\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 5, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)\n",
        "\n",
        "rforest.fit(X_train, y_train)\n",
        "predictions = rforest.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50jB1A8DR63L",
        "outputId": "9aa43eb4-6db7-4e58-db4d-997e9333e096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.66      0.76        56\n",
            "           2       0.71      0.95      0.82       130\n",
            "           3       0.87      0.77      0.81       137\n",
            "           4       0.79      0.77      0.78       121\n",
            "           5       0.88      0.84      0.86       125\n",
            "           6       0.80      0.84      0.82       120\n",
            "           7       0.86      0.68      0.76       136\n",
            "           8       0.77      0.88      0.82       137\n",
            "\n",
            "    accuracy                           0.81       962\n",
            "   macro avg       0.82      0.80      0.80       962\n",
            "weighted avg       0.82      0.81      0.81       962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsMeHCKnf3U2"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN MFCCs"
      ],
      "metadata": {
        "id": "dqyErBl184Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2) \n",
        "x_testcnn = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "y_test = y_test.astype(np.float)\n",
        "y_train = y_train.astype(np.float)\n",
        "\n",
        "x_traincnn.shape, x_testcnn.shape "
      ],
      "metadata": {
        "id": "Pz1SLeHk8GUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a12f7b-9b6a-46c2-8291-ce0f139cdc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1951, 40, 1), (962, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "#from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "#hidden neurons = (#input neurons + #output neurons)/2\n",
        "model.add(Conv1D(70, 5, padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0000580, rho=0.9, epsilon=None, decay=0.0) "
      ],
      "metadata": {
        "id": "ZjIfiRXt8GZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ubnXWfsZ-CoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794f829c-700f-42fe-a7a0-923cda70ad55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 5, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 5, 70)             44870     \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 5, 70)             0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 5, 70)             0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 350)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                3510      \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,148\n",
            "Trainable params: 49,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "88sgeI5k8Gc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1100, validation_data=(x_testcnn, y_test)) "
      ],
      "metadata": {
        "id": "s6irRRRX8e3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40afce8-9e91-430d-86e2-9f18a6a74180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 2.2333 - accuracy: 0.1599 - val_loss: 2.1575 - val_accuracy: 0.2152\n",
            "Epoch 2/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.0949 - accuracy: 0.2076 - val_loss: 2.0625 - val_accuracy: 0.2214\n",
            "Epoch 3/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.0239 - accuracy: 0.2260 - val_loss: 2.0077 - val_accuracy: 0.2516\n",
            "Epoch 4/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9762 - accuracy: 0.2435 - val_loss: 1.9681 - val_accuracy: 0.2568\n",
            "Epoch 5/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9499 - accuracy: 0.2522 - val_loss: 1.9424 - val_accuracy: 0.2723\n",
            "Epoch 6/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9341 - accuracy: 0.2553 - val_loss: 1.9203 - val_accuracy: 0.2786\n",
            "Epoch 7/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9075 - accuracy: 0.2645 - val_loss: 1.9025 - val_accuracy: 0.2921\n",
            "Epoch 8/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8812 - accuracy: 0.2686 - val_loss: 1.8848 - val_accuracy: 0.3108\n",
            "Epoch 9/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8721 - accuracy: 0.2819 - val_loss: 1.8714 - val_accuracy: 0.3119\n",
            "Epoch 10/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8637 - accuracy: 0.2722 - val_loss: 1.8608 - val_accuracy: 0.3264\n",
            "Epoch 11/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8429 - accuracy: 0.3029 - val_loss: 1.8499 - val_accuracy: 0.3274\n",
            "Epoch 12/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8306 - accuracy: 0.3024 - val_loss: 1.8391 - val_accuracy: 0.3337\n",
            "Epoch 13/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8337 - accuracy: 0.2988 - val_loss: 1.8309 - val_accuracy: 0.3285\n",
            "Epoch 14/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8139 - accuracy: 0.3183 - val_loss: 1.8227 - val_accuracy: 0.3368\n",
            "Epoch 15/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8037 - accuracy: 0.3209 - val_loss: 1.8117 - val_accuracy: 0.3389\n",
            "Epoch 16/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7969 - accuracy: 0.3229 - val_loss: 1.8054 - val_accuracy: 0.3368\n",
            "Epoch 17/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7841 - accuracy: 0.3291 - val_loss: 1.7977 - val_accuracy: 0.3399\n",
            "Epoch 18/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7791 - accuracy: 0.3352 - val_loss: 1.7911 - val_accuracy: 0.3503\n",
            "Epoch 19/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7629 - accuracy: 0.3470 - val_loss: 1.7838 - val_accuracy: 0.3462\n",
            "Epoch 20/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7645 - accuracy: 0.3439 - val_loss: 1.7756 - val_accuracy: 0.3607\n",
            "Epoch 21/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7571 - accuracy: 0.3491 - val_loss: 1.7720 - val_accuracy: 0.3638\n",
            "Epoch 22/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7384 - accuracy: 0.3475 - val_loss: 1.7646 - val_accuracy: 0.3493\n",
            "Epoch 23/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7417 - accuracy: 0.3378 - val_loss: 1.7532 - val_accuracy: 0.3628\n",
            "Epoch 24/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7265 - accuracy: 0.3711 - val_loss: 1.7489 - val_accuracy: 0.3638\n",
            "Epoch 25/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7114 - accuracy: 0.3696 - val_loss: 1.7401 - val_accuracy: 0.3753\n",
            "Epoch 26/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7054 - accuracy: 0.3731 - val_loss: 1.7329 - val_accuracy: 0.3680\n",
            "Epoch 27/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6939 - accuracy: 0.3675 - val_loss: 1.7274 - val_accuracy: 0.3638\n",
            "Epoch 28/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6921 - accuracy: 0.3808 - val_loss: 1.7191 - val_accuracy: 0.3638\n",
            "Epoch 29/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6926 - accuracy: 0.3706 - val_loss: 1.7130 - val_accuracy: 0.3794\n",
            "Epoch 30/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6777 - accuracy: 0.3829 - val_loss: 1.7088 - val_accuracy: 0.3857\n",
            "Epoch 31/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6704 - accuracy: 0.3829 - val_loss: 1.7039 - val_accuracy: 0.3773\n",
            "Epoch 32/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6538 - accuracy: 0.4054 - val_loss: 1.6955 - val_accuracy: 0.3815\n",
            "Epoch 33/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6570 - accuracy: 0.3911 - val_loss: 1.6894 - val_accuracy: 0.3888\n",
            "Epoch 34/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6485 - accuracy: 0.3972 - val_loss: 1.6802 - val_accuracy: 0.3950\n",
            "Epoch 35/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6300 - accuracy: 0.4152 - val_loss: 1.6738 - val_accuracy: 0.3940\n",
            "Epoch 36/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6347 - accuracy: 0.3952 - val_loss: 1.6709 - val_accuracy: 0.3846\n",
            "Epoch 37/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6250 - accuracy: 0.4157 - val_loss: 1.6635 - val_accuracy: 0.3950\n",
            "Epoch 38/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6287 - accuracy: 0.4003 - val_loss: 1.6611 - val_accuracy: 0.4002\n",
            "Epoch 39/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6131 - accuracy: 0.4147 - val_loss: 1.6542 - val_accuracy: 0.3960\n",
            "Epoch 40/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6020 - accuracy: 0.4157 - val_loss: 1.6482 - val_accuracy: 0.3971\n",
            "Epoch 41/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6078 - accuracy: 0.4198 - val_loss: 1.6410 - val_accuracy: 0.4002\n",
            "Epoch 42/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6053 - accuracy: 0.4085 - val_loss: 1.6367 - val_accuracy: 0.4096\n",
            "Epoch 43/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5806 - accuracy: 0.4398 - val_loss: 1.6306 - val_accuracy: 0.4012\n",
            "Epoch 44/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5768 - accuracy: 0.4372 - val_loss: 1.6243 - val_accuracy: 0.4137\n",
            "Epoch 45/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5769 - accuracy: 0.4229 - val_loss: 1.6222 - val_accuracy: 0.4044\n",
            "Epoch 46/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5600 - accuracy: 0.4326 - val_loss: 1.6155 - val_accuracy: 0.4148\n",
            "Epoch 47/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5584 - accuracy: 0.4295 - val_loss: 1.6096 - val_accuracy: 0.4158\n",
            "Epoch 48/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5597 - accuracy: 0.4382 - val_loss: 1.6054 - val_accuracy: 0.4158\n",
            "Epoch 49/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5404 - accuracy: 0.4377 - val_loss: 1.6038 - val_accuracy: 0.4137\n",
            "Epoch 50/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5387 - accuracy: 0.4403 - val_loss: 1.5960 - val_accuracy: 0.4189\n",
            "Epoch 51/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5379 - accuracy: 0.4495 - val_loss: 1.5909 - val_accuracy: 0.4210\n",
            "Epoch 52/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5202 - accuracy: 0.4403 - val_loss: 1.5846 - val_accuracy: 0.4272\n",
            "Epoch 53/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5208 - accuracy: 0.4531 - val_loss: 1.5808 - val_accuracy: 0.4293\n",
            "Epoch 54/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5091 - accuracy: 0.4505 - val_loss: 1.5787 - val_accuracy: 0.4231\n",
            "Epoch 55/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5051 - accuracy: 0.4654 - val_loss: 1.5692 - val_accuracy: 0.4220\n",
            "Epoch 56/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5076 - accuracy: 0.4557 - val_loss: 1.5685 - val_accuracy: 0.4252\n",
            "Epoch 57/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5090 - accuracy: 0.4449 - val_loss: 1.5606 - val_accuracy: 0.4345\n",
            "Epoch 58/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5027 - accuracy: 0.4500 - val_loss: 1.5560 - val_accuracy: 0.4356\n",
            "Epoch 59/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4839 - accuracy: 0.4721 - val_loss: 1.5536 - val_accuracy: 0.4356\n",
            "Epoch 60/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4909 - accuracy: 0.4639 - val_loss: 1.5473 - val_accuracy: 0.4366\n",
            "Epoch 61/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4742 - accuracy: 0.4762 - val_loss: 1.5433 - val_accuracy: 0.4252\n",
            "Epoch 62/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4769 - accuracy: 0.4716 - val_loss: 1.5404 - val_accuracy: 0.4356\n",
            "Epoch 63/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4537 - accuracy: 0.4849 - val_loss: 1.5331 - val_accuracy: 0.4449\n",
            "Epoch 64/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4645 - accuracy: 0.4823 - val_loss: 1.5366 - val_accuracy: 0.4304\n",
            "Epoch 65/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4638 - accuracy: 0.4787 - val_loss: 1.5273 - val_accuracy: 0.4366\n",
            "Epoch 66/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4486 - accuracy: 0.4736 - val_loss: 1.5224 - val_accuracy: 0.4491\n",
            "Epoch 67/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4398 - accuracy: 0.4808 - val_loss: 1.5175 - val_accuracy: 0.4470\n",
            "Epoch 68/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4443 - accuracy: 0.4885 - val_loss: 1.5135 - val_accuracy: 0.4387\n",
            "Epoch 69/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4351 - accuracy: 0.4890 - val_loss: 1.5075 - val_accuracy: 0.4532\n",
            "Epoch 70/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4382 - accuracy: 0.4808 - val_loss: 1.5059 - val_accuracy: 0.4584\n",
            "Epoch 71/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4236 - accuracy: 0.4890 - val_loss: 1.5010 - val_accuracy: 0.4387\n",
            "Epoch 72/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4282 - accuracy: 0.4921 - val_loss: 1.4959 - val_accuracy: 0.4553\n",
            "Epoch 73/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4268 - accuracy: 0.4941 - val_loss: 1.4923 - val_accuracy: 0.4574\n",
            "Epoch 74/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4221 - accuracy: 0.4910 - val_loss: 1.4907 - val_accuracy: 0.4532\n",
            "Epoch 75/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.4101 - accuracy: 0.5003 - val_loss: 1.4870 - val_accuracy: 0.4574\n",
            "Epoch 76/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3939 - accuracy: 0.4982 - val_loss: 1.4857 - val_accuracy: 0.4511\n",
            "Epoch 77/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3927 - accuracy: 0.5090 - val_loss: 1.4796 - val_accuracy: 0.4563\n",
            "Epoch 78/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3915 - accuracy: 0.5074 - val_loss: 1.4795 - val_accuracy: 0.4459\n",
            "Epoch 79/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3889 - accuracy: 0.4956 - val_loss: 1.4729 - val_accuracy: 0.4543\n",
            "Epoch 80/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3731 - accuracy: 0.5141 - val_loss: 1.4739 - val_accuracy: 0.4449\n",
            "Epoch 81/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3644 - accuracy: 0.5192 - val_loss: 1.4645 - val_accuracy: 0.4543\n",
            "Epoch 82/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3782 - accuracy: 0.5100 - val_loss: 1.4639 - val_accuracy: 0.4688\n",
            "Epoch 83/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3740 - accuracy: 0.5095 - val_loss: 1.4598 - val_accuracy: 0.4636\n",
            "Epoch 84/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3655 - accuracy: 0.5167 - val_loss: 1.4543 - val_accuracy: 0.4771\n",
            "Epoch 85/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3586 - accuracy: 0.5146 - val_loss: 1.4498 - val_accuracy: 0.4802\n",
            "Epoch 86/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3557 - accuracy: 0.5136 - val_loss: 1.4478 - val_accuracy: 0.4636\n",
            "Epoch 87/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3607 - accuracy: 0.5223 - val_loss: 1.4458 - val_accuracy: 0.4751\n",
            "Epoch 88/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3454 - accuracy: 0.5208 - val_loss: 1.4443 - val_accuracy: 0.4709\n",
            "Epoch 89/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3506 - accuracy: 0.5202 - val_loss: 1.4408 - val_accuracy: 0.4605\n",
            "Epoch 90/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3398 - accuracy: 0.5320 - val_loss: 1.4357 - val_accuracy: 0.4667\n",
            "Epoch 91/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3331 - accuracy: 0.5259 - val_loss: 1.4346 - val_accuracy: 0.4657\n",
            "Epoch 92/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3438 - accuracy: 0.5182 - val_loss: 1.4308 - val_accuracy: 0.4719\n",
            "Epoch 93/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3325 - accuracy: 0.5269 - val_loss: 1.4270 - val_accuracy: 0.4730\n",
            "Epoch 94/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3365 - accuracy: 0.5249 - val_loss: 1.4237 - val_accuracy: 0.4647\n",
            "Epoch 95/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3153 - accuracy: 0.5346 - val_loss: 1.4207 - val_accuracy: 0.4667\n",
            "Epoch 96/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3218 - accuracy: 0.5351 - val_loss: 1.4178 - val_accuracy: 0.4823\n",
            "Epoch 97/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3180 - accuracy: 0.5269 - val_loss: 1.4168 - val_accuracy: 0.4761\n",
            "Epoch 98/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3128 - accuracy: 0.5341 - val_loss: 1.4178 - val_accuracy: 0.4699\n",
            "Epoch 99/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.3052 - accuracy: 0.5300 - val_loss: 1.4092 - val_accuracy: 0.4834\n",
            "Epoch 100/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2961 - accuracy: 0.5464 - val_loss: 1.4064 - val_accuracy: 0.4730\n",
            "Epoch 101/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2951 - accuracy: 0.5489 - val_loss: 1.4031 - val_accuracy: 0.4834\n",
            "Epoch 102/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2877 - accuracy: 0.5443 - val_loss: 1.4020 - val_accuracy: 0.4740\n",
            "Epoch 103/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2714 - accuracy: 0.5587 - val_loss: 1.3974 - val_accuracy: 0.4823\n",
            "Epoch 104/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2722 - accuracy: 0.5448 - val_loss: 1.3934 - val_accuracy: 0.4834\n",
            "Epoch 105/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2739 - accuracy: 0.5495 - val_loss: 1.3933 - val_accuracy: 0.4813\n",
            "Epoch 106/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2711 - accuracy: 0.5530 - val_loss: 1.3876 - val_accuracy: 0.4886\n",
            "Epoch 107/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2630 - accuracy: 0.5577 - val_loss: 1.3845 - val_accuracy: 0.4834\n",
            "Epoch 108/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2570 - accuracy: 0.5489 - val_loss: 1.3840 - val_accuracy: 0.4792\n",
            "Epoch 109/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2614 - accuracy: 0.5633 - val_loss: 1.3804 - val_accuracy: 0.4865\n",
            "Epoch 110/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2623 - accuracy: 0.5582 - val_loss: 1.3777 - val_accuracy: 0.4854\n",
            "Epoch 111/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2569 - accuracy: 0.5607 - val_loss: 1.3747 - val_accuracy: 0.4865\n",
            "Epoch 112/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2583 - accuracy: 0.5566 - val_loss: 1.3732 - val_accuracy: 0.4896\n",
            "Epoch 113/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2426 - accuracy: 0.5602 - val_loss: 1.3741 - val_accuracy: 0.4886\n",
            "Epoch 114/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2469 - accuracy: 0.5484 - val_loss: 1.3678 - val_accuracy: 0.4958\n",
            "Epoch 115/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2449 - accuracy: 0.5566 - val_loss: 1.3658 - val_accuracy: 0.4958\n",
            "Epoch 116/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2281 - accuracy: 0.5746 - val_loss: 1.3630 - val_accuracy: 0.4886\n",
            "Epoch 117/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2458 - accuracy: 0.5582 - val_loss: 1.3596 - val_accuracy: 0.5021\n",
            "Epoch 118/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2343 - accuracy: 0.5684 - val_loss: 1.3538 - val_accuracy: 0.4979\n",
            "Epoch 119/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2175 - accuracy: 0.5833 - val_loss: 1.3566 - val_accuracy: 0.5031\n",
            "Epoch 120/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2189 - accuracy: 0.5710 - val_loss: 1.3506 - val_accuracy: 0.4990\n",
            "Epoch 121/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2316 - accuracy: 0.5623 - val_loss: 1.3496 - val_accuracy: 0.5031\n",
            "Epoch 122/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2283 - accuracy: 0.5715 - val_loss: 1.3443 - val_accuracy: 0.5000\n",
            "Epoch 123/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2226 - accuracy: 0.5669 - val_loss: 1.3432 - val_accuracy: 0.5042\n",
            "Epoch 124/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2049 - accuracy: 0.5771 - val_loss: 1.3382 - val_accuracy: 0.4979\n",
            "Epoch 125/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2024 - accuracy: 0.5771 - val_loss: 1.3359 - val_accuracy: 0.5114\n",
            "Epoch 126/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1966 - accuracy: 0.5859 - val_loss: 1.3356 - val_accuracy: 0.5114\n",
            "Epoch 127/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.2114 - accuracy: 0.5818 - val_loss: 1.3338 - val_accuracy: 0.5073\n",
            "Epoch 128/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1906 - accuracy: 0.5905 - val_loss: 1.3310 - val_accuracy: 0.5073\n",
            "Epoch 129/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1977 - accuracy: 0.5787 - val_loss: 1.3302 - val_accuracy: 0.5042\n",
            "Epoch 130/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1917 - accuracy: 0.5889 - val_loss: 1.3247 - val_accuracy: 0.5146\n",
            "Epoch 131/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1764 - accuracy: 0.6017 - val_loss: 1.3227 - val_accuracy: 0.5156\n",
            "Epoch 132/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1878 - accuracy: 0.5787 - val_loss: 1.3187 - val_accuracy: 0.5166\n",
            "Epoch 133/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1879 - accuracy: 0.5848 - val_loss: 1.3208 - val_accuracy: 0.5031\n",
            "Epoch 134/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1845 - accuracy: 0.5823 - val_loss: 1.3192 - val_accuracy: 0.5104\n",
            "Epoch 135/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1714 - accuracy: 0.5997 - val_loss: 1.3125 - val_accuracy: 0.5249\n",
            "Epoch 136/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1725 - accuracy: 0.5987 - val_loss: 1.3110 - val_accuracy: 0.5260\n",
            "Epoch 137/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1827 - accuracy: 0.5956 - val_loss: 1.3095 - val_accuracy: 0.5187\n",
            "Epoch 138/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1576 - accuracy: 0.5951 - val_loss: 1.3052 - val_accuracy: 0.5333\n",
            "Epoch 139/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1640 - accuracy: 0.5879 - val_loss: 1.3055 - val_accuracy: 0.5270\n",
            "Epoch 140/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1563 - accuracy: 0.5971 - val_loss: 1.3028 - val_accuracy: 0.5249\n",
            "Epoch 141/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1689 - accuracy: 0.5951 - val_loss: 1.2985 - val_accuracy: 0.5312\n",
            "Epoch 142/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1398 - accuracy: 0.6069 - val_loss: 1.2971 - val_accuracy: 0.5281\n",
            "Epoch 143/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1674 - accuracy: 0.5864 - val_loss: 1.2931 - val_accuracy: 0.5291\n",
            "Epoch 144/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1452 - accuracy: 0.5961 - val_loss: 1.2913 - val_accuracy: 0.5281\n",
            "Epoch 145/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1306 - accuracy: 0.5956 - val_loss: 1.2912 - val_accuracy: 0.5249\n",
            "Epoch 146/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1469 - accuracy: 0.5946 - val_loss: 1.2919 - val_accuracy: 0.5229\n",
            "Epoch 147/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1436 - accuracy: 0.6074 - val_loss: 1.2838 - val_accuracy: 0.5249\n",
            "Epoch 148/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1375 - accuracy: 0.6099 - val_loss: 1.2847 - val_accuracy: 0.5322\n",
            "Epoch 149/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1383 - accuracy: 0.5961 - val_loss: 1.2819 - val_accuracy: 0.5353\n",
            "Epoch 150/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1308 - accuracy: 0.6115 - val_loss: 1.2794 - val_accuracy: 0.5374\n",
            "Epoch 151/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1411 - accuracy: 0.5992 - val_loss: 1.2786 - val_accuracy: 0.5270\n",
            "Epoch 152/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1299 - accuracy: 0.6099 - val_loss: 1.2781 - val_accuracy: 0.5437\n",
            "Epoch 153/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1095 - accuracy: 0.6212 - val_loss: 1.2722 - val_accuracy: 0.5364\n",
            "Epoch 154/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1095 - accuracy: 0.6212 - val_loss: 1.2702 - val_accuracy: 0.5416\n",
            "Epoch 155/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1200 - accuracy: 0.6130 - val_loss: 1.2656 - val_accuracy: 0.5530\n",
            "Epoch 156/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1139 - accuracy: 0.6238 - val_loss: 1.2641 - val_accuracy: 0.5426\n",
            "Epoch 157/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1214 - accuracy: 0.6161 - val_loss: 1.2649 - val_accuracy: 0.5395\n",
            "Epoch 158/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1030 - accuracy: 0.6274 - val_loss: 1.2640 - val_accuracy: 0.5353\n",
            "Epoch 159/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.1067 - accuracy: 0.6120 - val_loss: 1.2616 - val_accuracy: 0.5416\n",
            "Epoch 160/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0951 - accuracy: 0.6135 - val_loss: 1.2593 - val_accuracy: 0.5437\n",
            "Epoch 161/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0998 - accuracy: 0.6171 - val_loss: 1.2553 - val_accuracy: 0.5353\n",
            "Epoch 162/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0966 - accuracy: 0.6187 - val_loss: 1.2552 - val_accuracy: 0.5468\n",
            "Epoch 163/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.1056 - accuracy: 0.6192 - val_loss: 1.2529 - val_accuracy: 0.5426\n",
            "Epoch 164/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.0811 - accuracy: 0.6222 - val_loss: 1.2520 - val_accuracy: 0.5374\n",
            "Epoch 165/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0881 - accuracy: 0.6248 - val_loss: 1.2479 - val_accuracy: 0.5489\n",
            "Epoch 166/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0995 - accuracy: 0.6171 - val_loss: 1.2462 - val_accuracy: 0.5478\n",
            "Epoch 167/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.0791 - accuracy: 0.6279 - val_loss: 1.2441 - val_accuracy: 0.5489\n",
            "Epoch 168/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0746 - accuracy: 0.6315 - val_loss: 1.2419 - val_accuracy: 0.5395\n",
            "Epoch 169/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0934 - accuracy: 0.6243 - val_loss: 1.2405 - val_accuracy: 0.5395\n",
            "Epoch 170/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0688 - accuracy: 0.6310 - val_loss: 1.2367 - val_accuracy: 0.5499\n",
            "Epoch 171/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0676 - accuracy: 0.6381 - val_loss: 1.2325 - val_accuracy: 0.5489\n",
            "Epoch 172/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0710 - accuracy: 0.6243 - val_loss: 1.2315 - val_accuracy: 0.5551\n",
            "Epoch 173/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0497 - accuracy: 0.6417 - val_loss: 1.2298 - val_accuracy: 0.5561\n",
            "Epoch 174/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0635 - accuracy: 0.6335 - val_loss: 1.2296 - val_accuracy: 0.5530\n",
            "Epoch 175/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0597 - accuracy: 0.6392 - val_loss: 1.2277 - val_accuracy: 0.5655\n",
            "Epoch 176/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0646 - accuracy: 0.6315 - val_loss: 1.2241 - val_accuracy: 0.5582\n",
            "Epoch 177/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0622 - accuracy: 0.6330 - val_loss: 1.2223 - val_accuracy: 0.5624\n",
            "Epoch 178/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0460 - accuracy: 0.6443 - val_loss: 1.2199 - val_accuracy: 0.5572\n",
            "Epoch 179/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0348 - accuracy: 0.6458 - val_loss: 1.2196 - val_accuracy: 0.5509\n",
            "Epoch 180/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0470 - accuracy: 0.6335 - val_loss: 1.2169 - val_accuracy: 0.5593\n",
            "Epoch 181/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0265 - accuracy: 0.6530 - val_loss: 1.2163 - val_accuracy: 0.5478\n",
            "Epoch 182/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0393 - accuracy: 0.6397 - val_loss: 1.2148 - val_accuracy: 0.5561\n",
            "Epoch 183/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0334 - accuracy: 0.6392 - val_loss: 1.2120 - val_accuracy: 0.5665\n",
            "Epoch 184/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0352 - accuracy: 0.6530 - val_loss: 1.2146 - val_accuracy: 0.5613\n",
            "Epoch 185/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0430 - accuracy: 0.6561 - val_loss: 1.2101 - val_accuracy: 0.5572\n",
            "Epoch 186/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0358 - accuracy: 0.6407 - val_loss: 1.2086 - val_accuracy: 0.5728\n",
            "Epoch 187/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.0194 - accuracy: 0.6520 - val_loss: 1.2032 - val_accuracy: 0.5655\n",
            "Epoch 188/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0319 - accuracy: 0.6376 - val_loss: 1.2020 - val_accuracy: 0.5728\n",
            "Epoch 189/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0364 - accuracy: 0.6427 - val_loss: 1.2036 - val_accuracy: 0.5811\n",
            "Epoch 190/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0185 - accuracy: 0.6468 - val_loss: 1.1986 - val_accuracy: 0.5759\n",
            "Epoch 191/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0206 - accuracy: 0.6556 - val_loss: 1.1951 - val_accuracy: 0.5686\n",
            "Epoch 192/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0093 - accuracy: 0.6556 - val_loss: 1.1991 - val_accuracy: 0.5665\n",
            "Epoch 193/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0072 - accuracy: 0.6576 - val_loss: 1.1955 - val_accuracy: 0.5613\n",
            "Epoch 194/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0233 - accuracy: 0.6509 - val_loss: 1.1898 - val_accuracy: 0.5821\n",
            "Epoch 195/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0053 - accuracy: 0.6632 - val_loss: 1.1878 - val_accuracy: 0.5655\n",
            "Epoch 196/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.9911 - accuracy: 0.6622 - val_loss: 1.1876 - val_accuracy: 0.5728\n",
            "Epoch 197/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.0009 - accuracy: 0.6525 - val_loss: 1.1832 - val_accuracy: 0.5800\n",
            "Epoch 198/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9998 - accuracy: 0.6550 - val_loss: 1.1836 - val_accuracy: 0.5821\n",
            "Epoch 199/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9972 - accuracy: 0.6674 - val_loss: 1.1796 - val_accuracy: 0.5728\n",
            "Epoch 200/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0028 - accuracy: 0.6509 - val_loss: 1.1784 - val_accuracy: 0.5738\n",
            "Epoch 201/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9983 - accuracy: 0.6586 - val_loss: 1.1799 - val_accuracy: 0.5894\n",
            "Epoch 202/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.0030 - accuracy: 0.6535 - val_loss: 1.1792 - val_accuracy: 0.5977\n",
            "Epoch 203/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.9825 - accuracy: 0.6781 - val_loss: 1.1809 - val_accuracy: 0.5748\n",
            "Epoch 204/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9810 - accuracy: 0.6704 - val_loss: 1.1689 - val_accuracy: 0.5738\n",
            "Epoch 205/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9874 - accuracy: 0.6684 - val_loss: 1.1690 - val_accuracy: 0.5748\n",
            "Epoch 206/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9888 - accuracy: 0.6653 - val_loss: 1.1692 - val_accuracy: 0.5738\n",
            "Epoch 207/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9793 - accuracy: 0.6745 - val_loss: 1.1633 - val_accuracy: 0.5904\n",
            "Epoch 208/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.9946 - accuracy: 0.6571 - val_loss: 1.1624 - val_accuracy: 0.5800\n",
            "Epoch 209/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9743 - accuracy: 0.6632 - val_loss: 1.1594 - val_accuracy: 0.5925\n",
            "Epoch 210/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9904 - accuracy: 0.6602 - val_loss: 1.1620 - val_accuracy: 0.5800\n",
            "Epoch 211/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9783 - accuracy: 0.6668 - val_loss: 1.1613 - val_accuracy: 0.6019\n",
            "Epoch 212/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9813 - accuracy: 0.6684 - val_loss: 1.1559 - val_accuracy: 0.5894\n",
            "Epoch 213/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9727 - accuracy: 0.6684 - val_loss: 1.1554 - val_accuracy: 0.5956\n",
            "Epoch 214/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9720 - accuracy: 0.6720 - val_loss: 1.1514 - val_accuracy: 0.5873\n",
            "Epoch 215/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9551 - accuracy: 0.6807 - val_loss: 1.1527 - val_accuracy: 0.6040\n",
            "Epoch 216/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9592 - accuracy: 0.6807 - val_loss: 1.1507 - val_accuracy: 0.5967\n",
            "Epoch 217/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9474 - accuracy: 0.6838 - val_loss: 1.1490 - val_accuracy: 0.5925\n",
            "Epoch 218/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9570 - accuracy: 0.6745 - val_loss: 1.1479 - val_accuracy: 0.5821\n",
            "Epoch 219/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.9537 - accuracy: 0.6694 - val_loss: 1.1470 - val_accuracy: 0.6040\n",
            "Epoch 220/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9455 - accuracy: 0.6745 - val_loss: 1.1398 - val_accuracy: 0.5998\n",
            "Epoch 221/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9553 - accuracy: 0.6884 - val_loss: 1.1420 - val_accuracy: 0.5967\n",
            "Epoch 222/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9548 - accuracy: 0.6771 - val_loss: 1.1432 - val_accuracy: 0.5988\n",
            "Epoch 223/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9426 - accuracy: 0.6827 - val_loss: 1.1477 - val_accuracy: 0.6071\n",
            "Epoch 224/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9576 - accuracy: 0.6879 - val_loss: 1.1369 - val_accuracy: 0.5956\n",
            "Epoch 225/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9391 - accuracy: 0.6889 - val_loss: 1.1367 - val_accuracy: 0.5904\n",
            "Epoch 226/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9464 - accuracy: 0.6735 - val_loss: 1.1358 - val_accuracy: 0.6102\n",
            "Epoch 227/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9377 - accuracy: 0.6868 - val_loss: 1.1314 - val_accuracy: 0.5956\n",
            "Epoch 228/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9266 - accuracy: 0.6909 - val_loss: 1.1275 - val_accuracy: 0.5998\n",
            "Epoch 229/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9325 - accuracy: 0.6725 - val_loss: 1.1227 - val_accuracy: 0.6050\n",
            "Epoch 230/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.9229 - accuracy: 0.6863 - val_loss: 1.1246 - val_accuracy: 0.5998\n",
            "Epoch 231/1100\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 0.9146 - accuracy: 0.6996 - val_loss: 1.1241 - val_accuracy: 0.6019\n",
            "Epoch 232/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 0.9246 - accuracy: 0.6899 - val_loss: 1.1188 - val_accuracy: 0.5936\n",
            "Epoch 233/1100\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 0.9163 - accuracy: 0.6894 - val_loss: 1.1209 - val_accuracy: 0.5977\n",
            "Epoch 234/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.9036 - accuracy: 0.7048 - val_loss: 1.1171 - val_accuracy: 0.6102\n",
            "Epoch 235/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.9392 - accuracy: 0.6899 - val_loss: 1.1154 - val_accuracy: 0.6040\n",
            "Epoch 236/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.9304 - accuracy: 0.6858 - val_loss: 1.1149 - val_accuracy: 0.6008\n",
            "Epoch 237/1100\n",
            "122/122 [==============================] - 1s 12ms/step - loss: 0.9128 - accuracy: 0.6920 - val_loss: 1.1155 - val_accuracy: 0.6143\n",
            "Epoch 238/1100\n",
            "122/122 [==============================] - 1s 12ms/step - loss: 0.9229 - accuracy: 0.6950 - val_loss: 1.1125 - val_accuracy: 0.6071\n",
            "Epoch 239/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.9040 - accuracy: 0.7012 - val_loss: 1.1084 - val_accuracy: 0.6143\n",
            "Epoch 240/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.9223 - accuracy: 0.6966 - val_loss: 1.1139 - val_accuracy: 0.6102\n",
            "Epoch 241/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.9031 - accuracy: 0.6966 - val_loss: 1.1110 - val_accuracy: 0.6237\n",
            "Epoch 242/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.8989 - accuracy: 0.6935 - val_loss: 1.1037 - val_accuracy: 0.6112\n",
            "Epoch 243/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.9008 - accuracy: 0.7043 - val_loss: 1.1011 - val_accuracy: 0.6133\n",
            "Epoch 244/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.9101 - accuracy: 0.6925 - val_loss: 1.1090 - val_accuracy: 0.6195\n",
            "Epoch 245/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.9039 - accuracy: 0.6879 - val_loss: 1.1035 - val_accuracy: 0.6112\n",
            "Epoch 246/1100\n",
            "122/122 [==============================] - 1s 12ms/step - loss: 0.9023 - accuracy: 0.6914 - val_loss: 1.1012 - val_accuracy: 0.6102\n",
            "Epoch 247/1100\n",
            "122/122 [==============================] - 1s 12ms/step - loss: 0.8949 - accuracy: 0.7043 - val_loss: 1.0957 - val_accuracy: 0.6154\n",
            "Epoch 248/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.8962 - accuracy: 0.7043 - val_loss: 1.1020 - val_accuracy: 0.6185\n",
            "Epoch 249/1100\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 0.8836 - accuracy: 0.7094 - val_loss: 1.0933 - val_accuracy: 0.6175\n",
            "Epoch 250/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 0.8928 - accuracy: 0.7068 - val_loss: 1.0918 - val_accuracy: 0.6164\n",
            "Epoch 251/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 0.8908 - accuracy: 0.6961 - val_loss: 1.0941 - val_accuracy: 0.6279\n",
            "Epoch 252/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.9043 - accuracy: 0.6991 - val_loss: 1.0907 - val_accuracy: 0.6268\n",
            "Epoch 253/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.8831 - accuracy: 0.7068 - val_loss: 1.0892 - val_accuracy: 0.6216\n",
            "Epoch 254/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.8826 - accuracy: 0.6981 - val_loss: 1.0855 - val_accuracy: 0.6133\n",
            "Epoch 255/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.8713 - accuracy: 0.7114 - val_loss: 1.0848 - val_accuracy: 0.6237\n",
            "Epoch 256/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 0.8553 - accuracy: 0.7099 - val_loss: 1.0794 - val_accuracy: 0.6247\n",
            "Epoch 257/1100\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 0.8570 - accuracy: 0.7191 - val_loss: 1.0803 - val_accuracy: 0.6258\n",
            "Epoch 258/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8696 - accuracy: 0.7027 - val_loss: 1.0787 - val_accuracy: 0.6331\n",
            "Epoch 259/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8681 - accuracy: 0.7048 - val_loss: 1.0747 - val_accuracy: 0.6279\n",
            "Epoch 260/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8561 - accuracy: 0.7155 - val_loss: 1.0739 - val_accuracy: 0.6227\n",
            "Epoch 261/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8815 - accuracy: 0.7032 - val_loss: 1.0762 - val_accuracy: 0.6320\n",
            "Epoch 262/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8820 - accuracy: 0.7094 - val_loss: 1.0715 - val_accuracy: 0.6320\n",
            "Epoch 263/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8560 - accuracy: 0.7155 - val_loss: 1.0685 - val_accuracy: 0.6279\n",
            "Epoch 264/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8587 - accuracy: 0.7135 - val_loss: 1.0735 - val_accuracy: 0.6289\n",
            "Epoch 265/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8407 - accuracy: 0.7078 - val_loss: 1.0686 - val_accuracy: 0.6227\n",
            "Epoch 266/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8634 - accuracy: 0.7171 - val_loss: 1.0666 - val_accuracy: 0.6351\n",
            "Epoch 267/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8487 - accuracy: 0.7212 - val_loss: 1.0648 - val_accuracy: 0.6289\n",
            "Epoch 268/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8325 - accuracy: 0.7227 - val_loss: 1.0719 - val_accuracy: 0.6216\n",
            "Epoch 269/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8521 - accuracy: 0.7053 - val_loss: 1.0607 - val_accuracy: 0.6351\n",
            "Epoch 270/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8395 - accuracy: 0.7150 - val_loss: 1.0631 - val_accuracy: 0.6414\n",
            "Epoch 271/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8338 - accuracy: 0.7258 - val_loss: 1.0576 - val_accuracy: 0.6403\n",
            "Epoch 272/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8374 - accuracy: 0.7242 - val_loss: 1.0531 - val_accuracy: 0.6424\n",
            "Epoch 273/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8514 - accuracy: 0.7063 - val_loss: 1.0561 - val_accuracy: 0.6362\n",
            "Epoch 274/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8439 - accuracy: 0.7232 - val_loss: 1.0547 - val_accuracy: 0.6310\n",
            "Epoch 275/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8267 - accuracy: 0.7294 - val_loss: 1.0505 - val_accuracy: 0.6372\n",
            "Epoch 276/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8436 - accuracy: 0.7222 - val_loss: 1.0523 - val_accuracy: 0.6341\n",
            "Epoch 277/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8363 - accuracy: 0.7130 - val_loss: 1.0497 - val_accuracy: 0.6393\n",
            "Epoch 278/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8311 - accuracy: 0.7340 - val_loss: 1.0509 - val_accuracy: 0.6497\n",
            "Epoch 279/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8309 - accuracy: 0.7278 - val_loss: 1.0456 - val_accuracy: 0.6497\n",
            "Epoch 280/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8290 - accuracy: 0.7207 - val_loss: 1.0441 - val_accuracy: 0.6435\n",
            "Epoch 281/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8190 - accuracy: 0.7381 - val_loss: 1.0427 - val_accuracy: 0.6497\n",
            "Epoch 282/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8054 - accuracy: 0.7340 - val_loss: 1.0439 - val_accuracy: 0.6414\n",
            "Epoch 283/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8116 - accuracy: 0.7345 - val_loss: 1.0373 - val_accuracy: 0.6486\n",
            "Epoch 284/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8311 - accuracy: 0.7222 - val_loss: 1.0404 - val_accuracy: 0.6424\n",
            "Epoch 285/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8092 - accuracy: 0.7330 - val_loss: 1.0368 - val_accuracy: 0.6476\n",
            "Epoch 286/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8037 - accuracy: 0.7324 - val_loss: 1.0339 - val_accuracy: 0.6455\n",
            "Epoch 287/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8264 - accuracy: 0.7324 - val_loss: 1.0362 - val_accuracy: 0.6435\n",
            "Epoch 288/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8046 - accuracy: 0.7360 - val_loss: 1.0339 - val_accuracy: 0.6466\n",
            "Epoch 289/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8275 - accuracy: 0.7253 - val_loss: 1.0351 - val_accuracy: 0.6570\n",
            "Epoch 290/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8133 - accuracy: 0.7283 - val_loss: 1.0294 - val_accuracy: 0.6528\n",
            "Epoch 291/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8058 - accuracy: 0.7186 - val_loss: 1.0278 - val_accuracy: 0.6476\n",
            "Epoch 292/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8126 - accuracy: 0.7324 - val_loss: 1.0262 - val_accuracy: 0.6497\n",
            "Epoch 293/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.8067 - accuracy: 0.7386 - val_loss: 1.0227 - val_accuracy: 0.6549\n",
            "Epoch 294/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7983 - accuracy: 0.7289 - val_loss: 1.0281 - val_accuracy: 0.6403\n",
            "Epoch 295/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7895 - accuracy: 0.7396 - val_loss: 1.0212 - val_accuracy: 0.6570\n",
            "Epoch 296/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.8082 - accuracy: 0.7319 - val_loss: 1.0170 - val_accuracy: 0.6590\n",
            "Epoch 297/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7937 - accuracy: 0.7417 - val_loss: 1.0173 - val_accuracy: 0.6653\n",
            "Epoch 298/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.7442 - val_loss: 1.0177 - val_accuracy: 0.6559\n",
            "Epoch 299/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.7376 - val_loss: 1.0174 - val_accuracy: 0.6570\n",
            "Epoch 300/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7965 - accuracy: 0.7365 - val_loss: 1.0138 - val_accuracy: 0.6549\n",
            "Epoch 301/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7962 - accuracy: 0.7376 - val_loss: 1.0137 - val_accuracy: 0.6590\n",
            "Epoch 302/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7963 - accuracy: 0.7309 - val_loss: 1.0095 - val_accuracy: 0.6549\n",
            "Epoch 303/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7868 - accuracy: 0.7463 - val_loss: 1.0140 - val_accuracy: 0.6622\n",
            "Epoch 304/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7780 - accuracy: 0.7488 - val_loss: 1.0098 - val_accuracy: 0.6601\n",
            "Epoch 305/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.7309 - val_loss: 1.0129 - val_accuracy: 0.6549\n",
            "Epoch 306/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7872 - accuracy: 0.7524 - val_loss: 1.0072 - val_accuracy: 0.6653\n",
            "Epoch 307/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7732 - accuracy: 0.7545 - val_loss: 1.0094 - val_accuracy: 0.6507\n",
            "Epoch 308/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7775 - accuracy: 0.7488 - val_loss: 1.0083 - val_accuracy: 0.6663\n",
            "Epoch 309/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7589 - accuracy: 0.7565 - val_loss: 1.0032 - val_accuracy: 0.6611\n",
            "Epoch 310/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7794 - accuracy: 0.7406 - val_loss: 1.0032 - val_accuracy: 0.6653\n",
            "Epoch 311/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7750 - accuracy: 0.7437 - val_loss: 0.9992 - val_accuracy: 0.6601\n",
            "Epoch 312/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7632 - accuracy: 0.7529 - val_loss: 0.9957 - val_accuracy: 0.6528\n",
            "Epoch 313/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7712 - accuracy: 0.7427 - val_loss: 0.9962 - val_accuracy: 0.6590\n",
            "Epoch 314/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7765 - accuracy: 0.7401 - val_loss: 0.9978 - val_accuracy: 0.6538\n",
            "Epoch 315/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7543 - accuracy: 0.7427 - val_loss: 1.0033 - val_accuracy: 0.6622\n",
            "Epoch 316/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7632 - accuracy: 0.7586 - val_loss: 0.9940 - val_accuracy: 0.6632\n",
            "Epoch 317/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7733 - accuracy: 0.7463 - val_loss: 0.9894 - val_accuracy: 0.6767\n",
            "Epoch 318/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7452 - accuracy: 0.7591 - val_loss: 0.9866 - val_accuracy: 0.6642\n",
            "Epoch 319/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7417 - accuracy: 0.7617 - val_loss: 0.9829 - val_accuracy: 0.6663\n",
            "Epoch 320/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7617 - accuracy: 0.7658 - val_loss: 0.9878 - val_accuracy: 0.6622\n",
            "Epoch 321/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7645 - accuracy: 0.7396 - val_loss: 0.9880 - val_accuracy: 0.6736\n",
            "Epoch 322/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7491 - accuracy: 0.7529 - val_loss: 0.9822 - val_accuracy: 0.6674\n",
            "Epoch 323/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7419 - accuracy: 0.7642 - val_loss: 0.9785 - val_accuracy: 0.6767\n",
            "Epoch 324/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7483 - accuracy: 0.7494 - val_loss: 0.9808 - val_accuracy: 0.6653\n",
            "Epoch 325/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7560 - accuracy: 0.7509 - val_loss: 0.9782 - val_accuracy: 0.6663\n",
            "Epoch 326/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7544 - accuracy: 0.7483 - val_loss: 0.9770 - val_accuracy: 0.6674\n",
            "Epoch 327/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7450 - accuracy: 0.7550 - val_loss: 0.9758 - val_accuracy: 0.6778\n",
            "Epoch 328/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7624 - accuracy: 0.7458 - val_loss: 0.9753 - val_accuracy: 0.6694\n",
            "Epoch 329/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7321 - accuracy: 0.7632 - val_loss: 0.9802 - val_accuracy: 0.6736\n",
            "Epoch 330/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7484 - accuracy: 0.7447 - val_loss: 0.9698 - val_accuracy: 0.6778\n",
            "Epoch 331/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7378 - accuracy: 0.7693 - val_loss: 0.9671 - val_accuracy: 0.6757\n",
            "Epoch 332/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7443 - accuracy: 0.7524 - val_loss: 0.9670 - val_accuracy: 0.6694\n",
            "Epoch 333/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7442 - accuracy: 0.7514 - val_loss: 0.9738 - val_accuracy: 0.6663\n",
            "Epoch 334/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7339 - accuracy: 0.7519 - val_loss: 0.9673 - val_accuracy: 0.6819\n",
            "Epoch 335/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7397 - accuracy: 0.7617 - val_loss: 0.9629 - val_accuracy: 0.6767\n",
            "Epoch 336/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7233 - accuracy: 0.7642 - val_loss: 0.9654 - val_accuracy: 0.6726\n",
            "Epoch 337/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7316 - accuracy: 0.7586 - val_loss: 0.9674 - val_accuracy: 0.6674\n",
            "Epoch 338/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7425 - accuracy: 0.7565 - val_loss: 0.9567 - val_accuracy: 0.6746\n",
            "Epoch 339/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7289 - accuracy: 0.7555 - val_loss: 0.9564 - val_accuracy: 0.6830\n",
            "Epoch 340/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7144 - accuracy: 0.7632 - val_loss: 0.9572 - val_accuracy: 0.6840\n",
            "Epoch 341/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7095 - accuracy: 0.7688 - val_loss: 0.9554 - val_accuracy: 0.6684\n",
            "Epoch 342/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7327 - accuracy: 0.7622 - val_loss: 0.9609 - val_accuracy: 0.6881\n",
            "Epoch 343/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7229 - accuracy: 0.7591 - val_loss: 0.9545 - val_accuracy: 0.6788\n",
            "Epoch 344/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7152 - accuracy: 0.7678 - val_loss: 0.9523 - val_accuracy: 0.6809\n",
            "Epoch 345/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7023 - accuracy: 0.7704 - val_loss: 0.9485 - val_accuracy: 0.6830\n",
            "Epoch 346/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7160 - accuracy: 0.7596 - val_loss: 0.9474 - val_accuracy: 0.6746\n",
            "Epoch 347/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7134 - accuracy: 0.7627 - val_loss: 0.9512 - val_accuracy: 0.6902\n",
            "Epoch 348/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7233 - accuracy: 0.7642 - val_loss: 0.9489 - val_accuracy: 0.6902\n",
            "Epoch 349/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7163 - accuracy: 0.7668 - val_loss: 0.9452 - val_accuracy: 0.6954\n",
            "Epoch 350/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7069 - accuracy: 0.7745 - val_loss: 0.9453 - val_accuracy: 0.6923\n",
            "Epoch 351/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7040 - accuracy: 0.7683 - val_loss: 0.9434 - val_accuracy: 0.6965\n",
            "Epoch 352/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7196 - accuracy: 0.7519 - val_loss: 0.9440 - val_accuracy: 0.6819\n",
            "Epoch 353/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7133 - accuracy: 0.7601 - val_loss: 0.9410 - val_accuracy: 0.6850\n",
            "Epoch 354/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.7106 - accuracy: 0.7683 - val_loss: 0.9370 - val_accuracy: 0.6881\n",
            "Epoch 355/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6989 - accuracy: 0.7683 - val_loss: 0.9370 - val_accuracy: 0.6881\n",
            "Epoch 356/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6994 - accuracy: 0.7714 - val_loss: 0.9367 - val_accuracy: 0.6788\n",
            "Epoch 357/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7020 - accuracy: 0.7775 - val_loss: 0.9356 - val_accuracy: 0.6902\n",
            "Epoch 358/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6961 - accuracy: 0.7760 - val_loss: 0.9335 - val_accuracy: 0.6881\n",
            "Epoch 359/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6987 - accuracy: 0.7683 - val_loss: 0.9347 - val_accuracy: 0.6881\n",
            "Epoch 360/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7032 - accuracy: 0.7714 - val_loss: 0.9337 - val_accuracy: 0.6746\n",
            "Epoch 361/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.7005 - accuracy: 0.7688 - val_loss: 0.9340 - val_accuracy: 0.6830\n",
            "Epoch 362/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6883 - accuracy: 0.7704 - val_loss: 0.9304 - val_accuracy: 0.6809\n",
            "Epoch 363/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6782 - accuracy: 0.7786 - val_loss: 0.9260 - val_accuracy: 0.6944\n",
            "Epoch 364/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6836 - accuracy: 0.7699 - val_loss: 0.9237 - val_accuracy: 0.6892\n",
            "Epoch 365/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6883 - accuracy: 0.7719 - val_loss: 0.9275 - val_accuracy: 0.6985\n",
            "Epoch 366/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6951 - accuracy: 0.7714 - val_loss: 0.9224 - val_accuracy: 0.7017\n",
            "Epoch 367/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6651 - accuracy: 0.7832 - val_loss: 0.9284 - val_accuracy: 0.6985\n",
            "Epoch 368/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6874 - accuracy: 0.7652 - val_loss: 0.9165 - val_accuracy: 0.6975\n",
            "Epoch 369/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6861 - accuracy: 0.7791 - val_loss: 0.9137 - val_accuracy: 0.6954\n",
            "Epoch 370/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6849 - accuracy: 0.7740 - val_loss: 0.9187 - val_accuracy: 0.6975\n",
            "Epoch 371/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6935 - accuracy: 0.7596 - val_loss: 0.9123 - val_accuracy: 0.6975\n",
            "Epoch 372/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6767 - accuracy: 0.7893 - val_loss: 0.9158 - val_accuracy: 0.6913\n",
            "Epoch 373/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6620 - accuracy: 0.7837 - val_loss: 0.9154 - val_accuracy: 0.7017\n",
            "Epoch 374/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.7683 - val_loss: 0.9104 - val_accuracy: 0.7006\n",
            "Epoch 375/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6728 - accuracy: 0.7760 - val_loss: 0.9082 - val_accuracy: 0.6861\n",
            "Epoch 376/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6718 - accuracy: 0.7852 - val_loss: 0.9091 - val_accuracy: 0.6944\n",
            "Epoch 377/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6675 - accuracy: 0.7775 - val_loss: 0.9086 - val_accuracy: 0.6985\n",
            "Epoch 378/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6727 - accuracy: 0.7842 - val_loss: 0.9075 - val_accuracy: 0.7017\n",
            "Epoch 379/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6661 - accuracy: 0.7801 - val_loss: 0.9060 - val_accuracy: 0.7048\n",
            "Epoch 380/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6511 - accuracy: 0.7924 - val_loss: 0.9072 - val_accuracy: 0.7069\n",
            "Epoch 381/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6672 - accuracy: 0.7806 - val_loss: 0.9046 - val_accuracy: 0.7006\n",
            "Epoch 382/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.7827 - val_loss: 0.9045 - val_accuracy: 0.7037\n",
            "Epoch 383/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6697 - accuracy: 0.7858 - val_loss: 0.9000 - val_accuracy: 0.6975\n",
            "Epoch 384/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6319 - accuracy: 0.7924 - val_loss: 0.8982 - val_accuracy: 0.6996\n",
            "Epoch 385/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6483 - accuracy: 0.7945 - val_loss: 0.9053 - val_accuracy: 0.6985\n",
            "Epoch 386/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6672 - accuracy: 0.7755 - val_loss: 0.9014 - val_accuracy: 0.7037\n",
            "Epoch 387/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6792 - accuracy: 0.7801 - val_loss: 0.9048 - val_accuracy: 0.7110\n",
            "Epoch 388/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6608 - accuracy: 0.7909 - val_loss: 0.8974 - val_accuracy: 0.7017\n",
            "Epoch 389/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6487 - accuracy: 0.7811 - val_loss: 0.8960 - val_accuracy: 0.6954\n",
            "Epoch 390/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6438 - accuracy: 0.8006 - val_loss: 0.8927 - val_accuracy: 0.7037\n",
            "Epoch 391/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6381 - accuracy: 0.7924 - val_loss: 0.8908 - val_accuracy: 0.6985\n",
            "Epoch 392/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.8027 - val_loss: 0.8921 - val_accuracy: 0.7048\n",
            "Epoch 393/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6406 - accuracy: 0.7868 - val_loss: 0.8858 - val_accuracy: 0.7069\n",
            "Epoch 394/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6369 - accuracy: 0.7960 - val_loss: 0.8871 - val_accuracy: 0.7100\n",
            "Epoch 395/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6284 - accuracy: 0.7863 - val_loss: 0.8840 - val_accuracy: 0.7058\n",
            "Epoch 396/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6392 - accuracy: 0.7904 - val_loss: 0.8844 - val_accuracy: 0.7079\n",
            "Epoch 397/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6380 - accuracy: 0.7893 - val_loss: 0.8792 - val_accuracy: 0.7006\n",
            "Epoch 398/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6519 - accuracy: 0.7740 - val_loss: 0.8837 - val_accuracy: 0.7089\n",
            "Epoch 399/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6172 - accuracy: 0.7986 - val_loss: 0.8833 - val_accuracy: 0.7100\n",
            "Epoch 400/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.7842 - val_loss: 0.8812 - val_accuracy: 0.7152\n",
            "Epoch 401/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6347 - accuracy: 0.7945 - val_loss: 0.8833 - val_accuracy: 0.7037\n",
            "Epoch 402/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6239 - accuracy: 0.8073 - val_loss: 0.8752 - val_accuracy: 0.7110\n",
            "Epoch 403/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6303 - accuracy: 0.7934 - val_loss: 0.8844 - val_accuracy: 0.7100\n",
            "Epoch 404/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6275 - accuracy: 0.8057 - val_loss: 0.8753 - val_accuracy: 0.7110\n",
            "Epoch 405/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6364 - accuracy: 0.7858 - val_loss: 0.8754 - val_accuracy: 0.7204\n",
            "Epoch 406/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6182 - accuracy: 0.8042 - val_loss: 0.8787 - val_accuracy: 0.7037\n",
            "Epoch 407/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6277 - accuracy: 0.7878 - val_loss: 0.8769 - val_accuracy: 0.7152\n",
            "Epoch 408/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6156 - accuracy: 0.8057 - val_loss: 0.8711 - val_accuracy: 0.7225\n",
            "Epoch 409/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6426 - accuracy: 0.7934 - val_loss: 0.8754 - val_accuracy: 0.7214\n",
            "Epoch 410/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6050 - accuracy: 0.8068 - val_loss: 0.8703 - val_accuracy: 0.7121\n",
            "Epoch 411/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6008 - accuracy: 0.8042 - val_loss: 0.8717 - val_accuracy: 0.7204\n",
            "Epoch 412/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6331 - accuracy: 0.7929 - val_loss: 0.8702 - val_accuracy: 0.7162\n",
            "Epoch 413/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6232 - accuracy: 0.8063 - val_loss: 0.8679 - val_accuracy: 0.7121\n",
            "Epoch 414/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6119 - accuracy: 0.8001 - val_loss: 0.8641 - val_accuracy: 0.7256\n",
            "Epoch 415/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6069 - accuracy: 0.7960 - val_loss: 0.8661 - val_accuracy: 0.7100\n",
            "Epoch 416/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6163 - accuracy: 0.7996 - val_loss: 0.8608 - val_accuracy: 0.7266\n",
            "Epoch 417/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6182 - accuracy: 0.8001 - val_loss: 0.8645 - val_accuracy: 0.7131\n",
            "Epoch 418/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6050 - accuracy: 0.8042 - val_loss: 0.8605 - val_accuracy: 0.7183\n",
            "Epoch 419/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6144 - accuracy: 0.8073 - val_loss: 0.8610 - val_accuracy: 0.7245\n",
            "Epoch 420/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6159 - accuracy: 0.7919 - val_loss: 0.8556 - val_accuracy: 0.7214\n",
            "Epoch 421/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6273 - accuracy: 0.8027 - val_loss: 0.8556 - val_accuracy: 0.7141\n",
            "Epoch 422/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5891 - accuracy: 0.8006 - val_loss: 0.8609 - val_accuracy: 0.7162\n",
            "Epoch 423/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6056 - accuracy: 0.8047 - val_loss: 0.8602 - val_accuracy: 0.7266\n",
            "Epoch 424/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6019 - accuracy: 0.8006 - val_loss: 0.8533 - val_accuracy: 0.7245\n",
            "Epoch 425/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5797 - accuracy: 0.8165 - val_loss: 0.8489 - val_accuracy: 0.7339\n",
            "Epoch 426/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.6185 - accuracy: 0.7945 - val_loss: 0.8470 - val_accuracy: 0.7308\n",
            "Epoch 427/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5892 - accuracy: 0.7975 - val_loss: 0.8480 - val_accuracy: 0.7266\n",
            "Epoch 428/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6119 - accuracy: 0.8042 - val_loss: 0.8504 - val_accuracy: 0.7245\n",
            "Epoch 429/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5851 - accuracy: 0.8109 - val_loss: 0.8464 - val_accuracy: 0.7266\n",
            "Epoch 430/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5989 - accuracy: 0.8037 - val_loss: 0.8453 - val_accuracy: 0.7235\n",
            "Epoch 431/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5946 - accuracy: 0.8047 - val_loss: 0.8463 - val_accuracy: 0.7152\n",
            "Epoch 432/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5759 - accuracy: 0.8273 - val_loss: 0.8514 - val_accuracy: 0.7173\n",
            "Epoch 433/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5824 - accuracy: 0.8052 - val_loss: 0.8459 - val_accuracy: 0.7245\n",
            "Epoch 434/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6011 - accuracy: 0.8052 - val_loss: 0.8450 - val_accuracy: 0.7204\n",
            "Epoch 435/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5782 - accuracy: 0.8191 - val_loss: 0.8437 - val_accuracy: 0.7214\n",
            "Epoch 436/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5836 - accuracy: 0.8165 - val_loss: 0.8430 - val_accuracy: 0.7360\n",
            "Epoch 437/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.6016 - accuracy: 0.8011 - val_loss: 0.8383 - val_accuracy: 0.7401\n",
            "Epoch 438/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5876 - accuracy: 0.8145 - val_loss: 0.8390 - val_accuracy: 0.7193\n",
            "Epoch 439/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5656 - accuracy: 0.8155 - val_loss: 0.8402 - val_accuracy: 0.7339\n",
            "Epoch 440/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5894 - accuracy: 0.8160 - val_loss: 0.8353 - val_accuracy: 0.7412\n",
            "Epoch 441/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5645 - accuracy: 0.8211 - val_loss: 0.8317 - val_accuracy: 0.7339\n",
            "Epoch 442/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5787 - accuracy: 0.8196 - val_loss: 0.8306 - val_accuracy: 0.7349\n",
            "Epoch 443/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5626 - accuracy: 0.8139 - val_loss: 0.8363 - val_accuracy: 0.7225\n",
            "Epoch 444/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5544 - accuracy: 0.8237 - val_loss: 0.8290 - val_accuracy: 0.7287\n",
            "Epoch 445/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5724 - accuracy: 0.8186 - val_loss: 0.8273 - val_accuracy: 0.7235\n",
            "Epoch 446/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5762 - accuracy: 0.8032 - val_loss: 0.8324 - val_accuracy: 0.7339\n",
            "Epoch 447/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5671 - accuracy: 0.8278 - val_loss: 0.8295 - val_accuracy: 0.7391\n",
            "Epoch 448/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5761 - accuracy: 0.8227 - val_loss: 0.8240 - val_accuracy: 0.7297\n",
            "Epoch 449/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5711 - accuracy: 0.8119 - val_loss: 0.8229 - val_accuracy: 0.7308\n",
            "Epoch 450/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5573 - accuracy: 0.8216 - val_loss: 0.8223 - val_accuracy: 0.7266\n",
            "Epoch 451/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5835 - accuracy: 0.8057 - val_loss: 0.8231 - val_accuracy: 0.7277\n",
            "Epoch 452/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5755 - accuracy: 0.8073 - val_loss: 0.8278 - val_accuracy: 0.7432\n",
            "Epoch 453/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5577 - accuracy: 0.8247 - val_loss: 0.8175 - val_accuracy: 0.7225\n",
            "Epoch 454/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5518 - accuracy: 0.8191 - val_loss: 0.8202 - val_accuracy: 0.7412\n",
            "Epoch 455/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5709 - accuracy: 0.8134 - val_loss: 0.8154 - val_accuracy: 0.7349\n",
            "Epoch 456/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5724 - accuracy: 0.8216 - val_loss: 0.8160 - val_accuracy: 0.7380\n",
            "Epoch 457/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5462 - accuracy: 0.8216 - val_loss: 0.8206 - val_accuracy: 0.7464\n",
            "Epoch 458/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5573 - accuracy: 0.8319 - val_loss: 0.8130 - val_accuracy: 0.7370\n",
            "Epoch 459/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5715 - accuracy: 0.8221 - val_loss: 0.8186 - val_accuracy: 0.7256\n",
            "Epoch 460/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5610 - accuracy: 0.8211 - val_loss: 0.8148 - val_accuracy: 0.7339\n",
            "Epoch 461/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5597 - accuracy: 0.8083 - val_loss: 0.8092 - val_accuracy: 0.7370\n",
            "Epoch 462/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5446 - accuracy: 0.8252 - val_loss: 0.8114 - val_accuracy: 0.7339\n",
            "Epoch 463/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5608 - accuracy: 0.8180 - val_loss: 0.8156 - val_accuracy: 0.7360\n",
            "Epoch 464/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5707 - accuracy: 0.8145 - val_loss: 0.8093 - val_accuracy: 0.7422\n",
            "Epoch 465/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5570 - accuracy: 0.8283 - val_loss: 0.8045 - val_accuracy: 0.7505\n",
            "Epoch 466/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5516 - accuracy: 0.8206 - val_loss: 0.8057 - val_accuracy: 0.7318\n",
            "Epoch 467/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5491 - accuracy: 0.8324 - val_loss: 0.8035 - val_accuracy: 0.7401\n",
            "Epoch 468/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5467 - accuracy: 0.8232 - val_loss: 0.8029 - val_accuracy: 0.7349\n",
            "Epoch 469/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5556 - accuracy: 0.8119 - val_loss: 0.8009 - val_accuracy: 0.7474\n",
            "Epoch 470/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5426 - accuracy: 0.8262 - val_loss: 0.8022 - val_accuracy: 0.7464\n",
            "Epoch 471/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5540 - accuracy: 0.8237 - val_loss: 0.8039 - val_accuracy: 0.7401\n",
            "Epoch 472/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5408 - accuracy: 0.8303 - val_loss: 0.7994 - val_accuracy: 0.7360\n",
            "Epoch 473/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5456 - accuracy: 0.8268 - val_loss: 0.7993 - val_accuracy: 0.7432\n",
            "Epoch 474/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5320 - accuracy: 0.8339 - val_loss: 0.8032 - val_accuracy: 0.7401\n",
            "Epoch 475/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5308 - accuracy: 0.8339 - val_loss: 0.8000 - val_accuracy: 0.7526\n",
            "Epoch 476/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 0.5416 - accuracy: 0.8186 - val_loss: 0.7961 - val_accuracy: 0.7505\n",
            "Epoch 477/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5468 - accuracy: 0.8283 - val_loss: 0.8064 - val_accuracy: 0.7536\n",
            "Epoch 478/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5426 - accuracy: 0.8288 - val_loss: 0.7985 - val_accuracy: 0.7380\n",
            "Epoch 479/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5376 - accuracy: 0.8303 - val_loss: 0.7915 - val_accuracy: 0.7547\n",
            "Epoch 480/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5389 - accuracy: 0.8334 - val_loss: 0.7917 - val_accuracy: 0.7547\n",
            "Epoch 481/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5486 - accuracy: 0.8216 - val_loss: 0.7982 - val_accuracy: 0.7495\n",
            "Epoch 482/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5133 - accuracy: 0.8324 - val_loss: 0.7919 - val_accuracy: 0.7443\n",
            "Epoch 483/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5434 - accuracy: 0.8298 - val_loss: 0.7881 - val_accuracy: 0.7505\n",
            "Epoch 484/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5303 - accuracy: 0.8273 - val_loss: 0.7870 - val_accuracy: 0.7453\n",
            "Epoch 485/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5198 - accuracy: 0.8324 - val_loss: 0.7878 - val_accuracy: 0.7474\n",
            "Epoch 486/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5195 - accuracy: 0.8370 - val_loss: 0.7878 - val_accuracy: 0.7568\n",
            "Epoch 487/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5202 - accuracy: 0.8334 - val_loss: 0.7852 - val_accuracy: 0.7547\n",
            "Epoch 488/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5435 - accuracy: 0.8252 - val_loss: 0.7877 - val_accuracy: 0.7432\n",
            "Epoch 489/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5067 - accuracy: 0.8401 - val_loss: 0.7842 - val_accuracy: 0.7557\n",
            "Epoch 490/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5188 - accuracy: 0.8329 - val_loss: 0.7847 - val_accuracy: 0.7640\n",
            "Epoch 491/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5178 - accuracy: 0.8344 - val_loss: 0.7863 - val_accuracy: 0.7391\n",
            "Epoch 492/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5226 - accuracy: 0.8324 - val_loss: 0.7911 - val_accuracy: 0.7453\n",
            "Epoch 493/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5207 - accuracy: 0.8278 - val_loss: 0.7788 - val_accuracy: 0.7620\n",
            "Epoch 494/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5049 - accuracy: 0.8396 - val_loss: 0.7799 - val_accuracy: 0.7568\n",
            "Epoch 495/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5205 - accuracy: 0.8293 - val_loss: 0.7766 - val_accuracy: 0.7505\n",
            "Epoch 496/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5235 - accuracy: 0.8406 - val_loss: 0.7731 - val_accuracy: 0.7609\n",
            "Epoch 497/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5210 - accuracy: 0.8370 - val_loss: 0.7806 - val_accuracy: 0.7453\n",
            "Epoch 498/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5105 - accuracy: 0.8391 - val_loss: 0.7776 - val_accuracy: 0.7464\n",
            "Epoch 499/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5156 - accuracy: 0.8350 - val_loss: 0.7722 - val_accuracy: 0.7588\n",
            "Epoch 500/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5100 - accuracy: 0.8355 - val_loss: 0.7736 - val_accuracy: 0.7516\n",
            "Epoch 501/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5220 - accuracy: 0.8273 - val_loss: 0.7712 - val_accuracy: 0.7547\n",
            "Epoch 502/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4897 - accuracy: 0.8421 - val_loss: 0.7692 - val_accuracy: 0.7588\n",
            "Epoch 503/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5057 - accuracy: 0.8401 - val_loss: 0.7669 - val_accuracy: 0.7620\n",
            "Epoch 504/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5239 - accuracy: 0.8303 - val_loss: 0.7696 - val_accuracy: 0.7599\n",
            "Epoch 505/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.8411 - val_loss: 0.7650 - val_accuracy: 0.7682\n",
            "Epoch 506/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4997 - accuracy: 0.8411 - val_loss: 0.7726 - val_accuracy: 0.7526\n",
            "Epoch 507/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5158 - accuracy: 0.8344 - val_loss: 0.7671 - val_accuracy: 0.7630\n",
            "Epoch 508/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5032 - accuracy: 0.8488 - val_loss: 0.7656 - val_accuracy: 0.7651\n",
            "Epoch 509/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4859 - accuracy: 0.8534 - val_loss: 0.7711 - val_accuracy: 0.7505\n",
            "Epoch 510/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4976 - accuracy: 0.8432 - val_loss: 0.7632 - val_accuracy: 0.7516\n",
            "Epoch 511/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.8339 - val_loss: 0.7613 - val_accuracy: 0.7640\n",
            "Epoch 512/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5060 - accuracy: 0.8406 - val_loss: 0.7658 - val_accuracy: 0.7609\n",
            "Epoch 513/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5185 - accuracy: 0.8283 - val_loss: 0.7724 - val_accuracy: 0.7557\n",
            "Epoch 514/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4783 - accuracy: 0.8467 - val_loss: 0.7622 - val_accuracy: 0.7620\n",
            "Epoch 515/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4923 - accuracy: 0.8457 - val_loss: 0.7628 - val_accuracy: 0.7588\n",
            "Epoch 516/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5127 - accuracy: 0.8462 - val_loss: 0.7574 - val_accuracy: 0.7640\n",
            "Epoch 517/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4914 - accuracy: 0.8514 - val_loss: 0.7731 - val_accuracy: 0.7630\n",
            "Epoch 518/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5064 - accuracy: 0.8344 - val_loss: 0.7597 - val_accuracy: 0.7682\n",
            "Epoch 519/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4775 - accuracy: 0.8508 - val_loss: 0.7646 - val_accuracy: 0.7609\n",
            "Epoch 520/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4905 - accuracy: 0.8365 - val_loss: 0.7521 - val_accuracy: 0.7557\n",
            "Epoch 521/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4846 - accuracy: 0.8498 - val_loss: 0.7534 - val_accuracy: 0.7651\n",
            "Epoch 522/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5071 - accuracy: 0.8344 - val_loss: 0.7550 - val_accuracy: 0.7755\n",
            "Epoch 523/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4765 - accuracy: 0.8503 - val_loss: 0.7515 - val_accuracy: 0.7588\n",
            "Epoch 524/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4867 - accuracy: 0.8462 - val_loss: 0.7571 - val_accuracy: 0.7703\n",
            "Epoch 525/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.5059 - accuracy: 0.8411 - val_loss: 0.7539 - val_accuracy: 0.7723\n",
            "Epoch 526/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4929 - accuracy: 0.8467 - val_loss: 0.7533 - val_accuracy: 0.7723\n",
            "Epoch 527/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4905 - accuracy: 0.8452 - val_loss: 0.7474 - val_accuracy: 0.7609\n",
            "Epoch 528/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4768 - accuracy: 0.8488 - val_loss: 0.7534 - val_accuracy: 0.7744\n",
            "Epoch 529/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4885 - accuracy: 0.8401 - val_loss: 0.7469 - val_accuracy: 0.7651\n",
            "Epoch 530/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4738 - accuracy: 0.8339 - val_loss: 0.7507 - val_accuracy: 0.7609\n",
            "Epoch 531/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4853 - accuracy: 0.8473 - val_loss: 0.7461 - val_accuracy: 0.7765\n",
            "Epoch 532/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4684 - accuracy: 0.8524 - val_loss: 0.7463 - val_accuracy: 0.7630\n",
            "Epoch 533/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4814 - accuracy: 0.8478 - val_loss: 0.7433 - val_accuracy: 0.7692\n",
            "Epoch 534/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4619 - accuracy: 0.8529 - val_loss: 0.7447 - val_accuracy: 0.7588\n",
            "Epoch 535/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4690 - accuracy: 0.8457 - val_loss: 0.7407 - val_accuracy: 0.7796\n",
            "Epoch 536/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4640 - accuracy: 0.8580 - val_loss: 0.7405 - val_accuracy: 0.7630\n",
            "Epoch 537/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4734 - accuracy: 0.8462 - val_loss: 0.7407 - val_accuracy: 0.7775\n",
            "Epoch 538/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4753 - accuracy: 0.8488 - val_loss: 0.7342 - val_accuracy: 0.7765\n",
            "Epoch 539/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4737 - accuracy: 0.8539 - val_loss: 0.7417 - val_accuracy: 0.7703\n",
            "Epoch 540/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4622 - accuracy: 0.8544 - val_loss: 0.7399 - val_accuracy: 0.7796\n",
            "Epoch 541/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4761 - accuracy: 0.8457 - val_loss: 0.7384 - val_accuracy: 0.7640\n",
            "Epoch 542/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4539 - accuracy: 0.8539 - val_loss: 0.7384 - val_accuracy: 0.7744\n",
            "Epoch 543/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4701 - accuracy: 0.8483 - val_loss: 0.7377 - val_accuracy: 0.7827\n",
            "Epoch 544/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4560 - accuracy: 0.8642 - val_loss: 0.7319 - val_accuracy: 0.7755\n",
            "Epoch 545/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4913 - accuracy: 0.8309 - val_loss: 0.7365 - val_accuracy: 0.7765\n",
            "Epoch 546/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4662 - accuracy: 0.8544 - val_loss: 0.7350 - val_accuracy: 0.7765\n",
            "Epoch 547/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4582 - accuracy: 0.8555 - val_loss: 0.7271 - val_accuracy: 0.7817\n",
            "Epoch 548/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.4464 - accuracy: 0.8555 - val_loss: 0.7298 - val_accuracy: 0.7703\n",
            "Epoch 549/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4728 - accuracy: 0.8503 - val_loss: 0.7367 - val_accuracy: 0.7807\n",
            "Epoch 550/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4753 - accuracy: 0.8508 - val_loss: 0.7329 - val_accuracy: 0.7807\n",
            "Epoch 551/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4774 - accuracy: 0.8437 - val_loss: 0.7310 - val_accuracy: 0.7713\n",
            "Epoch 552/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4732 - accuracy: 0.8462 - val_loss: 0.7294 - val_accuracy: 0.7723\n",
            "Epoch 553/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4543 - accuracy: 0.8524 - val_loss: 0.7265 - val_accuracy: 0.7817\n",
            "Epoch 554/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4603 - accuracy: 0.8585 - val_loss: 0.7242 - val_accuracy: 0.7765\n",
            "Epoch 555/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4614 - accuracy: 0.8549 - val_loss: 0.7209 - val_accuracy: 0.7827\n",
            "Epoch 556/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4549 - accuracy: 0.8462 - val_loss: 0.7212 - val_accuracy: 0.7744\n",
            "Epoch 557/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4766 - accuracy: 0.8478 - val_loss: 0.7247 - val_accuracy: 0.7786\n",
            "Epoch 558/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4621 - accuracy: 0.8503 - val_loss: 0.7229 - val_accuracy: 0.7838\n",
            "Epoch 559/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4450 - accuracy: 0.8590 - val_loss: 0.7200 - val_accuracy: 0.7817\n",
            "Epoch 560/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4447 - accuracy: 0.8678 - val_loss: 0.7211 - val_accuracy: 0.7827\n",
            "Epoch 561/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4422 - accuracy: 0.8652 - val_loss: 0.7201 - val_accuracy: 0.7786\n",
            "Epoch 562/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4622 - accuracy: 0.8539 - val_loss: 0.7165 - val_accuracy: 0.7848\n",
            "Epoch 563/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4350 - accuracy: 0.8616 - val_loss: 0.7247 - val_accuracy: 0.7879\n",
            "Epoch 564/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4330 - accuracy: 0.8616 - val_loss: 0.7204 - val_accuracy: 0.7900\n",
            "Epoch 565/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4581 - accuracy: 0.8498 - val_loss: 0.7166 - val_accuracy: 0.7838\n",
            "Epoch 566/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4557 - accuracy: 0.8503 - val_loss: 0.7186 - val_accuracy: 0.7723\n",
            "Epoch 567/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4439 - accuracy: 0.8596 - val_loss: 0.7175 - val_accuracy: 0.7786\n",
            "Epoch 568/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4371 - accuracy: 0.8621 - val_loss: 0.7155 - val_accuracy: 0.7827\n",
            "Epoch 569/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4283 - accuracy: 0.8693 - val_loss: 0.7188 - val_accuracy: 0.7921\n",
            "Epoch 570/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4626 - accuracy: 0.8580 - val_loss: 0.7188 - val_accuracy: 0.7807\n",
            "Epoch 571/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4274 - accuracy: 0.8678 - val_loss: 0.7209 - val_accuracy: 0.7734\n",
            "Epoch 572/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4392 - accuracy: 0.8667 - val_loss: 0.7137 - val_accuracy: 0.7879\n",
            "Epoch 573/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4355 - accuracy: 0.8601 - val_loss: 0.7179 - val_accuracy: 0.7765\n",
            "Epoch 574/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4477 - accuracy: 0.8621 - val_loss: 0.7133 - val_accuracy: 0.7765\n",
            "Epoch 575/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4343 - accuracy: 0.8575 - val_loss: 0.7153 - val_accuracy: 0.7817\n",
            "Epoch 576/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4454 - accuracy: 0.8498 - val_loss: 0.7076 - val_accuracy: 0.7983\n",
            "Epoch 577/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4159 - accuracy: 0.8708 - val_loss: 0.7082 - val_accuracy: 0.7921\n",
            "Epoch 578/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4414 - accuracy: 0.8560 - val_loss: 0.7053 - val_accuracy: 0.7796\n",
            "Epoch 579/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4413 - accuracy: 0.8575 - val_loss: 0.7086 - val_accuracy: 0.7879\n",
            "Epoch 580/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4306 - accuracy: 0.8626 - val_loss: 0.7077 - val_accuracy: 0.7786\n",
            "Epoch 581/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4382 - accuracy: 0.8616 - val_loss: 0.7044 - val_accuracy: 0.7869\n",
            "Epoch 582/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4259 - accuracy: 0.8580 - val_loss: 0.7035 - val_accuracy: 0.7890\n",
            "Epoch 583/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4228 - accuracy: 0.8693 - val_loss: 0.7029 - val_accuracy: 0.7848\n",
            "Epoch 584/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4240 - accuracy: 0.8698 - val_loss: 0.7068 - val_accuracy: 0.7911\n",
            "Epoch 585/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4327 - accuracy: 0.8611 - val_loss: 0.7005 - val_accuracy: 0.7879\n",
            "Epoch 586/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.8611 - val_loss: 0.7019 - val_accuracy: 0.7890\n",
            "Epoch 587/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4369 - accuracy: 0.8534 - val_loss: 0.7009 - val_accuracy: 0.8015\n",
            "Epoch 588/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4296 - accuracy: 0.8611 - val_loss: 0.7004 - val_accuracy: 0.7848\n",
            "Epoch 589/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4160 - accuracy: 0.8683 - val_loss: 0.7020 - val_accuracy: 0.7827\n",
            "Epoch 590/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4319 - accuracy: 0.8606 - val_loss: 0.6994 - val_accuracy: 0.7879\n",
            "Epoch 591/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4306 - accuracy: 0.8637 - val_loss: 0.6974 - val_accuracy: 0.7963\n",
            "Epoch 592/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4388 - accuracy: 0.8657 - val_loss: 0.7039 - val_accuracy: 0.8015\n",
            "Epoch 593/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4458 - accuracy: 0.8657 - val_loss: 0.6948 - val_accuracy: 0.7911\n",
            "Epoch 594/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4324 - accuracy: 0.8667 - val_loss: 0.6956 - val_accuracy: 0.7973\n",
            "Epoch 595/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4132 - accuracy: 0.8647 - val_loss: 0.6979 - val_accuracy: 0.7869\n",
            "Epoch 596/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4185 - accuracy: 0.8657 - val_loss: 0.6966 - val_accuracy: 0.7973\n",
            "Epoch 597/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3996 - accuracy: 0.8780 - val_loss: 0.6962 - val_accuracy: 0.7983\n",
            "Epoch 598/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4186 - accuracy: 0.8708 - val_loss: 0.6914 - val_accuracy: 0.7921\n",
            "Epoch 599/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4105 - accuracy: 0.8683 - val_loss: 0.6959 - val_accuracy: 0.7983\n",
            "Epoch 600/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4094 - accuracy: 0.8754 - val_loss: 0.6961 - val_accuracy: 0.7942\n",
            "Epoch 601/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4160 - accuracy: 0.8647 - val_loss: 0.6935 - val_accuracy: 0.7973\n",
            "Epoch 602/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4429 - accuracy: 0.8549 - val_loss: 0.6909 - val_accuracy: 0.7994\n",
            "Epoch 603/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4450 - accuracy: 0.8549 - val_loss: 0.6915 - val_accuracy: 0.8056\n",
            "Epoch 604/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4146 - accuracy: 0.8642 - val_loss: 0.6878 - val_accuracy: 0.7952\n",
            "Epoch 605/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4139 - accuracy: 0.8713 - val_loss: 0.6888 - val_accuracy: 0.8015\n",
            "Epoch 606/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4059 - accuracy: 0.8739 - val_loss: 0.6891 - val_accuracy: 0.7952\n",
            "Epoch 607/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4122 - accuracy: 0.8652 - val_loss: 0.6901 - val_accuracy: 0.7963\n",
            "Epoch 608/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4083 - accuracy: 0.8734 - val_loss: 0.6896 - val_accuracy: 0.7963\n",
            "Epoch 609/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4142 - accuracy: 0.8637 - val_loss: 0.6830 - val_accuracy: 0.7942\n",
            "Epoch 610/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4200 - accuracy: 0.8565 - val_loss: 0.6915 - val_accuracy: 0.7942\n",
            "Epoch 611/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4171 - accuracy: 0.8616 - val_loss: 0.6936 - val_accuracy: 0.7890\n",
            "Epoch 612/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4175 - accuracy: 0.8688 - val_loss: 0.6834 - val_accuracy: 0.7973\n",
            "Epoch 613/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3983 - accuracy: 0.8836 - val_loss: 0.6845 - val_accuracy: 0.7952\n",
            "Epoch 614/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4120 - accuracy: 0.8708 - val_loss: 0.6844 - val_accuracy: 0.8035\n",
            "Epoch 615/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4128 - accuracy: 0.8749 - val_loss: 0.6831 - val_accuracy: 0.7994\n",
            "Epoch 616/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4051 - accuracy: 0.8760 - val_loss: 0.6858 - val_accuracy: 0.8046\n",
            "Epoch 617/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4131 - accuracy: 0.8724 - val_loss: 0.6906 - val_accuracy: 0.7973\n",
            "Epoch 618/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4088 - accuracy: 0.8693 - val_loss: 0.6756 - val_accuracy: 0.7994\n",
            "Epoch 619/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4012 - accuracy: 0.8719 - val_loss: 0.6853 - val_accuracy: 0.7921\n",
            "Epoch 620/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3985 - accuracy: 0.8749 - val_loss: 0.6834 - val_accuracy: 0.7973\n",
            "Epoch 621/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4072 - accuracy: 0.8667 - val_loss: 0.6784 - val_accuracy: 0.8025\n",
            "Epoch 622/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4063 - accuracy: 0.8780 - val_loss: 0.6767 - val_accuracy: 0.8015\n",
            "Epoch 623/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3962 - accuracy: 0.8749 - val_loss: 0.6785 - val_accuracy: 0.8046\n",
            "Epoch 624/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4011 - accuracy: 0.8688 - val_loss: 0.6772 - val_accuracy: 0.8108\n",
            "Epoch 625/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4172 - accuracy: 0.8647 - val_loss: 0.6746 - val_accuracy: 0.8015\n",
            "Epoch 626/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3927 - accuracy: 0.8729 - val_loss: 0.6689 - val_accuracy: 0.7952\n",
            "Epoch 627/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3835 - accuracy: 0.8831 - val_loss: 0.6776 - val_accuracy: 0.8056\n",
            "Epoch 628/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3849 - accuracy: 0.8811 - val_loss: 0.6690 - val_accuracy: 0.8077\n",
            "Epoch 629/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3899 - accuracy: 0.8739 - val_loss: 0.6721 - val_accuracy: 0.8046\n",
            "Epoch 630/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3941 - accuracy: 0.8780 - val_loss: 0.6792 - val_accuracy: 0.7931\n",
            "Epoch 631/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3969 - accuracy: 0.8754 - val_loss: 0.6753 - val_accuracy: 0.8108\n",
            "Epoch 632/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3997 - accuracy: 0.8729 - val_loss: 0.6683 - val_accuracy: 0.8077\n",
            "Epoch 633/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4152 - accuracy: 0.8652 - val_loss: 0.6730 - val_accuracy: 0.8087\n",
            "Epoch 634/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3812 - accuracy: 0.8816 - val_loss: 0.6737 - val_accuracy: 0.8108\n",
            "Epoch 635/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.4134 - accuracy: 0.8637 - val_loss: 0.6687 - val_accuracy: 0.8035\n",
            "Epoch 636/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3840 - accuracy: 0.8785 - val_loss: 0.6730 - val_accuracy: 0.8077\n",
            "Epoch 637/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3757 - accuracy: 0.8780 - val_loss: 0.6703 - val_accuracy: 0.7994\n",
            "Epoch 638/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4065 - accuracy: 0.8672 - val_loss: 0.6687 - val_accuracy: 0.8181\n",
            "Epoch 639/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3659 - accuracy: 0.8872 - val_loss: 0.6723 - val_accuracy: 0.8098\n",
            "Epoch 640/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3807 - accuracy: 0.8872 - val_loss: 0.6697 - val_accuracy: 0.8046\n",
            "Epoch 641/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3990 - accuracy: 0.8662 - val_loss: 0.6631 - val_accuracy: 0.8108\n",
            "Epoch 642/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3805 - accuracy: 0.8816 - val_loss: 0.6612 - val_accuracy: 0.8098\n",
            "Epoch 643/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3809 - accuracy: 0.8795 - val_loss: 0.6724 - val_accuracy: 0.8108\n",
            "Epoch 644/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3868 - accuracy: 0.8760 - val_loss: 0.6673 - val_accuracy: 0.8150\n",
            "Epoch 645/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8688 - val_loss: 0.6606 - val_accuracy: 0.8056\n",
            "Epoch 646/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3964 - accuracy: 0.8760 - val_loss: 0.6627 - val_accuracy: 0.8108\n",
            "Epoch 647/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3776 - accuracy: 0.8888 - val_loss: 0.6570 - val_accuracy: 0.8108\n",
            "Epoch 648/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3851 - accuracy: 0.8806 - val_loss: 0.6633 - val_accuracy: 0.8119\n",
            "Epoch 649/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3757 - accuracy: 0.8826 - val_loss: 0.6659 - val_accuracy: 0.8150\n",
            "Epoch 650/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3810 - accuracy: 0.8836 - val_loss: 0.6614 - val_accuracy: 0.8087\n",
            "Epoch 651/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3684 - accuracy: 0.8857 - val_loss: 0.6591 - val_accuracy: 0.8077\n",
            "Epoch 652/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3782 - accuracy: 0.8806 - val_loss: 0.6629 - val_accuracy: 0.8056\n",
            "Epoch 653/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3858 - accuracy: 0.8836 - val_loss: 0.6607 - val_accuracy: 0.8160\n",
            "Epoch 654/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3689 - accuracy: 0.8811 - val_loss: 0.6599 - val_accuracy: 0.8139\n",
            "Epoch 655/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.4013 - accuracy: 0.8703 - val_loss: 0.6600 - val_accuracy: 0.8150\n",
            "Epoch 656/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3733 - accuracy: 0.8770 - val_loss: 0.6628 - val_accuracy: 0.7994\n",
            "Epoch 657/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3763 - accuracy: 0.8765 - val_loss: 0.6534 - val_accuracy: 0.8098\n",
            "Epoch 658/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3876 - accuracy: 0.8724 - val_loss: 0.6545 - val_accuracy: 0.8139\n",
            "Epoch 659/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3693 - accuracy: 0.8888 - val_loss: 0.6547 - val_accuracy: 0.8108\n",
            "Epoch 660/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3712 - accuracy: 0.8821 - val_loss: 0.6552 - val_accuracy: 0.8087\n",
            "Epoch 661/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3732 - accuracy: 0.8826 - val_loss: 0.6593 - val_accuracy: 0.8108\n",
            "Epoch 662/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3735 - accuracy: 0.8842 - val_loss: 0.6623 - val_accuracy: 0.8139\n",
            "Epoch 663/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3723 - accuracy: 0.8857 - val_loss: 0.6533 - val_accuracy: 0.8170\n",
            "Epoch 664/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3766 - accuracy: 0.8770 - val_loss: 0.6553 - val_accuracy: 0.8056\n",
            "Epoch 665/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3647 - accuracy: 0.8867 - val_loss: 0.6528 - val_accuracy: 0.8160\n",
            "Epoch 666/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3865 - accuracy: 0.8806 - val_loss: 0.6477 - val_accuracy: 0.8191\n",
            "Epoch 667/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3708 - accuracy: 0.8770 - val_loss: 0.6546 - val_accuracy: 0.8181\n",
            "Epoch 668/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3632 - accuracy: 0.8944 - val_loss: 0.6560 - val_accuracy: 0.8202\n",
            "Epoch 669/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3592 - accuracy: 0.8934 - val_loss: 0.6462 - val_accuracy: 0.8191\n",
            "Epoch 670/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3686 - accuracy: 0.8831 - val_loss: 0.6446 - val_accuracy: 0.8222\n",
            "Epoch 671/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3703 - accuracy: 0.8775 - val_loss: 0.6509 - val_accuracy: 0.8170\n",
            "Epoch 672/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3741 - accuracy: 0.8852 - val_loss: 0.6534 - val_accuracy: 0.8170\n",
            "Epoch 673/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3704 - accuracy: 0.8908 - val_loss: 0.6465 - val_accuracy: 0.8212\n",
            "Epoch 674/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3687 - accuracy: 0.8795 - val_loss: 0.6430 - val_accuracy: 0.8202\n",
            "Epoch 675/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3649 - accuracy: 0.8888 - val_loss: 0.6479 - val_accuracy: 0.8160\n",
            "Epoch 676/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3652 - accuracy: 0.8857 - val_loss: 0.6464 - val_accuracy: 0.8202\n",
            "Epoch 677/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3579 - accuracy: 0.8867 - val_loss: 0.6501 - val_accuracy: 0.8202\n",
            "Epoch 678/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3673 - accuracy: 0.8867 - val_loss: 0.6484 - val_accuracy: 0.8160\n",
            "Epoch 679/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3642 - accuracy: 0.8765 - val_loss: 0.6481 - val_accuracy: 0.8181\n",
            "Epoch 680/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3895 - accuracy: 0.8801 - val_loss: 0.6427 - val_accuracy: 0.8181\n",
            "Epoch 681/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3585 - accuracy: 0.8801 - val_loss: 0.6544 - val_accuracy: 0.8129\n",
            "Epoch 682/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3707 - accuracy: 0.8780 - val_loss: 0.6439 - val_accuracy: 0.8202\n",
            "Epoch 683/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3593 - accuracy: 0.8816 - val_loss: 0.6395 - val_accuracy: 0.8181\n",
            "Epoch 684/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3583 - accuracy: 0.8888 - val_loss: 0.6426 - val_accuracy: 0.8150\n",
            "Epoch 685/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3589 - accuracy: 0.8903 - val_loss: 0.6511 - val_accuracy: 0.8098\n",
            "Epoch 686/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3571 - accuracy: 0.8862 - val_loss: 0.6389 - val_accuracy: 0.8108\n",
            "Epoch 687/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3793 - accuracy: 0.8765 - val_loss: 0.6412 - val_accuracy: 0.8264\n",
            "Epoch 688/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3623 - accuracy: 0.8883 - val_loss: 0.6436 - val_accuracy: 0.8191\n",
            "Epoch 689/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3561 - accuracy: 0.8811 - val_loss: 0.6517 - val_accuracy: 0.8202\n",
            "Epoch 690/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3543 - accuracy: 0.8924 - val_loss: 0.6462 - val_accuracy: 0.8160\n",
            "Epoch 691/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3489 - accuracy: 0.8862 - val_loss: 0.6374 - val_accuracy: 0.8181\n",
            "Epoch 692/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3560 - accuracy: 0.8888 - val_loss: 0.6355 - val_accuracy: 0.8181\n",
            "Epoch 693/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3595 - accuracy: 0.8919 - val_loss: 0.6357 - val_accuracy: 0.8139\n",
            "Epoch 694/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3444 - accuracy: 0.8898 - val_loss: 0.6380 - val_accuracy: 0.8316\n",
            "Epoch 695/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3396 - accuracy: 0.8872 - val_loss: 0.6349 - val_accuracy: 0.8233\n",
            "Epoch 696/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3519 - accuracy: 0.8924 - val_loss: 0.6355 - val_accuracy: 0.8191\n",
            "Epoch 697/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3511 - accuracy: 0.8919 - val_loss: 0.6357 - val_accuracy: 0.8233\n",
            "Epoch 698/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3344 - accuracy: 0.8965 - val_loss: 0.6354 - val_accuracy: 0.8150\n",
            "Epoch 699/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3490 - accuracy: 0.8826 - val_loss: 0.6230 - val_accuracy: 0.8254\n",
            "Epoch 700/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3556 - accuracy: 0.8924 - val_loss: 0.6318 - val_accuracy: 0.8160\n",
            "Epoch 701/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3620 - accuracy: 0.8842 - val_loss: 0.6312 - val_accuracy: 0.8202\n",
            "Epoch 702/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3625 - accuracy: 0.8883 - val_loss: 0.6332 - val_accuracy: 0.8191\n",
            "Epoch 703/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3602 - accuracy: 0.8816 - val_loss: 0.6276 - val_accuracy: 0.8316\n",
            "Epoch 704/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3558 - accuracy: 0.8836 - val_loss: 0.6337 - val_accuracy: 0.8212\n",
            "Epoch 705/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3414 - accuracy: 0.8919 - val_loss: 0.6342 - val_accuracy: 0.8233\n",
            "Epoch 706/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3389 - accuracy: 0.8919 - val_loss: 0.6367 - val_accuracy: 0.8254\n",
            "Epoch 707/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3387 - accuracy: 0.8949 - val_loss: 0.6284 - val_accuracy: 0.8295\n",
            "Epoch 708/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3567 - accuracy: 0.8847 - val_loss: 0.6378 - val_accuracy: 0.8254\n",
            "Epoch 709/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3562 - accuracy: 0.8939 - val_loss: 0.6319 - val_accuracy: 0.8191\n",
            "Epoch 710/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3468 - accuracy: 0.8924 - val_loss: 0.6337 - val_accuracy: 0.8160\n",
            "Epoch 711/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3219 - accuracy: 0.9011 - val_loss: 0.6301 - val_accuracy: 0.8233\n",
            "Epoch 712/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3336 - accuracy: 0.8970 - val_loss: 0.6275 - val_accuracy: 0.8274\n",
            "Epoch 713/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3350 - accuracy: 0.9036 - val_loss: 0.6273 - val_accuracy: 0.8274\n",
            "Epoch 714/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3280 - accuracy: 0.8934 - val_loss: 0.6277 - val_accuracy: 0.8212\n",
            "Epoch 715/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3285 - accuracy: 0.9006 - val_loss: 0.6255 - val_accuracy: 0.8222\n",
            "Epoch 716/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3261 - accuracy: 0.8934 - val_loss: 0.6300 - val_accuracy: 0.8285\n",
            "Epoch 717/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3477 - accuracy: 0.8913 - val_loss: 0.6218 - val_accuracy: 0.8337\n",
            "Epoch 718/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3402 - accuracy: 0.8939 - val_loss: 0.6261 - val_accuracy: 0.8222\n",
            "Epoch 719/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3404 - accuracy: 0.8872 - val_loss: 0.6251 - val_accuracy: 0.8222\n",
            "Epoch 720/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3478 - accuracy: 0.8816 - val_loss: 0.6259 - val_accuracy: 0.8191\n",
            "Epoch 721/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3525 - accuracy: 0.8801 - val_loss: 0.6325 - val_accuracy: 0.8243\n",
            "Epoch 722/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3452 - accuracy: 0.8960 - val_loss: 0.6238 - val_accuracy: 0.8243\n",
            "Epoch 723/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3203 - accuracy: 0.9067 - val_loss: 0.6306 - val_accuracy: 0.8264\n",
            "Epoch 724/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3196 - accuracy: 0.8949 - val_loss: 0.6252 - val_accuracy: 0.8264\n",
            "Epoch 725/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3154 - accuracy: 0.8985 - val_loss: 0.6225 - val_accuracy: 0.8191\n",
            "Epoch 726/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3369 - accuracy: 0.8898 - val_loss: 0.6208 - val_accuracy: 0.8243\n",
            "Epoch 727/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3509 - accuracy: 0.8862 - val_loss: 0.6210 - val_accuracy: 0.8233\n",
            "Epoch 728/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3328 - accuracy: 0.8990 - val_loss: 0.6266 - val_accuracy: 0.8222\n",
            "Epoch 729/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3324 - accuracy: 0.8919 - val_loss: 0.6238 - val_accuracy: 0.8181\n",
            "Epoch 730/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3329 - accuracy: 0.8970 - val_loss: 0.6239 - val_accuracy: 0.8212\n",
            "Epoch 731/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3223 - accuracy: 0.8990 - val_loss: 0.6223 - val_accuracy: 0.8254\n",
            "Epoch 732/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3290 - accuracy: 0.8995 - val_loss: 0.6197 - val_accuracy: 0.8254\n",
            "Epoch 733/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3352 - accuracy: 0.8883 - val_loss: 0.6185 - val_accuracy: 0.8160\n",
            "Epoch 734/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3051 - accuracy: 0.9057 - val_loss: 0.6162 - val_accuracy: 0.8212\n",
            "Epoch 735/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3349 - accuracy: 0.8898 - val_loss: 0.6196 - val_accuracy: 0.8295\n",
            "Epoch 736/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3226 - accuracy: 0.8990 - val_loss: 0.6163 - val_accuracy: 0.8316\n",
            "Epoch 737/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3323 - accuracy: 0.8934 - val_loss: 0.6163 - val_accuracy: 0.8306\n",
            "Epoch 738/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3366 - accuracy: 0.8975 - val_loss: 0.6206 - val_accuracy: 0.8326\n",
            "Epoch 739/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3178 - accuracy: 0.9016 - val_loss: 0.6156 - val_accuracy: 0.8264\n",
            "Epoch 740/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3262 - accuracy: 0.8980 - val_loss: 0.6153 - val_accuracy: 0.8181\n",
            "Epoch 741/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3296 - accuracy: 0.8965 - val_loss: 0.6160 - val_accuracy: 0.8264\n",
            "Epoch 742/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3180 - accuracy: 0.8980 - val_loss: 0.6112 - val_accuracy: 0.8274\n",
            "Epoch 743/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3173 - accuracy: 0.8995 - val_loss: 0.6111 - val_accuracy: 0.8254\n",
            "Epoch 744/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3279 - accuracy: 0.9016 - val_loss: 0.6184 - val_accuracy: 0.8222\n",
            "Epoch 745/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3316 - accuracy: 0.8934 - val_loss: 0.6165 - val_accuracy: 0.8233\n",
            "Epoch 746/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3148 - accuracy: 0.9021 - val_loss: 0.6204 - val_accuracy: 0.8160\n",
            "Epoch 747/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3395 - accuracy: 0.8898 - val_loss: 0.6173 - val_accuracy: 0.8233\n",
            "Epoch 748/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3169 - accuracy: 0.9062 - val_loss: 0.6134 - val_accuracy: 0.8222\n",
            "Epoch 749/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3140 - accuracy: 0.9036 - val_loss: 0.6245 - val_accuracy: 0.8274\n",
            "Epoch 750/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3178 - accuracy: 0.8934 - val_loss: 0.6164 - val_accuracy: 0.8191\n",
            "Epoch 751/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3240 - accuracy: 0.8944 - val_loss: 0.6134 - val_accuracy: 0.8306\n",
            "Epoch 752/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3146 - accuracy: 0.9021 - val_loss: 0.6063 - val_accuracy: 0.8274\n",
            "Epoch 753/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3123 - accuracy: 0.9072 - val_loss: 0.6117 - val_accuracy: 0.8222\n",
            "Epoch 754/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3208 - accuracy: 0.8985 - val_loss: 0.6124 - val_accuracy: 0.8316\n",
            "Epoch 755/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3162 - accuracy: 0.8985 - val_loss: 0.6100 - val_accuracy: 0.8264\n",
            "Epoch 756/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3282 - accuracy: 0.9026 - val_loss: 0.6104 - val_accuracy: 0.8389\n",
            "Epoch 757/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3226 - accuracy: 0.9042 - val_loss: 0.6207 - val_accuracy: 0.8326\n",
            "Epoch 758/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3114 - accuracy: 0.9042 - val_loss: 0.6122 - val_accuracy: 0.8295\n",
            "Epoch 759/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3277 - accuracy: 0.8924 - val_loss: 0.6130 - val_accuracy: 0.8389\n",
            "Epoch 760/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3140 - accuracy: 0.9016 - val_loss: 0.6069 - val_accuracy: 0.8295\n",
            "Epoch 761/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3225 - accuracy: 0.9031 - val_loss: 0.6087 - val_accuracy: 0.8181\n",
            "Epoch 762/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3285 - accuracy: 0.8929 - val_loss: 0.6040 - val_accuracy: 0.8274\n",
            "Epoch 763/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2920 - accuracy: 0.9077 - val_loss: 0.6039 - val_accuracy: 0.8326\n",
            "Epoch 764/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3248 - accuracy: 0.8949 - val_loss: 0.6141 - val_accuracy: 0.8368\n",
            "Epoch 765/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2824 - accuracy: 0.9149 - val_loss: 0.6023 - val_accuracy: 0.8378\n",
            "Epoch 766/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3348 - accuracy: 0.8898 - val_loss: 0.5982 - val_accuracy: 0.8306\n",
            "Epoch 767/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3209 - accuracy: 0.8919 - val_loss: 0.6086 - val_accuracy: 0.8358\n",
            "Epoch 768/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3098 - accuracy: 0.8970 - val_loss: 0.6111 - val_accuracy: 0.8243\n",
            "Epoch 769/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3123 - accuracy: 0.8990 - val_loss: 0.6126 - val_accuracy: 0.8264\n",
            "Epoch 770/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3247 - accuracy: 0.8913 - val_loss: 0.6108 - val_accuracy: 0.8368\n",
            "Epoch 771/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3105 - accuracy: 0.9067 - val_loss: 0.6024 - val_accuracy: 0.8233\n",
            "Epoch 772/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3124 - accuracy: 0.9031 - val_loss: 0.6061 - val_accuracy: 0.8285\n",
            "Epoch 773/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3124 - accuracy: 0.8980 - val_loss: 0.6063 - val_accuracy: 0.8399\n",
            "Epoch 774/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3205 - accuracy: 0.9016 - val_loss: 0.6000 - val_accuracy: 0.8368\n",
            "Epoch 775/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3000 - accuracy: 0.9098 - val_loss: 0.6094 - val_accuracy: 0.8368\n",
            "Epoch 776/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3007 - accuracy: 0.9047 - val_loss: 0.6086 - val_accuracy: 0.8326\n",
            "Epoch 777/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2979 - accuracy: 0.9067 - val_loss: 0.6052 - val_accuracy: 0.8399\n",
            "Epoch 778/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3049 - accuracy: 0.9031 - val_loss: 0.6016 - val_accuracy: 0.8358\n",
            "Epoch 779/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3118 - accuracy: 0.9021 - val_loss: 0.6034 - val_accuracy: 0.8316\n",
            "Epoch 780/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3278 - accuracy: 0.8913 - val_loss: 0.5998 - val_accuracy: 0.8295\n",
            "Epoch 781/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3029 - accuracy: 0.9067 - val_loss: 0.6001 - val_accuracy: 0.8378\n",
            "Epoch 782/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3065 - accuracy: 0.9036 - val_loss: 0.5965 - val_accuracy: 0.8368\n",
            "Epoch 783/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2849 - accuracy: 0.9144 - val_loss: 0.5947 - val_accuracy: 0.8347\n",
            "Epoch 784/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2954 - accuracy: 0.9093 - val_loss: 0.5986 - val_accuracy: 0.8274\n",
            "Epoch 785/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3068 - accuracy: 0.9011 - val_loss: 0.5994 - val_accuracy: 0.8347\n",
            "Epoch 786/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3068 - accuracy: 0.9057 - val_loss: 0.6002 - val_accuracy: 0.8368\n",
            "Epoch 787/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3077 - accuracy: 0.9057 - val_loss: 0.6019 - val_accuracy: 0.8358\n",
            "Epoch 788/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2880 - accuracy: 0.9139 - val_loss: 0.6015 - val_accuracy: 0.8306\n",
            "Epoch 789/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3097 - accuracy: 0.8995 - val_loss: 0.6059 - val_accuracy: 0.8378\n",
            "Epoch 790/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3063 - accuracy: 0.9062 - val_loss: 0.5997 - val_accuracy: 0.8306\n",
            "Epoch 791/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3031 - accuracy: 0.9021 - val_loss: 0.6032 - val_accuracy: 0.8264\n",
            "Epoch 792/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3040 - accuracy: 0.9031 - val_loss: 0.5965 - val_accuracy: 0.8347\n",
            "Epoch 793/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2941 - accuracy: 0.9072 - val_loss: 0.5974 - val_accuracy: 0.8358\n",
            "Epoch 794/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2912 - accuracy: 0.9077 - val_loss: 0.5966 - val_accuracy: 0.8326\n",
            "Epoch 795/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2822 - accuracy: 0.9083 - val_loss: 0.5999 - val_accuracy: 0.8337\n",
            "Epoch 796/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3078 - accuracy: 0.9011 - val_loss: 0.5979 - val_accuracy: 0.8378\n",
            "Epoch 797/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2844 - accuracy: 0.9088 - val_loss: 0.5897 - val_accuracy: 0.8368\n",
            "Epoch 798/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3016 - accuracy: 0.9001 - val_loss: 0.5931 - val_accuracy: 0.8337\n",
            "Epoch 799/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2971 - accuracy: 0.9118 - val_loss: 0.6005 - val_accuracy: 0.8358\n",
            "Epoch 800/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2934 - accuracy: 0.9077 - val_loss: 0.5903 - val_accuracy: 0.8399\n",
            "Epoch 801/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2925 - accuracy: 0.9083 - val_loss: 0.5974 - val_accuracy: 0.8347\n",
            "Epoch 802/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2903 - accuracy: 0.9083 - val_loss: 0.5917 - val_accuracy: 0.8347\n",
            "Epoch 803/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2842 - accuracy: 0.9103 - val_loss: 0.5901 - val_accuracy: 0.8316\n",
            "Epoch 804/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2964 - accuracy: 0.9103 - val_loss: 0.5885 - val_accuracy: 0.8316\n",
            "Epoch 805/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2875 - accuracy: 0.9077 - val_loss: 0.5906 - val_accuracy: 0.8316\n",
            "Epoch 806/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2866 - accuracy: 0.9118 - val_loss: 0.5926 - val_accuracy: 0.8482\n",
            "Epoch 807/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2968 - accuracy: 0.9011 - val_loss: 0.5863 - val_accuracy: 0.8368\n",
            "Epoch 808/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2771 - accuracy: 0.9159 - val_loss: 0.5825 - val_accuracy: 0.8306\n",
            "Epoch 809/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.3018 - accuracy: 0.9036 - val_loss: 0.5844 - val_accuracy: 0.8295\n",
            "Epoch 810/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2847 - accuracy: 0.9072 - val_loss: 0.5918 - val_accuracy: 0.8420\n",
            "Epoch 811/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2942 - accuracy: 0.9077 - val_loss: 0.5926 - val_accuracy: 0.8441\n",
            "Epoch 812/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2937 - accuracy: 0.9077 - val_loss: 0.5848 - val_accuracy: 0.8410\n",
            "Epoch 813/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3129 - accuracy: 0.8888 - val_loss: 0.6001 - val_accuracy: 0.8493\n",
            "Epoch 814/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2716 - accuracy: 0.9144 - val_loss: 0.5910 - val_accuracy: 0.8462\n",
            "Epoch 815/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2866 - accuracy: 0.9077 - val_loss: 0.5872 - val_accuracy: 0.8306\n",
            "Epoch 816/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2910 - accuracy: 0.9057 - val_loss: 0.5797 - val_accuracy: 0.8462\n",
            "Epoch 817/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2773 - accuracy: 0.9144 - val_loss: 0.5869 - val_accuracy: 0.8368\n",
            "Epoch 818/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2757 - accuracy: 0.9124 - val_loss: 0.5832 - val_accuracy: 0.8347\n",
            "Epoch 819/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2769 - accuracy: 0.9124 - val_loss: 0.5849 - val_accuracy: 0.8389\n",
            "Epoch 820/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2970 - accuracy: 0.9016 - val_loss: 0.5939 - val_accuracy: 0.8368\n",
            "Epoch 821/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2936 - accuracy: 0.9088 - val_loss: 0.5838 - val_accuracy: 0.8410\n",
            "Epoch 822/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.3012 - accuracy: 0.8970 - val_loss: 0.5899 - val_accuracy: 0.8410\n",
            "Epoch 823/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2998 - accuracy: 0.9170 - val_loss: 0.5833 - val_accuracy: 0.8358\n",
            "Epoch 824/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2954 - accuracy: 0.9052 - val_loss: 0.5930 - val_accuracy: 0.8420\n",
            "Epoch 825/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2731 - accuracy: 0.9113 - val_loss: 0.5812 - val_accuracy: 0.8347\n",
            "Epoch 826/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2866 - accuracy: 0.9098 - val_loss: 0.5980 - val_accuracy: 0.8389\n",
            "Epoch 827/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2714 - accuracy: 0.9139 - val_loss: 0.5959 - val_accuracy: 0.8389\n",
            "Epoch 828/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2557 - accuracy: 0.9216 - val_loss: 0.5829 - val_accuracy: 0.8378\n",
            "Epoch 829/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2798 - accuracy: 0.9083 - val_loss: 0.5822 - val_accuracy: 0.8389\n",
            "Epoch 830/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2836 - accuracy: 0.9108 - val_loss: 0.5849 - val_accuracy: 0.8378\n",
            "Epoch 831/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2794 - accuracy: 0.9072 - val_loss: 0.5849 - val_accuracy: 0.8378\n",
            "Epoch 832/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2830 - accuracy: 0.9129 - val_loss: 0.5803 - val_accuracy: 0.8430\n",
            "Epoch 833/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2764 - accuracy: 0.9175 - val_loss: 0.5761 - val_accuracy: 0.8420\n",
            "Epoch 834/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2656 - accuracy: 0.9180 - val_loss: 0.5753 - val_accuracy: 0.8462\n",
            "Epoch 835/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2829 - accuracy: 0.9144 - val_loss: 0.5861 - val_accuracy: 0.8441\n",
            "Epoch 836/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2785 - accuracy: 0.9134 - val_loss: 0.5813 - val_accuracy: 0.8368\n",
            "Epoch 837/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2603 - accuracy: 0.9231 - val_loss: 0.5782 - val_accuracy: 0.8347\n",
            "Epoch 838/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2577 - accuracy: 0.9185 - val_loss: 0.5783 - val_accuracy: 0.8378\n",
            "Epoch 839/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2863 - accuracy: 0.9139 - val_loss: 0.5767 - val_accuracy: 0.8441\n",
            "Epoch 840/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2871 - accuracy: 0.9093 - val_loss: 0.5814 - val_accuracy: 0.8410\n",
            "Epoch 841/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2790 - accuracy: 0.9083 - val_loss: 0.5818 - val_accuracy: 0.8389\n",
            "Epoch 842/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2648 - accuracy: 0.9211 - val_loss: 0.5794 - val_accuracy: 0.8399\n",
            "Epoch 843/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2800 - accuracy: 0.9077 - val_loss: 0.5763 - val_accuracy: 0.8514\n",
            "Epoch 844/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2719 - accuracy: 0.9103 - val_loss: 0.5835 - val_accuracy: 0.8493\n",
            "Epoch 845/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2822 - accuracy: 0.9134 - val_loss: 0.5723 - val_accuracy: 0.8462\n",
            "Epoch 846/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2671 - accuracy: 0.9149 - val_loss: 0.5799 - val_accuracy: 0.8399\n",
            "Epoch 847/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2692 - accuracy: 0.9211 - val_loss: 0.5778 - val_accuracy: 0.8389\n",
            "Epoch 848/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2609 - accuracy: 0.9154 - val_loss: 0.5722 - val_accuracy: 0.8430\n",
            "Epoch 849/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2673 - accuracy: 0.9144 - val_loss: 0.5747 - val_accuracy: 0.8430\n",
            "Epoch 850/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2786 - accuracy: 0.9134 - val_loss: 0.5705 - val_accuracy: 0.8503\n",
            "Epoch 851/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2643 - accuracy: 0.9236 - val_loss: 0.5728 - val_accuracy: 0.8472\n",
            "Epoch 852/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2778 - accuracy: 0.9124 - val_loss: 0.5739 - val_accuracy: 0.8482\n",
            "Epoch 853/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2650 - accuracy: 0.9113 - val_loss: 0.5686 - val_accuracy: 0.8493\n",
            "Epoch 854/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2783 - accuracy: 0.9144 - val_loss: 0.5728 - val_accuracy: 0.8410\n",
            "Epoch 855/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2617 - accuracy: 0.9206 - val_loss: 0.5759 - val_accuracy: 0.8430\n",
            "Epoch 856/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2597 - accuracy: 0.9190 - val_loss: 0.5795 - val_accuracy: 0.8378\n",
            "Epoch 857/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2600 - accuracy: 0.9221 - val_loss: 0.5769 - val_accuracy: 0.8503\n",
            "Epoch 858/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2614 - accuracy: 0.9159 - val_loss: 0.5684 - val_accuracy: 0.8462\n",
            "Epoch 859/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2601 - accuracy: 0.9221 - val_loss: 0.5770 - val_accuracy: 0.8514\n",
            "Epoch 860/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2703 - accuracy: 0.9093 - val_loss: 0.5656 - val_accuracy: 0.8410\n",
            "Epoch 861/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2605 - accuracy: 0.9144 - val_loss: 0.5796 - val_accuracy: 0.8441\n",
            "Epoch 862/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2640 - accuracy: 0.9159 - val_loss: 0.5638 - val_accuracy: 0.8441\n",
            "Epoch 863/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2693 - accuracy: 0.9175 - val_loss: 0.5613 - val_accuracy: 0.8430\n",
            "Epoch 864/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2660 - accuracy: 0.9170 - val_loss: 0.5644 - val_accuracy: 0.8503\n",
            "Epoch 865/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2631 - accuracy: 0.9211 - val_loss: 0.5758 - val_accuracy: 0.8493\n",
            "Epoch 866/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2569 - accuracy: 0.9190 - val_loss: 0.5756 - val_accuracy: 0.8534\n",
            "Epoch 867/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2756 - accuracy: 0.9057 - val_loss: 0.5805 - val_accuracy: 0.8451\n",
            "Epoch 868/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2692 - accuracy: 0.9113 - val_loss: 0.5738 - val_accuracy: 0.8347\n",
            "Epoch 869/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2494 - accuracy: 0.9231 - val_loss: 0.5657 - val_accuracy: 0.8493\n",
            "Epoch 870/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2599 - accuracy: 0.9211 - val_loss: 0.5664 - val_accuracy: 0.8472\n",
            "Epoch 871/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2630 - accuracy: 0.9206 - val_loss: 0.5717 - val_accuracy: 0.8503\n",
            "Epoch 872/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2657 - accuracy: 0.9129 - val_loss: 0.5679 - val_accuracy: 0.8420\n",
            "Epoch 873/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2697 - accuracy: 0.9175 - val_loss: 0.5797 - val_accuracy: 0.8378\n",
            "Epoch 874/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2578 - accuracy: 0.9195 - val_loss: 0.5683 - val_accuracy: 0.8430\n",
            "Epoch 875/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2796 - accuracy: 0.9052 - val_loss: 0.5682 - val_accuracy: 0.8451\n",
            "Epoch 876/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2727 - accuracy: 0.9149 - val_loss: 0.5698 - val_accuracy: 0.8503\n",
            "Epoch 877/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2702 - accuracy: 0.9124 - val_loss: 0.5634 - val_accuracy: 0.8482\n",
            "Epoch 878/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2517 - accuracy: 0.9190 - val_loss: 0.5684 - val_accuracy: 0.8399\n",
            "Epoch 879/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2634 - accuracy: 0.9139 - val_loss: 0.5733 - val_accuracy: 0.8482\n",
            "Epoch 880/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2669 - accuracy: 0.9170 - val_loss: 0.5684 - val_accuracy: 0.8482\n",
            "Epoch 881/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2530 - accuracy: 0.9175 - val_loss: 0.5660 - val_accuracy: 0.8451\n",
            "Epoch 882/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2602 - accuracy: 0.9241 - val_loss: 0.5637 - val_accuracy: 0.8503\n",
            "Epoch 883/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2646 - accuracy: 0.9093 - val_loss: 0.5633 - val_accuracy: 0.8514\n",
            "Epoch 884/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2566 - accuracy: 0.9252 - val_loss: 0.5682 - val_accuracy: 0.8451\n",
            "Epoch 885/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2426 - accuracy: 0.9247 - val_loss: 0.5714 - val_accuracy: 0.8451\n",
            "Epoch 886/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2650 - accuracy: 0.9195 - val_loss: 0.5707 - val_accuracy: 0.8441\n",
            "Epoch 887/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2728 - accuracy: 0.9062 - val_loss: 0.5581 - val_accuracy: 0.8451\n",
            "Epoch 888/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2442 - accuracy: 0.9262 - val_loss: 0.5663 - val_accuracy: 0.8482\n",
            "Epoch 889/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2649 - accuracy: 0.9113 - val_loss: 0.5737 - val_accuracy: 0.8514\n",
            "Epoch 890/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2577 - accuracy: 0.9144 - val_loss: 0.5672 - val_accuracy: 0.8472\n",
            "Epoch 891/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2580 - accuracy: 0.9108 - val_loss: 0.5586 - val_accuracy: 0.8493\n",
            "Epoch 892/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2534 - accuracy: 0.9231 - val_loss: 0.5637 - val_accuracy: 0.8524\n",
            "Epoch 893/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2531 - accuracy: 0.9118 - val_loss: 0.5661 - val_accuracy: 0.8451\n",
            "Epoch 894/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2680 - accuracy: 0.9170 - val_loss: 0.5598 - val_accuracy: 0.8472\n",
            "Epoch 895/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2427 - accuracy: 0.9257 - val_loss: 0.5624 - val_accuracy: 0.8503\n",
            "Epoch 896/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2348 - accuracy: 0.9262 - val_loss: 0.5628 - val_accuracy: 0.8441\n",
            "Epoch 897/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2514 - accuracy: 0.9180 - val_loss: 0.5558 - val_accuracy: 0.8482\n",
            "Epoch 898/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2539 - accuracy: 0.9231 - val_loss: 0.5582 - val_accuracy: 0.8524\n",
            "Epoch 899/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2533 - accuracy: 0.9180 - val_loss: 0.5580 - val_accuracy: 0.8451\n",
            "Epoch 900/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2537 - accuracy: 0.9252 - val_loss: 0.5624 - val_accuracy: 0.8462\n",
            "Epoch 901/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2443 - accuracy: 0.9241 - val_loss: 0.5546 - val_accuracy: 0.8410\n",
            "Epoch 902/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2431 - accuracy: 0.9313 - val_loss: 0.5545 - val_accuracy: 0.8534\n",
            "Epoch 903/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2557 - accuracy: 0.9149 - val_loss: 0.5635 - val_accuracy: 0.8545\n",
            "Epoch 904/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2559 - accuracy: 0.9139 - val_loss: 0.5578 - val_accuracy: 0.8514\n",
            "Epoch 905/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2555 - accuracy: 0.9252 - val_loss: 0.5576 - val_accuracy: 0.8451\n",
            "Epoch 906/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2379 - accuracy: 0.9236 - val_loss: 0.5566 - val_accuracy: 0.8451\n",
            "Epoch 907/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2422 - accuracy: 0.9282 - val_loss: 0.5659 - val_accuracy: 0.8503\n",
            "Epoch 908/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2432 - accuracy: 0.9185 - val_loss: 0.5597 - val_accuracy: 0.8514\n",
            "Epoch 909/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2504 - accuracy: 0.9231 - val_loss: 0.5542 - val_accuracy: 0.8462\n",
            "Epoch 910/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2508 - accuracy: 0.9226 - val_loss: 0.5571 - val_accuracy: 0.8441\n",
            "Epoch 911/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2545 - accuracy: 0.9216 - val_loss: 0.5604 - val_accuracy: 0.8503\n",
            "Epoch 912/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2497 - accuracy: 0.9298 - val_loss: 0.5758 - val_accuracy: 0.8441\n",
            "Epoch 913/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2466 - accuracy: 0.9216 - val_loss: 0.5623 - val_accuracy: 0.8534\n",
            "Epoch 914/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2475 - accuracy: 0.9241 - val_loss: 0.5604 - val_accuracy: 0.8545\n",
            "Epoch 915/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2318 - accuracy: 0.9354 - val_loss: 0.5653 - val_accuracy: 0.8524\n",
            "Epoch 916/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2590 - accuracy: 0.9113 - val_loss: 0.5642 - val_accuracy: 0.8441\n",
            "Epoch 917/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2424 - accuracy: 0.9216 - val_loss: 0.5618 - val_accuracy: 0.8503\n",
            "Epoch 918/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2390 - accuracy: 0.9190 - val_loss: 0.5525 - val_accuracy: 0.8472\n",
            "Epoch 919/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2418 - accuracy: 0.9288 - val_loss: 0.5646 - val_accuracy: 0.8420\n",
            "Epoch 920/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2428 - accuracy: 0.9221 - val_loss: 0.5519 - val_accuracy: 0.8451\n",
            "Epoch 921/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2266 - accuracy: 0.9318 - val_loss: 0.5632 - val_accuracy: 0.8493\n",
            "Epoch 922/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2366 - accuracy: 0.9236 - val_loss: 0.5587 - val_accuracy: 0.8503\n",
            "Epoch 923/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2457 - accuracy: 0.9175 - val_loss: 0.5616 - val_accuracy: 0.8493\n",
            "Epoch 924/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2539 - accuracy: 0.9195 - val_loss: 0.5512 - val_accuracy: 0.8441\n",
            "Epoch 925/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2514 - accuracy: 0.9180 - val_loss: 0.5514 - val_accuracy: 0.8524\n",
            "Epoch 926/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2402 - accuracy: 0.9221 - val_loss: 0.5537 - val_accuracy: 0.8482\n",
            "Epoch 927/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2470 - accuracy: 0.9252 - val_loss: 0.5601 - val_accuracy: 0.8462\n",
            "Epoch 928/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2352 - accuracy: 0.9308 - val_loss: 0.5629 - val_accuracy: 0.8399\n",
            "Epoch 929/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2350 - accuracy: 0.9293 - val_loss: 0.5585 - val_accuracy: 0.8482\n",
            "Epoch 930/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2210 - accuracy: 0.9313 - val_loss: 0.5630 - val_accuracy: 0.8451\n",
            "Epoch 931/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2523 - accuracy: 0.9180 - val_loss: 0.5607 - val_accuracy: 0.8472\n",
            "Epoch 932/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2333 - accuracy: 0.9149 - val_loss: 0.5634 - val_accuracy: 0.8493\n",
            "Epoch 933/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2347 - accuracy: 0.9293 - val_loss: 0.5593 - val_accuracy: 0.8503\n",
            "Epoch 934/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2527 - accuracy: 0.9180 - val_loss: 0.5531 - val_accuracy: 0.8451\n",
            "Epoch 935/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2266 - accuracy: 0.9349 - val_loss: 0.5611 - val_accuracy: 0.8482\n",
            "Epoch 936/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2280 - accuracy: 0.9313 - val_loss: 0.5514 - val_accuracy: 0.8451\n",
            "Epoch 937/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2409 - accuracy: 0.9211 - val_loss: 0.5499 - val_accuracy: 0.8514\n",
            "Epoch 938/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2388 - accuracy: 0.9221 - val_loss: 0.5523 - val_accuracy: 0.8441\n",
            "Epoch 939/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2350 - accuracy: 0.9257 - val_loss: 0.5486 - val_accuracy: 0.8545\n",
            "Epoch 940/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2265 - accuracy: 0.9349 - val_loss: 0.5489 - val_accuracy: 0.8555\n",
            "Epoch 941/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2265 - accuracy: 0.9308 - val_loss: 0.5523 - val_accuracy: 0.8462\n",
            "Epoch 942/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2372 - accuracy: 0.9226 - val_loss: 0.5489 - val_accuracy: 0.8586\n",
            "Epoch 943/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2396 - accuracy: 0.9241 - val_loss: 0.5497 - val_accuracy: 0.8472\n",
            "Epoch 944/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2488 - accuracy: 0.9231 - val_loss: 0.5603 - val_accuracy: 0.8555\n",
            "Epoch 945/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2360 - accuracy: 0.9221 - val_loss: 0.5645 - val_accuracy: 0.8451\n",
            "Epoch 946/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2535 - accuracy: 0.9180 - val_loss: 0.5522 - val_accuracy: 0.8482\n",
            "Epoch 947/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2233 - accuracy: 0.9303 - val_loss: 0.5499 - val_accuracy: 0.8462\n",
            "Epoch 948/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2291 - accuracy: 0.9354 - val_loss: 0.5541 - val_accuracy: 0.8524\n",
            "Epoch 949/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2326 - accuracy: 0.9170 - val_loss: 0.5513 - val_accuracy: 0.8462\n",
            "Epoch 950/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2341 - accuracy: 0.9288 - val_loss: 0.5483 - val_accuracy: 0.8493\n",
            "Epoch 951/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2230 - accuracy: 0.9323 - val_loss: 0.5492 - val_accuracy: 0.8482\n",
            "Epoch 952/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2249 - accuracy: 0.9303 - val_loss: 0.5499 - val_accuracy: 0.8493\n",
            "Epoch 953/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2209 - accuracy: 0.9400 - val_loss: 0.5507 - val_accuracy: 0.8472\n",
            "Epoch 954/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2301 - accuracy: 0.9277 - val_loss: 0.5514 - val_accuracy: 0.8462\n",
            "Epoch 955/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2354 - accuracy: 0.9185 - val_loss: 0.5515 - val_accuracy: 0.8493\n",
            "Epoch 956/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2304 - accuracy: 0.9354 - val_loss: 0.5490 - val_accuracy: 0.8534\n",
            "Epoch 957/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2317 - accuracy: 0.9267 - val_loss: 0.5468 - val_accuracy: 0.8534\n",
            "Epoch 958/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2322 - accuracy: 0.9257 - val_loss: 0.5531 - val_accuracy: 0.8524\n",
            "Epoch 959/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2457 - accuracy: 0.9216 - val_loss: 0.5552 - val_accuracy: 0.8555\n",
            "Epoch 960/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2229 - accuracy: 0.9288 - val_loss: 0.5571 - val_accuracy: 0.8451\n",
            "Epoch 961/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2478 - accuracy: 0.9195 - val_loss: 0.5500 - val_accuracy: 0.8514\n",
            "Epoch 962/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2211 - accuracy: 0.9288 - val_loss: 0.5487 - val_accuracy: 0.8514\n",
            "Epoch 963/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2373 - accuracy: 0.9252 - val_loss: 0.5525 - val_accuracy: 0.8534\n",
            "Epoch 964/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2166 - accuracy: 0.9257 - val_loss: 0.5396 - val_accuracy: 0.8514\n",
            "Epoch 965/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2212 - accuracy: 0.9303 - val_loss: 0.5516 - val_accuracy: 0.8493\n",
            "Epoch 966/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2331 - accuracy: 0.9236 - val_loss: 0.5582 - val_accuracy: 0.8482\n",
            "Epoch 967/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2184 - accuracy: 0.9329 - val_loss: 0.5437 - val_accuracy: 0.8493\n",
            "Epoch 968/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2252 - accuracy: 0.9344 - val_loss: 0.5543 - val_accuracy: 0.8545\n",
            "Epoch 969/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2442 - accuracy: 0.9211 - val_loss: 0.5460 - val_accuracy: 0.8545\n",
            "Epoch 970/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2360 - accuracy: 0.9216 - val_loss: 0.5512 - val_accuracy: 0.8462\n",
            "Epoch 971/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2358 - accuracy: 0.9257 - val_loss: 0.5588 - val_accuracy: 0.8441\n",
            "Epoch 972/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2343 - accuracy: 0.9180 - val_loss: 0.5493 - val_accuracy: 0.8545\n",
            "Epoch 973/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2275 - accuracy: 0.9247 - val_loss: 0.5498 - val_accuracy: 0.8514\n",
            "Epoch 974/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2192 - accuracy: 0.9282 - val_loss: 0.5508 - val_accuracy: 0.8493\n",
            "Epoch 975/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2262 - accuracy: 0.9354 - val_loss: 0.5560 - val_accuracy: 0.8493\n",
            "Epoch 976/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2253 - accuracy: 0.9257 - val_loss: 0.5445 - val_accuracy: 0.8482\n",
            "Epoch 977/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2270 - accuracy: 0.9282 - val_loss: 0.5475 - val_accuracy: 0.8472\n",
            "Epoch 978/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2169 - accuracy: 0.9344 - val_loss: 0.5578 - val_accuracy: 0.8482\n",
            "Epoch 979/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2155 - accuracy: 0.9370 - val_loss: 0.5568 - val_accuracy: 0.8462\n",
            "Epoch 980/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2200 - accuracy: 0.9318 - val_loss: 0.5517 - val_accuracy: 0.8420\n",
            "Epoch 981/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2164 - accuracy: 0.9380 - val_loss: 0.5503 - val_accuracy: 0.8472\n",
            "Epoch 982/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2267 - accuracy: 0.9288 - val_loss: 0.5418 - val_accuracy: 0.8472\n",
            "Epoch 983/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2291 - accuracy: 0.9277 - val_loss: 0.5432 - val_accuracy: 0.8555\n",
            "Epoch 984/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2132 - accuracy: 0.9323 - val_loss: 0.5455 - val_accuracy: 0.8545\n",
            "Epoch 985/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2309 - accuracy: 0.9323 - val_loss: 0.5483 - val_accuracy: 0.8472\n",
            "Epoch 986/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9267 - val_loss: 0.5502 - val_accuracy: 0.8482\n",
            "Epoch 987/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2225 - accuracy: 0.9313 - val_loss: 0.5501 - val_accuracy: 0.8462\n",
            "Epoch 988/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2098 - accuracy: 0.9426 - val_loss: 0.5476 - val_accuracy: 0.8534\n",
            "Epoch 989/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2179 - accuracy: 0.9334 - val_loss: 0.5465 - val_accuracy: 0.8514\n",
            "Epoch 990/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2187 - accuracy: 0.9323 - val_loss: 0.5540 - val_accuracy: 0.8503\n",
            "Epoch 991/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2209 - accuracy: 0.9313 - val_loss: 0.5564 - val_accuracy: 0.8451\n",
            "Epoch 992/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2114 - accuracy: 0.9323 - val_loss: 0.5473 - val_accuracy: 0.8503\n",
            "Epoch 993/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2152 - accuracy: 0.9364 - val_loss: 0.5510 - val_accuracy: 0.8576\n",
            "Epoch 994/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2193 - accuracy: 0.9323 - val_loss: 0.5416 - val_accuracy: 0.8545\n",
            "Epoch 995/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2108 - accuracy: 0.9380 - val_loss: 0.5487 - val_accuracy: 0.8503\n",
            "Epoch 996/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2106 - accuracy: 0.9400 - val_loss: 0.5386 - val_accuracy: 0.8482\n",
            "Epoch 997/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2051 - accuracy: 0.9405 - val_loss: 0.5490 - val_accuracy: 0.8555\n",
            "Epoch 998/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9236 - val_loss: 0.5497 - val_accuracy: 0.8555\n",
            "Epoch 999/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2074 - accuracy: 0.9380 - val_loss: 0.5500 - val_accuracy: 0.8493\n",
            "Epoch 1000/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2292 - accuracy: 0.9247 - val_loss: 0.5483 - val_accuracy: 0.8524\n",
            "Epoch 1001/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2128 - accuracy: 0.9236 - val_loss: 0.5419 - val_accuracy: 0.8555\n",
            "Epoch 1002/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1891 - accuracy: 0.9364 - val_loss: 0.5467 - val_accuracy: 0.8524\n",
            "Epoch 1003/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2176 - accuracy: 0.9288 - val_loss: 0.5418 - val_accuracy: 0.8545\n",
            "Epoch 1004/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2197 - accuracy: 0.9247 - val_loss: 0.5438 - val_accuracy: 0.8586\n",
            "Epoch 1005/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2232 - accuracy: 0.9231 - val_loss: 0.5472 - val_accuracy: 0.8586\n",
            "Epoch 1006/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2264 - accuracy: 0.9216 - val_loss: 0.5420 - val_accuracy: 0.8617\n",
            "Epoch 1007/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2166 - accuracy: 0.9282 - val_loss: 0.5545 - val_accuracy: 0.8482\n",
            "Epoch 1008/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2201 - accuracy: 0.9252 - val_loss: 0.5382 - val_accuracy: 0.8482\n",
            "Epoch 1009/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2054 - accuracy: 0.9359 - val_loss: 0.5471 - val_accuracy: 0.8514\n",
            "Epoch 1010/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2181 - accuracy: 0.9298 - val_loss: 0.5423 - val_accuracy: 0.8576\n",
            "Epoch 1011/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2169 - accuracy: 0.9293 - val_loss: 0.5401 - val_accuracy: 0.8524\n",
            "Epoch 1012/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2175 - accuracy: 0.9329 - val_loss: 0.5453 - val_accuracy: 0.8503\n",
            "Epoch 1013/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2089 - accuracy: 0.9411 - val_loss: 0.5428 - val_accuracy: 0.8514\n",
            "Epoch 1014/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2100 - accuracy: 0.9318 - val_loss: 0.5377 - val_accuracy: 0.8524\n",
            "Epoch 1015/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2064 - accuracy: 0.9349 - val_loss: 0.5410 - val_accuracy: 0.8462\n",
            "Epoch 1016/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2013 - accuracy: 0.9380 - val_loss: 0.5402 - val_accuracy: 0.8503\n",
            "Epoch 1017/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1987 - accuracy: 0.9452 - val_loss: 0.5440 - val_accuracy: 0.8514\n",
            "Epoch 1018/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2130 - accuracy: 0.9318 - val_loss: 0.5474 - val_accuracy: 0.8534\n",
            "Epoch 1019/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.5487 - val_accuracy: 0.8451\n",
            "Epoch 1020/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2079 - accuracy: 0.9303 - val_loss: 0.5460 - val_accuracy: 0.8493\n",
            "Epoch 1021/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2164 - accuracy: 0.9308 - val_loss: 0.5416 - val_accuracy: 0.8472\n",
            "Epoch 1022/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2193 - accuracy: 0.9257 - val_loss: 0.5386 - val_accuracy: 0.8545\n",
            "Epoch 1023/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1968 - accuracy: 0.9431 - val_loss: 0.5442 - val_accuracy: 0.8514\n",
            "Epoch 1024/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1933 - accuracy: 0.9416 - val_loss: 0.5472 - val_accuracy: 0.8555\n",
            "Epoch 1025/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2261 - accuracy: 0.9247 - val_loss: 0.5447 - val_accuracy: 0.8493\n",
            "Epoch 1026/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2042 - accuracy: 0.9364 - val_loss: 0.5440 - val_accuracy: 0.8555\n",
            "Epoch 1027/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2057 - accuracy: 0.9390 - val_loss: 0.5476 - val_accuracy: 0.8503\n",
            "Epoch 1028/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2067 - accuracy: 0.9375 - val_loss: 0.5451 - val_accuracy: 0.8493\n",
            "Epoch 1029/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2273 - accuracy: 0.9267 - val_loss: 0.5513 - val_accuracy: 0.8534\n",
            "Epoch 1030/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2050 - accuracy: 0.9375 - val_loss: 0.5423 - val_accuracy: 0.8607\n",
            "Epoch 1031/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2024 - accuracy: 0.9400 - val_loss: 0.5390 - val_accuracy: 0.8534\n",
            "Epoch 1032/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1978 - accuracy: 0.9370 - val_loss: 0.5381 - val_accuracy: 0.8607\n",
            "Epoch 1033/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1983 - accuracy: 0.9323 - val_loss: 0.5408 - val_accuracy: 0.8493\n",
            "Epoch 1034/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2056 - accuracy: 0.9308 - val_loss: 0.5483 - val_accuracy: 0.8565\n",
            "Epoch 1035/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1959 - accuracy: 0.9313 - val_loss: 0.5428 - val_accuracy: 0.8545\n",
            "Epoch 1036/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2125 - accuracy: 0.9303 - val_loss: 0.5362 - val_accuracy: 0.8524\n",
            "Epoch 1037/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2110 - accuracy: 0.9329 - val_loss: 0.5517 - val_accuracy: 0.8597\n",
            "Epoch 1038/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2030 - accuracy: 0.9375 - val_loss: 0.5507 - val_accuracy: 0.8555\n",
            "Epoch 1039/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2030 - accuracy: 0.9344 - val_loss: 0.5425 - val_accuracy: 0.8534\n",
            "Epoch 1040/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2005 - accuracy: 0.9329 - val_loss: 0.5434 - val_accuracy: 0.8451\n",
            "Epoch 1041/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2018 - accuracy: 0.9359 - val_loss: 0.5480 - val_accuracy: 0.8565\n",
            "Epoch 1042/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1965 - accuracy: 0.9380 - val_loss: 0.5357 - val_accuracy: 0.8514\n",
            "Epoch 1043/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1993 - accuracy: 0.9364 - val_loss: 0.5417 - val_accuracy: 0.8597\n",
            "Epoch 1044/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1876 - accuracy: 0.9400 - val_loss: 0.5398 - val_accuracy: 0.8565\n",
            "Epoch 1045/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2036 - accuracy: 0.9354 - val_loss: 0.5363 - val_accuracy: 0.8555\n",
            "Epoch 1046/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2198 - accuracy: 0.9282 - val_loss: 0.5365 - val_accuracy: 0.8534\n",
            "Epoch 1047/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2008 - accuracy: 0.9329 - val_loss: 0.5363 - val_accuracy: 0.8514\n",
            "Epoch 1048/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2025 - accuracy: 0.9375 - val_loss: 0.5398 - val_accuracy: 0.8617\n",
            "Epoch 1049/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2129 - accuracy: 0.9364 - val_loss: 0.5363 - val_accuracy: 0.8545\n",
            "Epoch 1050/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2275 - accuracy: 0.9288 - val_loss: 0.5485 - val_accuracy: 0.8503\n",
            "Epoch 1051/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2165 - accuracy: 0.9334 - val_loss: 0.5371 - val_accuracy: 0.8524\n",
            "Epoch 1052/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9390 - val_loss: 0.5369 - val_accuracy: 0.8576\n",
            "Epoch 1053/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2228 - accuracy: 0.9282 - val_loss: 0.5300 - val_accuracy: 0.8514\n",
            "Epoch 1054/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2074 - accuracy: 0.9303 - val_loss: 0.5451 - val_accuracy: 0.8482\n",
            "Epoch 1055/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1994 - accuracy: 0.9318 - val_loss: 0.5334 - val_accuracy: 0.8555\n",
            "Epoch 1056/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1950 - accuracy: 0.9431 - val_loss: 0.5416 - val_accuracy: 0.8576\n",
            "Epoch 1057/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2008 - accuracy: 0.9323 - val_loss: 0.5431 - val_accuracy: 0.8638\n",
            "Epoch 1058/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1980 - accuracy: 0.9339 - val_loss: 0.5369 - val_accuracy: 0.8607\n",
            "Epoch 1059/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1934 - accuracy: 0.9426 - val_loss: 0.5404 - val_accuracy: 0.8586\n",
            "Epoch 1060/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1865 - accuracy: 0.9457 - val_loss: 0.5433 - val_accuracy: 0.8524\n",
            "Epoch 1061/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1994 - accuracy: 0.9318 - val_loss: 0.5343 - val_accuracy: 0.8545\n",
            "Epoch 1062/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1995 - accuracy: 0.9359 - val_loss: 0.5354 - val_accuracy: 0.8576\n",
            "Epoch 1063/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2037 - accuracy: 0.9395 - val_loss: 0.5252 - val_accuracy: 0.8586\n",
            "Epoch 1064/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1975 - accuracy: 0.9313 - val_loss: 0.5229 - val_accuracy: 0.8638\n",
            "Epoch 1065/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1838 - accuracy: 0.9395 - val_loss: 0.5363 - val_accuracy: 0.8534\n",
            "Epoch 1066/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1826 - accuracy: 0.9441 - val_loss: 0.5245 - val_accuracy: 0.8555\n",
            "Epoch 1067/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2007 - accuracy: 0.9390 - val_loss: 0.5350 - val_accuracy: 0.8586\n",
            "Epoch 1068/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2064 - accuracy: 0.9349 - val_loss: 0.5338 - val_accuracy: 0.8586\n",
            "Epoch 1069/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1897 - accuracy: 0.9385 - val_loss: 0.5346 - val_accuracy: 0.8565\n",
            "Epoch 1070/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1853 - accuracy: 0.9421 - val_loss: 0.5383 - val_accuracy: 0.8503\n",
            "Epoch 1071/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2005 - accuracy: 0.9323 - val_loss: 0.5397 - val_accuracy: 0.8649\n",
            "Epoch 1072/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1850 - accuracy: 0.9431 - val_loss: 0.5269 - val_accuracy: 0.8607\n",
            "Epoch 1073/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9380 - val_loss: 0.5397 - val_accuracy: 0.8524\n",
            "Epoch 1074/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.2099 - accuracy: 0.9288 - val_loss: 0.5329 - val_accuracy: 0.8628\n",
            "Epoch 1075/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1914 - accuracy: 0.9411 - val_loss: 0.5264 - val_accuracy: 0.8524\n",
            "Epoch 1076/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2054 - accuracy: 0.9272 - val_loss: 0.5365 - val_accuracy: 0.8545\n",
            "Epoch 1077/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1894 - accuracy: 0.9375 - val_loss: 0.5294 - val_accuracy: 0.8555\n",
            "Epoch 1078/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.1756 - accuracy: 0.9405 - val_loss: 0.5344 - val_accuracy: 0.8545\n",
            "Epoch 1079/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1958 - accuracy: 0.9375 - val_loss: 0.5328 - val_accuracy: 0.8597\n",
            "Epoch 1080/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1827 - accuracy: 0.9400 - val_loss: 0.5435 - val_accuracy: 0.8628\n",
            "Epoch 1081/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2055 - accuracy: 0.9364 - val_loss: 0.5409 - val_accuracy: 0.8555\n",
            "Epoch 1082/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1941 - accuracy: 0.9446 - val_loss: 0.5373 - val_accuracy: 0.8534\n",
            "Epoch 1083/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9416 - val_loss: 0.5327 - val_accuracy: 0.8576\n",
            "Epoch 1084/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1792 - accuracy: 0.9416 - val_loss: 0.5268 - val_accuracy: 0.8617\n",
            "Epoch 1085/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1757 - accuracy: 0.9467 - val_loss: 0.5347 - val_accuracy: 0.8493\n",
            "Epoch 1086/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1657 - accuracy: 0.9498 - val_loss: 0.5323 - val_accuracy: 0.8597\n",
            "Epoch 1087/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9416 - val_loss: 0.5314 - val_accuracy: 0.8617\n",
            "Epoch 1088/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1780 - accuracy: 0.9498 - val_loss: 0.5340 - val_accuracy: 0.8586\n",
            "Epoch 1089/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 0.2015 - accuracy: 0.9344 - val_loss: 0.5367 - val_accuracy: 0.8555\n",
            "Epoch 1090/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1872 - accuracy: 0.9416 - val_loss: 0.5380 - val_accuracy: 0.8607\n",
            "Epoch 1091/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1807 - accuracy: 0.9441 - val_loss: 0.5291 - val_accuracy: 0.8514\n",
            "Epoch 1092/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1940 - accuracy: 0.9457 - val_loss: 0.5432 - val_accuracy: 0.8565\n",
            "Epoch 1093/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1827 - accuracy: 0.9416 - val_loss: 0.5399 - val_accuracy: 0.8565\n",
            "Epoch 1094/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1886 - accuracy: 0.9426 - val_loss: 0.5383 - val_accuracy: 0.8545\n",
            "Epoch 1095/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1928 - accuracy: 0.9405 - val_loss: 0.5380 - val_accuracy: 0.8586\n",
            "Epoch 1096/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1875 - accuracy: 0.9436 - val_loss: 0.5462 - val_accuracy: 0.8503\n",
            "Epoch 1097/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1932 - accuracy: 0.9400 - val_loss: 0.5417 - val_accuracy: 0.8493\n",
            "Epoch 1098/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1823 - accuracy: 0.9426 - val_loss: 0.5479 - val_accuracy: 0.8597\n",
            "Epoch 1099/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1753 - accuracy: 0.9431 - val_loss: 0.5408 - val_accuracy: 0.8514\n",
            "Epoch 1100/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 0.1755 - accuracy: 0.9467 - val_loss: 0.5425 - val_accuracy: 0.8565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "yXGQ9TO98e8T",
        "outputId": "4c753b44-c043-42db-d218-510a99d3d517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy plot\n",
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pM-Z_jT99Sid",
        "outputId": "429b7c0e-36ee-4145-b7d0-d4ea172638de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRPgV5cN9SqK",
        "outputId": "5ffc0179-3441-4cec-9d81-746b95b030fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reload\n",
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBrX4iNM9SyS",
        "outputId": "8764b126-f316-495a-cd1f-91051ba08b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 5, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 5, 70)             44870     \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 5, 70)             0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 5, 70)             0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 350)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                3510      \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,148\n",
            "Trainable params: 49,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "pred = model.predict(x_testcnn) \n",
        "pred = np.argmax(pred, axis = 1)\n",
        "y_test = np.array([int(x) for x in y_test])\n",
        "\n",
        "\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test, pred))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ],
      "metadata": {
        "id": "GOP5-gGKRnO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bec4c1f-aee0-4990-b49a-0e0b98b7bd4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.84      0.86      0.85        56\n",
            "           2       0.90      0.92      0.91       130\n",
            "           3       0.81      0.90      0.85       137\n",
            "           4       0.82      0.81      0.82       121\n",
            "           5       0.88      0.84      0.86       125\n",
            "           6       0.82      0.83      0.83       120\n",
            "           7       0.93      0.77      0.84       136\n",
            "           8       0.86      0.91      0.88       137\n",
            "\n",
            "    accuracy                           0.86       962\n",
            "   macro avg       0.86      0.86      0.86       962\n",
            "weighted avg       0.86      0.86      0.86       962\n",
            "\n",
            "[[ 48   2   2   4   0   0   0   0]\n",
            " [  6 120   2   0   0   0   2   0]\n",
            " [  1   0 123   4   3   4   0   2]\n",
            " [  0   8   2  98   0   4   4   5]\n",
            " [  0   2   4   2 105   2   2   8]\n",
            " [  0   2   9   7   2 100   0   0]\n",
            " [  0   0   6   2   7  10 105   6]\n",
            " [  2   0   4   2   2   2   0 125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN MELSPECTOGRAMs"
      ],
      "metadata": {
        "id": "b6UvpDci8w08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "import librosa\n",
        "from librosa import display\n",
        "import numpy as np\n",
        "\n",
        "#EXTRACTING MELSPECTOGRAM INSTEAD OF MFCC, I'm using a CNN so the melspec should perform better\n",
        "path = '/content/drive/MyDrive/ColabNotebooks/dataset'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        melSpectograms = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=40).T,axis=0)\n",
        "        file = file[6:8] #extract features\n",
        "        arr = melSpectograms, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue   \n"
      ],
      "metadata": {
        "id": "o7NC3mlRrc4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = zip(*lst)\n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #splitting for training and testing"
      ],
      "metadata": {
        "id": "GIdxUtQBwFCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhCA_tqBf3U2"
      },
      "outputs": [],
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2) \n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqcUYeFqbqf"
      },
      "outputs": [],
      "source": [
        "y_test = y_test.astype(np.float)\n",
        "y_train = y_train.astype(np.float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n8zem21f3U2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9043a595-bb8e-4b11-81cc-e4ac449e5e49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1951, 40, 1), (962, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "x_traincnn.shape, x_testcnn.shape #mels: ((1951, 20, 1), (962, 20, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9fWeYARf3U3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "#hidden neurons = (#input neurons + #output neurons)/2\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(70, 5,padding='same',)) # era 128\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0000580, rho=0.9, epsilon=None, decay=0.0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4GKrM5ef3U3",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f766c79-90d5-4eed-937f-f88a6a7a3665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 5, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 5, 70)             44870     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 5, 70)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 70)             0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 350)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                3510      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,148\n",
            "Trainable params: 49,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ubQdvef3U3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzKpdUrRf3U3",
        "outputId": "18ffc793-93c1-49fb-97cf-8fe06d3865ed",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1100\n",
            "122/122 [==============================] - 2s 9ms/step - loss: 2.2671 - accuracy: 0.1932 - val_loss: 2.2030 - val_accuracy: 0.1913\n",
            "Epoch 2/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.1697 - accuracy: 0.1943 - val_loss: 2.1532 - val_accuracy: 0.1985\n",
            "Epoch 3/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.1245 - accuracy: 0.2117 - val_loss: 2.1137 - val_accuracy: 0.2141\n",
            "Epoch 4/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.0791 - accuracy: 0.2127 - val_loss: 2.0652 - val_accuracy: 0.2235\n",
            "Epoch 5/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 2.0315 - accuracy: 0.2317 - val_loss: 2.0293 - val_accuracy: 0.2131\n",
            "Epoch 6/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9977 - accuracy: 0.2153 - val_loss: 1.9814 - val_accuracy: 0.2131\n",
            "Epoch 7/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9631 - accuracy: 0.2307 - val_loss: 1.9452 - val_accuracy: 0.2474\n",
            "Epoch 8/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9333 - accuracy: 0.2399 - val_loss: 1.9170 - val_accuracy: 0.2380\n",
            "Epoch 9/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9137 - accuracy: 0.2332 - val_loss: 1.8998 - val_accuracy: 0.2391\n",
            "Epoch 10/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.9053 - accuracy: 0.2373 - val_loss: 1.8847 - val_accuracy: 0.2827\n",
            "Epoch 11/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8880 - accuracy: 0.2450 - val_loss: 1.8686 - val_accuracy: 0.2921\n",
            "Epoch 12/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8829 - accuracy: 0.2496 - val_loss: 1.8581 - val_accuracy: 0.2911\n",
            "Epoch 13/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8738 - accuracy: 0.2430 - val_loss: 1.8534 - val_accuracy: 0.2859\n",
            "Epoch 14/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8667 - accuracy: 0.2578 - val_loss: 1.8450 - val_accuracy: 0.2765\n",
            "Epoch 15/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8595 - accuracy: 0.2532 - val_loss: 1.8393 - val_accuracy: 0.2796\n",
            "Epoch 16/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.8489 - accuracy: 0.2655 - val_loss: 1.8382 - val_accuracy: 0.2765\n",
            "Epoch 17/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8496 - accuracy: 0.2691 - val_loss: 1.8274 - val_accuracy: 0.2817\n",
            "Epoch 18/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8431 - accuracy: 0.2573 - val_loss: 1.8189 - val_accuracy: 0.2879\n",
            "Epoch 19/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8310 - accuracy: 0.2799 - val_loss: 1.8127 - val_accuracy: 0.2848\n",
            "Epoch 20/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8356 - accuracy: 0.2665 - val_loss: 1.8108 - val_accuracy: 0.3025\n",
            "Epoch 21/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8403 - accuracy: 0.2752 - val_loss: 1.8130 - val_accuracy: 0.2994\n",
            "Epoch 22/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8199 - accuracy: 0.2793 - val_loss: 1.8016 - val_accuracy: 0.3025\n",
            "Epoch 23/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8178 - accuracy: 0.2778 - val_loss: 1.8003 - val_accuracy: 0.2983\n",
            "Epoch 24/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.8137 - accuracy: 0.2968 - val_loss: 1.7948 - val_accuracy: 0.3056\n",
            "Epoch 25/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8191 - accuracy: 0.2747 - val_loss: 1.7995 - val_accuracy: 0.3067\n",
            "Epoch 26/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8147 - accuracy: 0.2870 - val_loss: 1.7922 - val_accuracy: 0.3233\n",
            "Epoch 27/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8019 - accuracy: 0.2963 - val_loss: 1.7845 - val_accuracy: 0.3077\n",
            "Epoch 28/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7986 - accuracy: 0.2957 - val_loss: 1.7795 - val_accuracy: 0.3150\n",
            "Epoch 29/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8010 - accuracy: 0.2881 - val_loss: 1.7782 - val_accuracy: 0.3098\n",
            "Epoch 30/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8050 - accuracy: 0.2875 - val_loss: 1.7851 - val_accuracy: 0.3098\n",
            "Epoch 31/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.8000 - accuracy: 0.2916 - val_loss: 1.7719 - val_accuracy: 0.3181\n",
            "Epoch 32/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7956 - accuracy: 0.2824 - val_loss: 1.7698 - val_accuracy: 0.3191\n",
            "Epoch 33/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7896 - accuracy: 0.2906 - val_loss: 1.7792 - val_accuracy: 0.3170\n",
            "Epoch 34/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7868 - accuracy: 0.2983 - val_loss: 1.7681 - val_accuracy: 0.3129\n",
            "Epoch 35/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7887 - accuracy: 0.2814 - val_loss: 1.7719 - val_accuracy: 0.3212\n",
            "Epoch 36/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7848 - accuracy: 0.2957 - val_loss: 1.7598 - val_accuracy: 0.3212\n",
            "Epoch 37/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7836 - accuracy: 0.2983 - val_loss: 1.7566 - val_accuracy: 0.3264\n",
            "Epoch 38/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7867 - accuracy: 0.2998 - val_loss: 1.7538 - val_accuracy: 0.3347\n",
            "Epoch 39/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7706 - accuracy: 0.3065 - val_loss: 1.7549 - val_accuracy: 0.3243\n",
            "Epoch 40/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7716 - accuracy: 0.3014 - val_loss: 1.7577 - val_accuracy: 0.3316\n",
            "Epoch 41/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7680 - accuracy: 0.3024 - val_loss: 1.7502 - val_accuracy: 0.3264\n",
            "Epoch 42/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7690 - accuracy: 0.3065 - val_loss: 1.7543 - val_accuracy: 0.3306\n",
            "Epoch 43/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7719 - accuracy: 0.3086 - val_loss: 1.7427 - val_accuracy: 0.3285\n",
            "Epoch 44/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7670 - accuracy: 0.3050 - val_loss: 1.7441 - val_accuracy: 0.3295\n",
            "Epoch 45/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7641 - accuracy: 0.3096 - val_loss: 1.7472 - val_accuracy: 0.3326\n",
            "Epoch 46/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7655 - accuracy: 0.3086 - val_loss: 1.7383 - val_accuracy: 0.3347\n",
            "Epoch 47/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7520 - accuracy: 0.3096 - val_loss: 1.7371 - val_accuracy: 0.3347\n",
            "Epoch 48/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7652 - accuracy: 0.3121 - val_loss: 1.7362 - val_accuracy: 0.3306\n",
            "Epoch 49/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7590 - accuracy: 0.3029 - val_loss: 1.7338 - val_accuracy: 0.3326\n",
            "Epoch 50/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7527 - accuracy: 0.3091 - val_loss: 1.7318 - val_accuracy: 0.3326\n",
            "Epoch 51/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7532 - accuracy: 0.3121 - val_loss: 1.7316 - val_accuracy: 0.3326\n",
            "Epoch 52/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7534 - accuracy: 0.3147 - val_loss: 1.7334 - val_accuracy: 0.3306\n",
            "Epoch 53/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7448 - accuracy: 0.3132 - val_loss: 1.7288 - val_accuracy: 0.3316\n",
            "Epoch 54/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7526 - accuracy: 0.3132 - val_loss: 1.7283 - val_accuracy: 0.3326\n",
            "Epoch 55/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7469 - accuracy: 0.3173 - val_loss: 1.7261 - val_accuracy: 0.3358\n",
            "Epoch 56/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7430 - accuracy: 0.3132 - val_loss: 1.7290 - val_accuracy: 0.3389\n",
            "Epoch 57/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7438 - accuracy: 0.3127 - val_loss: 1.7232 - val_accuracy: 0.3368\n",
            "Epoch 58/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7378 - accuracy: 0.3285 - val_loss: 1.7235 - val_accuracy: 0.3430\n",
            "Epoch 59/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7401 - accuracy: 0.3250 - val_loss: 1.7178 - val_accuracy: 0.3441\n",
            "Epoch 60/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7370 - accuracy: 0.3239 - val_loss: 1.7194 - val_accuracy: 0.3430\n",
            "Epoch 61/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7325 - accuracy: 0.3260 - val_loss: 1.7168 - val_accuracy: 0.3441\n",
            "Epoch 62/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7289 - accuracy: 0.3260 - val_loss: 1.7144 - val_accuracy: 0.3451\n",
            "Epoch 63/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7392 - accuracy: 0.3188 - val_loss: 1.7124 - val_accuracy: 0.3451\n",
            "Epoch 64/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7399 - accuracy: 0.3127 - val_loss: 1.7126 - val_accuracy: 0.3524\n",
            "Epoch 65/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7295 - accuracy: 0.3116 - val_loss: 1.7117 - val_accuracy: 0.3451\n",
            "Epoch 66/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7339 - accuracy: 0.3198 - val_loss: 1.7122 - val_accuracy: 0.3451\n",
            "Epoch 67/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7282 - accuracy: 0.3275 - val_loss: 1.7105 - val_accuracy: 0.3503\n",
            "Epoch 68/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7365 - accuracy: 0.3091 - val_loss: 1.7109 - val_accuracy: 0.3441\n",
            "Epoch 69/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7284 - accuracy: 0.3219 - val_loss: 1.7090 - val_accuracy: 0.3493\n",
            "Epoch 70/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7295 - accuracy: 0.3250 - val_loss: 1.7095 - val_accuracy: 0.3472\n",
            "Epoch 71/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7269 - accuracy: 0.3250 - val_loss: 1.7106 - val_accuracy: 0.3482\n",
            "Epoch 72/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7231 - accuracy: 0.3326 - val_loss: 1.7079 - val_accuracy: 0.3410\n",
            "Epoch 73/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7182 - accuracy: 0.3280 - val_loss: 1.7071 - val_accuracy: 0.3482\n",
            "Epoch 74/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7251 - accuracy: 0.3188 - val_loss: 1.7053 - val_accuracy: 0.3503\n",
            "Epoch 75/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7256 - accuracy: 0.3209 - val_loss: 1.7055 - val_accuracy: 0.3472\n",
            "Epoch 76/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7080 - accuracy: 0.3306 - val_loss: 1.7039 - val_accuracy: 0.3503\n",
            "Epoch 77/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7127 - accuracy: 0.3342 - val_loss: 1.7058 - val_accuracy: 0.3534\n",
            "Epoch 78/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7253 - accuracy: 0.3306 - val_loss: 1.7060 - val_accuracy: 0.3472\n",
            "Epoch 79/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7138 - accuracy: 0.3352 - val_loss: 1.7093 - val_accuracy: 0.3534\n",
            "Epoch 80/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7259 - accuracy: 0.3265 - val_loss: 1.7039 - val_accuracy: 0.3503\n",
            "Epoch 81/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7145 - accuracy: 0.3260 - val_loss: 1.7021 - val_accuracy: 0.3576\n",
            "Epoch 82/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7139 - accuracy: 0.3337 - val_loss: 1.6975 - val_accuracy: 0.3534\n",
            "Epoch 83/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7162 - accuracy: 0.3250 - val_loss: 1.6983 - val_accuracy: 0.3586\n",
            "Epoch 84/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7113 - accuracy: 0.3316 - val_loss: 1.6953 - val_accuracy: 0.3472\n",
            "Epoch 85/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7076 - accuracy: 0.3326 - val_loss: 1.6944 - val_accuracy: 0.3586\n",
            "Epoch 86/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7176 - accuracy: 0.3296 - val_loss: 1.6962 - val_accuracy: 0.3628\n",
            "Epoch 87/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7125 - accuracy: 0.3347 - val_loss: 1.6989 - val_accuracy: 0.3638\n",
            "Epoch 88/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7094 - accuracy: 0.3321 - val_loss: 1.6965 - val_accuracy: 0.3617\n",
            "Epoch 89/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7141 - accuracy: 0.3378 - val_loss: 1.6956 - val_accuracy: 0.3690\n",
            "Epoch 90/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7024 - accuracy: 0.3316 - val_loss: 1.6940 - val_accuracy: 0.3638\n",
            "Epoch 91/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7068 - accuracy: 0.3224 - val_loss: 1.6933 - val_accuracy: 0.3649\n",
            "Epoch 92/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7036 - accuracy: 0.3419 - val_loss: 1.6926 - val_accuracy: 0.3607\n",
            "Epoch 93/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.7111 - accuracy: 0.3234 - val_loss: 1.6900 - val_accuracy: 0.3597\n",
            "Epoch 94/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7079 - accuracy: 0.3255 - val_loss: 1.6908 - val_accuracy: 0.3701\n",
            "Epoch 95/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7035 - accuracy: 0.3291 - val_loss: 1.6927 - val_accuracy: 0.3721\n",
            "Epoch 96/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7011 - accuracy: 0.3403 - val_loss: 1.6887 - val_accuracy: 0.3482\n",
            "Epoch 97/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7090 - accuracy: 0.3265 - val_loss: 1.6909 - val_accuracy: 0.3597\n",
            "Epoch 98/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6971 - accuracy: 0.3362 - val_loss: 1.6891 - val_accuracy: 0.3617\n",
            "Epoch 99/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6966 - accuracy: 0.3352 - val_loss: 1.6852 - val_accuracy: 0.3659\n",
            "Epoch 100/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.7019 - accuracy: 0.3352 - val_loss: 1.6911 - val_accuracy: 0.3597\n",
            "Epoch 101/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6941 - accuracy: 0.3373 - val_loss: 1.6881 - val_accuracy: 0.3669\n",
            "Epoch 102/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6940 - accuracy: 0.3332 - val_loss: 1.6874 - val_accuracy: 0.3659\n",
            "Epoch 103/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6943 - accuracy: 0.3439 - val_loss: 1.6878 - val_accuracy: 0.3649\n",
            "Epoch 104/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6884 - accuracy: 0.3403 - val_loss: 1.6863 - val_accuracy: 0.3721\n",
            "Epoch 105/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6888 - accuracy: 0.3455 - val_loss: 1.6898 - val_accuracy: 0.3753\n",
            "Epoch 106/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6931 - accuracy: 0.3419 - val_loss: 1.6859 - val_accuracy: 0.3617\n",
            "Epoch 107/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6873 - accuracy: 0.3414 - val_loss: 1.6844 - val_accuracy: 0.3784\n",
            "Epoch 108/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6881 - accuracy: 0.3455 - val_loss: 1.6833 - val_accuracy: 0.3732\n",
            "Epoch 109/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6975 - accuracy: 0.3403 - val_loss: 1.6830 - val_accuracy: 0.3721\n",
            "Epoch 110/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6867 - accuracy: 0.3496 - val_loss: 1.6895 - val_accuracy: 0.3701\n",
            "Epoch 111/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6946 - accuracy: 0.3332 - val_loss: 1.6864 - val_accuracy: 0.3773\n",
            "Epoch 112/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6843 - accuracy: 0.3424 - val_loss: 1.6867 - val_accuracy: 0.3711\n",
            "Epoch 113/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6841 - accuracy: 0.3429 - val_loss: 1.6838 - val_accuracy: 0.3649\n",
            "Epoch 114/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6872 - accuracy: 0.3424 - val_loss: 1.6775 - val_accuracy: 0.3680\n",
            "Epoch 115/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6951 - accuracy: 0.3383 - val_loss: 1.6790 - val_accuracy: 0.3711\n",
            "Epoch 116/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6899 - accuracy: 0.3470 - val_loss: 1.6791 - val_accuracy: 0.3742\n",
            "Epoch 117/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6764 - accuracy: 0.3434 - val_loss: 1.6773 - val_accuracy: 0.3794\n",
            "Epoch 118/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6864 - accuracy: 0.3311 - val_loss: 1.6761 - val_accuracy: 0.3753\n",
            "Epoch 119/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6911 - accuracy: 0.3383 - val_loss: 1.6797 - val_accuracy: 0.3763\n",
            "Epoch 120/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6866 - accuracy: 0.3414 - val_loss: 1.6784 - val_accuracy: 0.3701\n",
            "Epoch 121/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6740 - accuracy: 0.3450 - val_loss: 1.6804 - val_accuracy: 0.3711\n",
            "Epoch 122/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6845 - accuracy: 0.3393 - val_loss: 1.6792 - val_accuracy: 0.3773\n",
            "Epoch 123/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6862 - accuracy: 0.3414 - val_loss: 1.6806 - val_accuracy: 0.3732\n",
            "Epoch 124/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6823 - accuracy: 0.3352 - val_loss: 1.6743 - val_accuracy: 0.3732\n",
            "Epoch 125/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6787 - accuracy: 0.3542 - val_loss: 1.6735 - val_accuracy: 0.3742\n",
            "Epoch 126/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6712 - accuracy: 0.3326 - val_loss: 1.6732 - val_accuracy: 0.3763\n",
            "Epoch 127/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6878 - accuracy: 0.3434 - val_loss: 1.6762 - val_accuracy: 0.3732\n",
            "Epoch 128/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6804 - accuracy: 0.3470 - val_loss: 1.6712 - val_accuracy: 0.3805\n",
            "Epoch 129/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6750 - accuracy: 0.3526 - val_loss: 1.6766 - val_accuracy: 0.3732\n",
            "Epoch 130/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6813 - accuracy: 0.3378 - val_loss: 1.6714 - val_accuracy: 0.3773\n",
            "Epoch 131/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6762 - accuracy: 0.3398 - val_loss: 1.6711 - val_accuracy: 0.3753\n",
            "Epoch 132/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6739 - accuracy: 0.3460 - val_loss: 1.6745 - val_accuracy: 0.3805\n",
            "Epoch 133/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6719 - accuracy: 0.3532 - val_loss: 1.6709 - val_accuracy: 0.3805\n",
            "Epoch 134/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6664 - accuracy: 0.3532 - val_loss: 1.6708 - val_accuracy: 0.3773\n",
            "Epoch 135/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6781 - accuracy: 0.3521 - val_loss: 1.6720 - val_accuracy: 0.3836\n",
            "Epoch 136/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6717 - accuracy: 0.3475 - val_loss: 1.6689 - val_accuracy: 0.3742\n",
            "Epoch 137/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6792 - accuracy: 0.3491 - val_loss: 1.6687 - val_accuracy: 0.3773\n",
            "Epoch 138/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6643 - accuracy: 0.3393 - val_loss: 1.6708 - val_accuracy: 0.3773\n",
            "Epoch 139/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6698 - accuracy: 0.3429 - val_loss: 1.6710 - val_accuracy: 0.3794\n",
            "Epoch 140/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6709 - accuracy: 0.3521 - val_loss: 1.6679 - val_accuracy: 0.3753\n",
            "Epoch 141/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6715 - accuracy: 0.3521 - val_loss: 1.6678 - val_accuracy: 0.3784\n",
            "Epoch 142/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6652 - accuracy: 0.3444 - val_loss: 1.6673 - val_accuracy: 0.3815\n",
            "Epoch 143/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6728 - accuracy: 0.3455 - val_loss: 1.6670 - val_accuracy: 0.3753\n",
            "Epoch 144/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6661 - accuracy: 0.3547 - val_loss: 1.6671 - val_accuracy: 0.3836\n",
            "Epoch 145/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6750 - accuracy: 0.3532 - val_loss: 1.6651 - val_accuracy: 0.3815\n",
            "Epoch 146/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6735 - accuracy: 0.3516 - val_loss: 1.6680 - val_accuracy: 0.3825\n",
            "Epoch 147/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6683 - accuracy: 0.3557 - val_loss: 1.6641 - val_accuracy: 0.3836\n",
            "Epoch 148/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6618 - accuracy: 0.3521 - val_loss: 1.6697 - val_accuracy: 0.3763\n",
            "Epoch 149/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6577 - accuracy: 0.3567 - val_loss: 1.6638 - val_accuracy: 0.3805\n",
            "Epoch 150/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6699 - accuracy: 0.3455 - val_loss: 1.6597 - val_accuracy: 0.3857\n",
            "Epoch 151/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6632 - accuracy: 0.3455 - val_loss: 1.6610 - val_accuracy: 0.3805\n",
            "Epoch 152/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6673 - accuracy: 0.3501 - val_loss: 1.6610 - val_accuracy: 0.3825\n",
            "Epoch 153/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6737 - accuracy: 0.3506 - val_loss: 1.6630 - val_accuracy: 0.3836\n",
            "Epoch 154/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6703 - accuracy: 0.3562 - val_loss: 1.6615 - val_accuracy: 0.3857\n",
            "Epoch 155/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6623 - accuracy: 0.3496 - val_loss: 1.6602 - val_accuracy: 0.3867\n",
            "Epoch 156/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6662 - accuracy: 0.3532 - val_loss: 1.6588 - val_accuracy: 0.3805\n",
            "Epoch 157/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6602 - accuracy: 0.3557 - val_loss: 1.6599 - val_accuracy: 0.3763\n",
            "Epoch 158/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6613 - accuracy: 0.3516 - val_loss: 1.6632 - val_accuracy: 0.3836\n",
            "Epoch 159/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6569 - accuracy: 0.3429 - val_loss: 1.6598 - val_accuracy: 0.3836\n",
            "Epoch 160/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6580 - accuracy: 0.3526 - val_loss: 1.6616 - val_accuracy: 0.3857\n",
            "Epoch 161/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6614 - accuracy: 0.3573 - val_loss: 1.6598 - val_accuracy: 0.3888\n",
            "Epoch 162/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6590 - accuracy: 0.3552 - val_loss: 1.6582 - val_accuracy: 0.3867\n",
            "Epoch 163/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6645 - accuracy: 0.3598 - val_loss: 1.6534 - val_accuracy: 0.3857\n",
            "Epoch 164/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6537 - accuracy: 0.3501 - val_loss: 1.6534 - val_accuracy: 0.3825\n",
            "Epoch 165/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6556 - accuracy: 0.3660 - val_loss: 1.6542 - val_accuracy: 0.3857\n",
            "Epoch 166/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6477 - accuracy: 0.3603 - val_loss: 1.6565 - val_accuracy: 0.3825\n",
            "Epoch 167/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6551 - accuracy: 0.3573 - val_loss: 1.6562 - val_accuracy: 0.3888\n",
            "Epoch 168/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6502 - accuracy: 0.3567 - val_loss: 1.6547 - val_accuracy: 0.3836\n",
            "Epoch 169/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6528 - accuracy: 0.3542 - val_loss: 1.6566 - val_accuracy: 0.3836\n",
            "Epoch 170/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6476 - accuracy: 0.3680 - val_loss: 1.6553 - val_accuracy: 0.3825\n",
            "Epoch 171/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6561 - accuracy: 0.3608 - val_loss: 1.6542 - val_accuracy: 0.3857\n",
            "Epoch 172/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6566 - accuracy: 0.3547 - val_loss: 1.6539 - val_accuracy: 0.3857\n",
            "Epoch 173/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6554 - accuracy: 0.3537 - val_loss: 1.6572 - val_accuracy: 0.3867\n",
            "Epoch 174/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6510 - accuracy: 0.3532 - val_loss: 1.6518 - val_accuracy: 0.3825\n",
            "Epoch 175/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6463 - accuracy: 0.3608 - val_loss: 1.6552 - val_accuracy: 0.3857\n",
            "Epoch 176/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6540 - accuracy: 0.3547 - val_loss: 1.6562 - val_accuracy: 0.3877\n",
            "Epoch 177/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6585 - accuracy: 0.3485 - val_loss: 1.6587 - val_accuracy: 0.3815\n",
            "Epoch 178/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6507 - accuracy: 0.3608 - val_loss: 1.6559 - val_accuracy: 0.3857\n",
            "Epoch 179/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6521 - accuracy: 0.3655 - val_loss: 1.6523 - val_accuracy: 0.3877\n",
            "Epoch 180/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6420 - accuracy: 0.3624 - val_loss: 1.6501 - val_accuracy: 0.3898\n",
            "Epoch 181/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6450 - accuracy: 0.3552 - val_loss: 1.6471 - val_accuracy: 0.3857\n",
            "Epoch 182/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6514 - accuracy: 0.3619 - val_loss: 1.6481 - val_accuracy: 0.3836\n",
            "Epoch 183/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6437 - accuracy: 0.3624 - val_loss: 1.6477 - val_accuracy: 0.3815\n",
            "Epoch 184/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6447 - accuracy: 0.3665 - val_loss: 1.6515 - val_accuracy: 0.3825\n",
            "Epoch 185/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6452 - accuracy: 0.3634 - val_loss: 1.6505 - val_accuracy: 0.3898\n",
            "Epoch 186/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6346 - accuracy: 0.3696 - val_loss: 1.6495 - val_accuracy: 0.3867\n",
            "Epoch 187/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6425 - accuracy: 0.3634 - val_loss: 1.6505 - val_accuracy: 0.3846\n",
            "Epoch 188/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6494 - accuracy: 0.3660 - val_loss: 1.6497 - val_accuracy: 0.3805\n",
            "Epoch 189/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6431 - accuracy: 0.3660 - val_loss: 1.6540 - val_accuracy: 0.3877\n",
            "Epoch 190/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6389 - accuracy: 0.3706 - val_loss: 1.6518 - val_accuracy: 0.3753\n",
            "Epoch 191/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6406 - accuracy: 0.3680 - val_loss: 1.6501 - val_accuracy: 0.3846\n",
            "Epoch 192/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6419 - accuracy: 0.3731 - val_loss: 1.6518 - val_accuracy: 0.3846\n",
            "Epoch 193/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6333 - accuracy: 0.3603 - val_loss: 1.6457 - val_accuracy: 0.3867\n",
            "Epoch 194/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6350 - accuracy: 0.3772 - val_loss: 1.6500 - val_accuracy: 0.3815\n",
            "Epoch 195/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6423 - accuracy: 0.3614 - val_loss: 1.6463 - val_accuracy: 0.3888\n",
            "Epoch 196/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6397 - accuracy: 0.3737 - val_loss: 1.6440 - val_accuracy: 0.3846\n",
            "Epoch 197/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6317 - accuracy: 0.3747 - val_loss: 1.6458 - val_accuracy: 0.3867\n",
            "Epoch 198/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6314 - accuracy: 0.3593 - val_loss: 1.6391 - val_accuracy: 0.3867\n",
            "Epoch 199/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6453 - accuracy: 0.3567 - val_loss: 1.6377 - val_accuracy: 0.3877\n",
            "Epoch 200/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6314 - accuracy: 0.3731 - val_loss: 1.6451 - val_accuracy: 0.3877\n",
            "Epoch 201/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6328 - accuracy: 0.3608 - val_loss: 1.6459 - val_accuracy: 0.3825\n",
            "Epoch 202/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6405 - accuracy: 0.3624 - val_loss: 1.6447 - val_accuracy: 0.3898\n",
            "Epoch 203/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6387 - accuracy: 0.3655 - val_loss: 1.6402 - val_accuracy: 0.3825\n",
            "Epoch 204/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6382 - accuracy: 0.3731 - val_loss: 1.6435 - val_accuracy: 0.3867\n",
            "Epoch 205/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6282 - accuracy: 0.3706 - val_loss: 1.6381 - val_accuracy: 0.3867\n",
            "Epoch 206/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6353 - accuracy: 0.3813 - val_loss: 1.6397 - val_accuracy: 0.3919\n",
            "Epoch 207/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6271 - accuracy: 0.3731 - val_loss: 1.6402 - val_accuracy: 0.3846\n",
            "Epoch 208/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6364 - accuracy: 0.3783 - val_loss: 1.6354 - val_accuracy: 0.3857\n",
            "Epoch 209/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6251 - accuracy: 0.3665 - val_loss: 1.6369 - val_accuracy: 0.3763\n",
            "Epoch 210/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6449 - accuracy: 0.3603 - val_loss: 1.6400 - val_accuracy: 0.3846\n",
            "Epoch 211/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6361 - accuracy: 0.3680 - val_loss: 1.6340 - val_accuracy: 0.3919\n",
            "Epoch 212/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6304 - accuracy: 0.3747 - val_loss: 1.6341 - val_accuracy: 0.3888\n",
            "Epoch 213/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6374 - accuracy: 0.3562 - val_loss: 1.6350 - val_accuracy: 0.3815\n",
            "Epoch 214/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6296 - accuracy: 0.3614 - val_loss: 1.6441 - val_accuracy: 0.3846\n",
            "Epoch 215/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6282 - accuracy: 0.3778 - val_loss: 1.6388 - val_accuracy: 0.3857\n",
            "Epoch 216/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6321 - accuracy: 0.3690 - val_loss: 1.6365 - val_accuracy: 0.3857\n",
            "Epoch 217/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6256 - accuracy: 0.3737 - val_loss: 1.6394 - val_accuracy: 0.3825\n",
            "Epoch 218/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6167 - accuracy: 0.3829 - val_loss: 1.6343 - val_accuracy: 0.3794\n",
            "Epoch 219/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6277 - accuracy: 0.3660 - val_loss: 1.6374 - val_accuracy: 0.3836\n",
            "Epoch 220/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6232 - accuracy: 0.3706 - val_loss: 1.6342 - val_accuracy: 0.3836\n",
            "Epoch 221/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6270 - accuracy: 0.3696 - val_loss: 1.6417 - val_accuracy: 0.3846\n",
            "Epoch 222/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6216 - accuracy: 0.3690 - val_loss: 1.6340 - val_accuracy: 0.3877\n",
            "Epoch 223/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6306 - accuracy: 0.3737 - val_loss: 1.6389 - val_accuracy: 0.3836\n",
            "Epoch 224/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6308 - accuracy: 0.3711 - val_loss: 1.6389 - val_accuracy: 0.3846\n",
            "Epoch 225/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6192 - accuracy: 0.3762 - val_loss: 1.6302 - val_accuracy: 0.3846\n",
            "Epoch 226/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6270 - accuracy: 0.3803 - val_loss: 1.6329 - val_accuracy: 0.3888\n",
            "Epoch 227/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6234 - accuracy: 0.3752 - val_loss: 1.6311 - val_accuracy: 0.3825\n",
            "Epoch 228/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6226 - accuracy: 0.3711 - val_loss: 1.6350 - val_accuracy: 0.3898\n",
            "Epoch 229/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6230 - accuracy: 0.3885 - val_loss: 1.6274 - val_accuracy: 0.3825\n",
            "Epoch 230/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6176 - accuracy: 0.3737 - val_loss: 1.6297 - val_accuracy: 0.3919\n",
            "Epoch 231/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6231 - accuracy: 0.3762 - val_loss: 1.6302 - val_accuracy: 0.3857\n",
            "Epoch 232/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6120 - accuracy: 0.3772 - val_loss: 1.6304 - val_accuracy: 0.3857\n",
            "Epoch 233/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6147 - accuracy: 0.3726 - val_loss: 1.6278 - val_accuracy: 0.3877\n",
            "Epoch 234/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6188 - accuracy: 0.3875 - val_loss: 1.6322 - val_accuracy: 0.3909\n",
            "Epoch 235/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.6210 - accuracy: 0.3675 - val_loss: 1.6318 - val_accuracy: 0.3888\n",
            "Epoch 236/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6249 - accuracy: 0.3701 - val_loss: 1.6317 - val_accuracy: 0.3888\n",
            "Epoch 237/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6223 - accuracy: 0.3747 - val_loss: 1.6291 - val_accuracy: 0.3867\n",
            "Epoch 238/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6192 - accuracy: 0.3834 - val_loss: 1.6311 - val_accuracy: 0.3867\n",
            "Epoch 239/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6204 - accuracy: 0.3752 - val_loss: 1.6349 - val_accuracy: 0.3888\n",
            "Epoch 240/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6204 - accuracy: 0.3849 - val_loss: 1.6360 - val_accuracy: 0.3898\n",
            "Epoch 241/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6183 - accuracy: 0.3731 - val_loss: 1.6246 - val_accuracy: 0.3867\n",
            "Epoch 242/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6187 - accuracy: 0.3742 - val_loss: 1.6284 - val_accuracy: 0.3919\n",
            "Epoch 243/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6134 - accuracy: 0.3772 - val_loss: 1.6288 - val_accuracy: 0.3846\n",
            "Epoch 244/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6114 - accuracy: 0.3788 - val_loss: 1.6293 - val_accuracy: 0.3846\n",
            "Epoch 245/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6091 - accuracy: 0.3783 - val_loss: 1.6300 - val_accuracy: 0.3898\n",
            "Epoch 246/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6085 - accuracy: 0.3824 - val_loss: 1.6243 - val_accuracy: 0.3867\n",
            "Epoch 247/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6256 - accuracy: 0.3895 - val_loss: 1.6283 - val_accuracy: 0.3919\n",
            "Epoch 248/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6149 - accuracy: 0.3706 - val_loss: 1.6243 - val_accuracy: 0.3836\n",
            "Epoch 249/1100\n",
            "122/122 [==============================] - 2s 12ms/step - loss: 1.6136 - accuracy: 0.3788 - val_loss: 1.6228 - val_accuracy: 0.3888\n",
            "Epoch 250/1100\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 1.6105 - accuracy: 0.3824 - val_loss: 1.6229 - val_accuracy: 0.3898\n",
            "Epoch 251/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6055 - accuracy: 0.3844 - val_loss: 1.6267 - val_accuracy: 0.3898\n",
            "Epoch 252/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.6123 - accuracy: 0.3834 - val_loss: 1.6320 - val_accuracy: 0.3888\n",
            "Epoch 253/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6049 - accuracy: 0.3834 - val_loss: 1.6209 - val_accuracy: 0.3857\n",
            "Epoch 254/1100\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 1.6076 - accuracy: 0.3762 - val_loss: 1.6224 - val_accuracy: 0.3909\n",
            "Epoch 255/1100\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 1.6095 - accuracy: 0.3788 - val_loss: 1.6220 - val_accuracy: 0.3909\n",
            "Epoch 256/1100\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 1.6071 - accuracy: 0.3737 - val_loss: 1.6191 - val_accuracy: 0.3909\n",
            "Epoch 257/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6010 - accuracy: 0.3824 - val_loss: 1.6269 - val_accuracy: 0.3929\n",
            "Epoch 258/1100\n",
            "122/122 [==============================] - 1s 12ms/step - loss: 1.6011 - accuracy: 0.3895 - val_loss: 1.6331 - val_accuracy: 0.3836\n",
            "Epoch 259/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6164 - accuracy: 0.3747 - val_loss: 1.6295 - val_accuracy: 0.3898\n",
            "Epoch 260/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6023 - accuracy: 0.3849 - val_loss: 1.6201 - val_accuracy: 0.3794\n",
            "Epoch 261/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6075 - accuracy: 0.3844 - val_loss: 1.6295 - val_accuracy: 0.3940\n",
            "Epoch 262/1100\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 1.6055 - accuracy: 0.3834 - val_loss: 1.6216 - val_accuracy: 0.3898\n",
            "Epoch 263/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.6010 - accuracy: 0.3808 - val_loss: 1.6251 - val_accuracy: 0.3888\n",
            "Epoch 264/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6174 - accuracy: 0.3783 - val_loss: 1.6259 - val_accuracy: 0.3846\n",
            "Epoch 265/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6090 - accuracy: 0.3757 - val_loss: 1.6289 - val_accuracy: 0.3888\n",
            "Epoch 266/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6010 - accuracy: 0.3788 - val_loss: 1.6256 - val_accuracy: 0.3867\n",
            "Epoch 267/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6033 - accuracy: 0.3901 - val_loss: 1.6228 - val_accuracy: 0.3877\n",
            "Epoch 268/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5995 - accuracy: 0.3895 - val_loss: 1.6236 - val_accuracy: 0.3867\n",
            "Epoch 269/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5988 - accuracy: 0.3767 - val_loss: 1.6204 - val_accuracy: 0.3836\n",
            "Epoch 270/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6038 - accuracy: 0.3824 - val_loss: 1.6231 - val_accuracy: 0.3888\n",
            "Epoch 271/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5909 - accuracy: 0.3860 - val_loss: 1.6245 - val_accuracy: 0.3857\n",
            "Epoch 272/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6048 - accuracy: 0.3901 - val_loss: 1.6369 - val_accuracy: 0.3919\n",
            "Epoch 273/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5965 - accuracy: 0.3906 - val_loss: 1.6260 - val_accuracy: 0.3888\n",
            "Epoch 274/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6017 - accuracy: 0.3865 - val_loss: 1.6256 - val_accuracy: 0.3919\n",
            "Epoch 275/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6001 - accuracy: 0.3808 - val_loss: 1.6260 - val_accuracy: 0.3940\n",
            "Epoch 276/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5965 - accuracy: 0.3885 - val_loss: 1.6246 - val_accuracy: 0.3690\n",
            "Epoch 277/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5993 - accuracy: 0.3829 - val_loss: 1.6249 - val_accuracy: 0.3888\n",
            "Epoch 278/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5920 - accuracy: 0.3844 - val_loss: 1.6223 - val_accuracy: 0.3846\n",
            "Epoch 279/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6043 - accuracy: 0.3783 - val_loss: 1.6172 - val_accuracy: 0.3857\n",
            "Epoch 280/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5891 - accuracy: 0.3793 - val_loss: 1.6153 - val_accuracy: 0.3846\n",
            "Epoch 281/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5875 - accuracy: 0.3911 - val_loss: 1.6151 - val_accuracy: 0.3888\n",
            "Epoch 282/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5985 - accuracy: 0.3895 - val_loss: 1.6269 - val_accuracy: 0.3909\n",
            "Epoch 283/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5942 - accuracy: 0.3890 - val_loss: 1.6200 - val_accuracy: 0.3898\n",
            "Epoch 284/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.6021 - accuracy: 0.3931 - val_loss: 1.6197 - val_accuracy: 0.3877\n",
            "Epoch 285/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5873 - accuracy: 0.3916 - val_loss: 1.6141 - val_accuracy: 0.3836\n",
            "Epoch 286/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5925 - accuracy: 0.3901 - val_loss: 1.6144 - val_accuracy: 0.3877\n",
            "Epoch 287/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5923 - accuracy: 0.3952 - val_loss: 1.6137 - val_accuracy: 0.3877\n",
            "Epoch 288/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.6003 - accuracy: 0.3936 - val_loss: 1.6127 - val_accuracy: 0.3877\n",
            "Epoch 289/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5997 - accuracy: 0.3865 - val_loss: 1.6089 - val_accuracy: 0.3950\n",
            "Epoch 290/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5860 - accuracy: 0.3880 - val_loss: 1.6135 - val_accuracy: 0.3909\n",
            "Epoch 291/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5972 - accuracy: 0.3926 - val_loss: 1.6121 - val_accuracy: 0.3888\n",
            "Epoch 292/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5867 - accuracy: 0.3839 - val_loss: 1.6168 - val_accuracy: 0.3940\n",
            "Epoch 293/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5800 - accuracy: 0.3942 - val_loss: 1.6110 - val_accuracy: 0.3898\n",
            "Epoch 294/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5941 - accuracy: 0.3844 - val_loss: 1.6113 - val_accuracy: 0.3909\n",
            "Epoch 295/1100\n",
            "122/122 [==============================] - 1s 6ms/step - loss: 1.5826 - accuracy: 0.3895 - val_loss: 1.6086 - val_accuracy: 0.3877\n",
            "Epoch 296/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5823 - accuracy: 0.3967 - val_loss: 1.6075 - val_accuracy: 0.3971\n",
            "Epoch 297/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5940 - accuracy: 0.3875 - val_loss: 1.6152 - val_accuracy: 0.3960\n",
            "Epoch 298/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5816 - accuracy: 0.3921 - val_loss: 1.6078 - val_accuracy: 0.3960\n",
            "Epoch 299/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5909 - accuracy: 0.3890 - val_loss: 1.6021 - val_accuracy: 0.3929\n",
            "Epoch 300/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5916 - accuracy: 0.3885 - val_loss: 1.6034 - val_accuracy: 0.3888\n",
            "Epoch 301/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5807 - accuracy: 0.3952 - val_loss: 1.6032 - val_accuracy: 0.3846\n",
            "Epoch 302/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5984 - accuracy: 0.3880 - val_loss: 1.6024 - val_accuracy: 0.3981\n",
            "Epoch 303/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5911 - accuracy: 0.3778 - val_loss: 1.6086 - val_accuracy: 0.3960\n",
            "Epoch 304/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5927 - accuracy: 0.3731 - val_loss: 1.6112 - val_accuracy: 0.3940\n",
            "Epoch 305/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5799 - accuracy: 0.3906 - val_loss: 1.6055 - val_accuracy: 0.3929\n",
            "Epoch 306/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5797 - accuracy: 0.3952 - val_loss: 1.6071 - val_accuracy: 0.3898\n",
            "Epoch 307/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5743 - accuracy: 0.3890 - val_loss: 1.6007 - val_accuracy: 0.3846\n",
            "Epoch 308/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5851 - accuracy: 0.3942 - val_loss: 1.6030 - val_accuracy: 0.3981\n",
            "Epoch 309/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5864 - accuracy: 0.3962 - val_loss: 1.6102 - val_accuracy: 0.3815\n",
            "Epoch 310/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5873 - accuracy: 0.3834 - val_loss: 1.6035 - val_accuracy: 0.3929\n",
            "Epoch 311/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5762 - accuracy: 0.3865 - val_loss: 1.6227 - val_accuracy: 0.3877\n",
            "Epoch 312/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5840 - accuracy: 0.3844 - val_loss: 1.6034 - val_accuracy: 0.3929\n",
            "Epoch 313/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5739 - accuracy: 0.3936 - val_loss: 1.6035 - val_accuracy: 0.3960\n",
            "Epoch 314/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5843 - accuracy: 0.3767 - val_loss: 1.6047 - val_accuracy: 0.3877\n",
            "Epoch 315/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5749 - accuracy: 0.3942 - val_loss: 1.6065 - val_accuracy: 0.3877\n",
            "Epoch 316/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5818 - accuracy: 0.3972 - val_loss: 1.6037 - val_accuracy: 0.3929\n",
            "Epoch 317/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5709 - accuracy: 0.3860 - val_loss: 1.6019 - val_accuracy: 0.3950\n",
            "Epoch 318/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5727 - accuracy: 0.3983 - val_loss: 1.6117 - val_accuracy: 0.3888\n",
            "Epoch 319/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5756 - accuracy: 0.3931 - val_loss: 1.6048 - val_accuracy: 0.3919\n",
            "Epoch 320/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5687 - accuracy: 0.3911 - val_loss: 1.6073 - val_accuracy: 0.3877\n",
            "Epoch 321/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5742 - accuracy: 0.3993 - val_loss: 1.6078 - val_accuracy: 0.3960\n",
            "Epoch 322/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5830 - accuracy: 0.3854 - val_loss: 1.6030 - val_accuracy: 0.3981\n",
            "Epoch 323/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5847 - accuracy: 0.3895 - val_loss: 1.5970 - val_accuracy: 0.3877\n",
            "Epoch 324/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5803 - accuracy: 0.4008 - val_loss: 1.5990 - val_accuracy: 0.3919\n",
            "Epoch 325/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5695 - accuracy: 0.4090 - val_loss: 1.5975 - val_accuracy: 0.3919\n",
            "Epoch 326/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5674 - accuracy: 0.4003 - val_loss: 1.6022 - val_accuracy: 0.3992\n",
            "Epoch 327/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5702 - accuracy: 0.3988 - val_loss: 1.6028 - val_accuracy: 0.3877\n",
            "Epoch 328/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5725 - accuracy: 0.3947 - val_loss: 1.6024 - val_accuracy: 0.4012\n",
            "Epoch 329/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5678 - accuracy: 0.4039 - val_loss: 1.5937 - val_accuracy: 0.3981\n",
            "Epoch 330/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5756 - accuracy: 0.3860 - val_loss: 1.5990 - val_accuracy: 0.3857\n",
            "Epoch 331/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5743 - accuracy: 0.3988 - val_loss: 1.6017 - val_accuracy: 0.4002\n",
            "Epoch 332/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5671 - accuracy: 0.3895 - val_loss: 1.5966 - val_accuracy: 0.3971\n",
            "Epoch 333/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5709 - accuracy: 0.3936 - val_loss: 1.5977 - val_accuracy: 0.3919\n",
            "Epoch 334/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5578 - accuracy: 0.4100 - val_loss: 1.5912 - val_accuracy: 0.3888\n",
            "Epoch 335/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5731 - accuracy: 0.3952 - val_loss: 1.5949 - val_accuracy: 0.4002\n",
            "Epoch 336/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5678 - accuracy: 0.3957 - val_loss: 1.5923 - val_accuracy: 0.3888\n",
            "Epoch 337/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.5700 - accuracy: 0.4013 - val_loss: 1.5913 - val_accuracy: 0.3950\n",
            "Epoch 338/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5546 - accuracy: 0.3936 - val_loss: 1.5913 - val_accuracy: 0.3992\n",
            "Epoch 339/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5701 - accuracy: 0.4100 - val_loss: 1.6037 - val_accuracy: 0.3909\n",
            "Epoch 340/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5621 - accuracy: 0.4054 - val_loss: 1.6041 - val_accuracy: 0.3909\n",
            "Epoch 341/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5591 - accuracy: 0.4008 - val_loss: 1.5953 - val_accuracy: 0.3950\n",
            "Epoch 342/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5648 - accuracy: 0.3998 - val_loss: 1.5928 - val_accuracy: 0.3950\n",
            "Epoch 343/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5561 - accuracy: 0.4013 - val_loss: 1.5900 - val_accuracy: 0.3909\n",
            "Epoch 344/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5647 - accuracy: 0.3957 - val_loss: 1.5890 - val_accuracy: 0.3981\n",
            "Epoch 345/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5542 - accuracy: 0.4044 - val_loss: 1.5942 - val_accuracy: 0.4033\n",
            "Epoch 346/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5660 - accuracy: 0.4029 - val_loss: 1.6020 - val_accuracy: 0.3960\n",
            "Epoch 347/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5631 - accuracy: 0.3977 - val_loss: 1.5924 - val_accuracy: 0.3971\n",
            "Epoch 348/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5640 - accuracy: 0.3952 - val_loss: 1.5942 - val_accuracy: 0.3929\n",
            "Epoch 349/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5553 - accuracy: 0.3993 - val_loss: 1.5979 - val_accuracy: 0.4075\n",
            "Epoch 350/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5668 - accuracy: 0.3936 - val_loss: 1.5899 - val_accuracy: 0.3971\n",
            "Epoch 351/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5577 - accuracy: 0.4070 - val_loss: 1.5894 - val_accuracy: 0.4023\n",
            "Epoch 352/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5457 - accuracy: 0.4008 - val_loss: 1.5953 - val_accuracy: 0.3867\n",
            "Epoch 353/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5569 - accuracy: 0.3942 - val_loss: 1.5887 - val_accuracy: 0.4002\n",
            "Epoch 354/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5602 - accuracy: 0.3993 - val_loss: 1.5936 - val_accuracy: 0.4012\n",
            "Epoch 355/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5664 - accuracy: 0.3977 - val_loss: 1.6011 - val_accuracy: 0.3971\n",
            "Epoch 356/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5541 - accuracy: 0.4136 - val_loss: 1.5820 - val_accuracy: 0.3971\n",
            "Epoch 357/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5542 - accuracy: 0.4121 - val_loss: 1.5942 - val_accuracy: 0.4002\n",
            "Epoch 358/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5563 - accuracy: 0.3916 - val_loss: 1.5884 - val_accuracy: 0.4012\n",
            "Epoch 359/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5516 - accuracy: 0.4018 - val_loss: 1.5793 - val_accuracy: 0.4012\n",
            "Epoch 360/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5570 - accuracy: 0.4080 - val_loss: 1.5893 - val_accuracy: 0.3992\n",
            "Epoch 361/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5714 - accuracy: 0.3983 - val_loss: 1.6129 - val_accuracy: 0.3992\n",
            "Epoch 362/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5574 - accuracy: 0.4059 - val_loss: 1.5874 - val_accuracy: 0.4012\n",
            "Epoch 363/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5463 - accuracy: 0.4126 - val_loss: 1.5913 - val_accuracy: 0.3971\n",
            "Epoch 364/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5610 - accuracy: 0.4059 - val_loss: 1.5901 - val_accuracy: 0.3981\n",
            "Epoch 365/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5389 - accuracy: 0.4116 - val_loss: 1.5804 - val_accuracy: 0.4075\n",
            "Epoch 366/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5547 - accuracy: 0.4049 - val_loss: 1.5947 - val_accuracy: 0.3950\n",
            "Epoch 367/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5479 - accuracy: 0.4121 - val_loss: 1.5973 - val_accuracy: 0.3992\n",
            "Epoch 368/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5610 - accuracy: 0.4070 - val_loss: 1.6068 - val_accuracy: 0.3950\n",
            "Epoch 369/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5518 - accuracy: 0.4075 - val_loss: 1.5877 - val_accuracy: 0.4033\n",
            "Epoch 370/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5382 - accuracy: 0.4141 - val_loss: 1.5812 - val_accuracy: 0.4033\n",
            "Epoch 371/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5387 - accuracy: 0.4100 - val_loss: 1.5861 - val_accuracy: 0.3867\n",
            "Epoch 372/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5629 - accuracy: 0.3993 - val_loss: 1.5772 - val_accuracy: 0.4044\n",
            "Epoch 373/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5423 - accuracy: 0.4070 - val_loss: 1.5782 - val_accuracy: 0.3992\n",
            "Epoch 374/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5484 - accuracy: 0.4136 - val_loss: 1.5842 - val_accuracy: 0.3960\n",
            "Epoch 375/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5497 - accuracy: 0.4121 - val_loss: 1.5877 - val_accuracy: 0.3898\n",
            "Epoch 376/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5507 - accuracy: 0.4157 - val_loss: 1.5758 - val_accuracy: 0.3971\n",
            "Epoch 377/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5522 - accuracy: 0.4111 - val_loss: 1.5836 - val_accuracy: 0.3971\n",
            "Epoch 378/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5567 - accuracy: 0.4039 - val_loss: 1.5871 - val_accuracy: 0.4012\n",
            "Epoch 379/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5459 - accuracy: 0.4131 - val_loss: 1.5909 - val_accuracy: 0.3940\n",
            "Epoch 380/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5426 - accuracy: 0.4075 - val_loss: 1.5801 - val_accuracy: 0.3960\n",
            "Epoch 381/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5428 - accuracy: 0.4065 - val_loss: 1.5831 - val_accuracy: 0.4033\n",
            "Epoch 382/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5342 - accuracy: 0.4090 - val_loss: 1.5813 - val_accuracy: 0.3940\n",
            "Epoch 383/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5433 - accuracy: 0.4157 - val_loss: 1.5804 - val_accuracy: 0.3981\n",
            "Epoch 384/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5323 - accuracy: 0.4223 - val_loss: 1.5792 - val_accuracy: 0.3992\n",
            "Epoch 385/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5454 - accuracy: 0.4039 - val_loss: 1.5870 - val_accuracy: 0.4002\n",
            "Epoch 386/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5408 - accuracy: 0.4131 - val_loss: 1.5868 - val_accuracy: 0.4023\n",
            "Epoch 387/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5577 - accuracy: 0.4090 - val_loss: 1.5781 - val_accuracy: 0.4002\n",
            "Epoch 388/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5421 - accuracy: 0.4136 - val_loss: 1.5797 - val_accuracy: 0.4012\n",
            "Epoch 389/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5383 - accuracy: 0.4126 - val_loss: 1.5732 - val_accuracy: 0.4033\n",
            "Epoch 390/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5387 - accuracy: 0.4054 - val_loss: 1.5773 - val_accuracy: 0.4002\n",
            "Epoch 391/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5380 - accuracy: 0.4095 - val_loss: 1.5771 - val_accuracy: 0.4012\n",
            "Epoch 392/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5383 - accuracy: 0.4141 - val_loss: 1.5780 - val_accuracy: 0.3981\n",
            "Epoch 393/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5450 - accuracy: 0.4029 - val_loss: 1.5783 - val_accuracy: 0.3950\n",
            "Epoch 394/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5551 - accuracy: 0.4080 - val_loss: 1.5820 - val_accuracy: 0.3960\n",
            "Epoch 395/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5428 - accuracy: 0.4203 - val_loss: 1.5788 - val_accuracy: 0.4044\n",
            "Epoch 396/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5244 - accuracy: 0.4182 - val_loss: 1.5790 - val_accuracy: 0.4054\n",
            "Epoch 397/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5467 - accuracy: 0.4126 - val_loss: 1.5808 - val_accuracy: 0.4085\n",
            "Epoch 398/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5361 - accuracy: 0.4141 - val_loss: 1.5707 - val_accuracy: 0.4023\n",
            "Epoch 399/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5394 - accuracy: 0.4167 - val_loss: 1.5712 - val_accuracy: 0.4012\n",
            "Epoch 400/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5485 - accuracy: 0.4136 - val_loss: 1.5808 - val_accuracy: 0.4012\n",
            "Epoch 401/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5398 - accuracy: 0.4157 - val_loss: 1.5817 - val_accuracy: 0.4012\n",
            "Epoch 402/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5293 - accuracy: 0.4049 - val_loss: 1.5777 - val_accuracy: 0.4012\n",
            "Epoch 403/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5345 - accuracy: 0.4121 - val_loss: 1.5742 - val_accuracy: 0.4023\n",
            "Epoch 404/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5276 - accuracy: 0.4116 - val_loss: 1.5803 - val_accuracy: 0.4044\n",
            "Epoch 405/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5300 - accuracy: 0.4111 - val_loss: 1.5900 - val_accuracy: 0.3971\n",
            "Epoch 406/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5301 - accuracy: 0.4126 - val_loss: 1.5719 - val_accuracy: 0.4064\n",
            "Epoch 407/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5316 - accuracy: 0.4203 - val_loss: 1.5759 - val_accuracy: 0.3950\n",
            "Epoch 408/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5430 - accuracy: 0.4106 - val_loss: 1.5694 - val_accuracy: 0.4023\n",
            "Epoch 409/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5374 - accuracy: 0.4095 - val_loss: 1.5730 - val_accuracy: 0.4044\n",
            "Epoch 410/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5353 - accuracy: 0.4203 - val_loss: 1.5764 - val_accuracy: 0.4106\n",
            "Epoch 411/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5233 - accuracy: 0.4080 - val_loss: 1.5768 - val_accuracy: 0.3971\n",
            "Epoch 412/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5417 - accuracy: 0.4131 - val_loss: 1.5820 - val_accuracy: 0.3950\n",
            "Epoch 413/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5289 - accuracy: 0.4121 - val_loss: 1.5809 - val_accuracy: 0.4075\n",
            "Epoch 414/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5336 - accuracy: 0.4182 - val_loss: 1.5747 - val_accuracy: 0.4116\n",
            "Epoch 415/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5289 - accuracy: 0.4136 - val_loss: 1.5672 - val_accuracy: 0.4064\n",
            "Epoch 416/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5429 - accuracy: 0.4018 - val_loss: 1.5711 - val_accuracy: 0.3971\n",
            "Epoch 417/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5342 - accuracy: 0.4177 - val_loss: 1.5839 - val_accuracy: 0.3992\n",
            "Epoch 418/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5513 - accuracy: 0.4121 - val_loss: 1.5815 - val_accuracy: 0.4044\n",
            "Epoch 419/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5428 - accuracy: 0.4152 - val_loss: 1.5770 - val_accuracy: 0.4148\n",
            "Epoch 420/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5337 - accuracy: 0.4157 - val_loss: 1.5823 - val_accuracy: 0.4085\n",
            "Epoch 421/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5305 - accuracy: 0.4244 - val_loss: 1.5788 - val_accuracy: 0.4044\n",
            "Epoch 422/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5221 - accuracy: 0.4121 - val_loss: 1.5806 - val_accuracy: 0.3960\n",
            "Epoch 423/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5362 - accuracy: 0.4126 - val_loss: 1.5749 - val_accuracy: 0.4085\n",
            "Epoch 424/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5245 - accuracy: 0.4218 - val_loss: 1.5829 - val_accuracy: 0.4012\n",
            "Epoch 425/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5216 - accuracy: 0.4193 - val_loss: 1.5827 - val_accuracy: 0.4054\n",
            "Epoch 426/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5326 - accuracy: 0.4270 - val_loss: 1.5823 - val_accuracy: 0.4075\n",
            "Epoch 427/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5222 - accuracy: 0.4229 - val_loss: 1.5751 - val_accuracy: 0.4116\n",
            "Epoch 428/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5246 - accuracy: 0.4229 - val_loss: 1.5780 - val_accuracy: 0.3992\n",
            "Epoch 429/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5306 - accuracy: 0.4285 - val_loss: 1.5727 - val_accuracy: 0.4116\n",
            "Epoch 430/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5159 - accuracy: 0.4249 - val_loss: 1.5785 - val_accuracy: 0.4075\n",
            "Epoch 431/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5315 - accuracy: 0.4111 - val_loss: 1.5730 - val_accuracy: 0.4096\n",
            "Epoch 432/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5264 - accuracy: 0.4259 - val_loss: 1.5652 - val_accuracy: 0.4064\n",
            "Epoch 433/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5350 - accuracy: 0.4234 - val_loss: 1.5737 - val_accuracy: 0.4064\n",
            "Epoch 434/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5292 - accuracy: 0.4193 - val_loss: 1.5793 - val_accuracy: 0.3940\n",
            "Epoch 435/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5207 - accuracy: 0.4270 - val_loss: 1.5729 - val_accuracy: 0.4044\n",
            "Epoch 436/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5193 - accuracy: 0.4244 - val_loss: 1.5692 - val_accuracy: 0.4012\n",
            "Epoch 437/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5166 - accuracy: 0.4249 - val_loss: 1.5627 - val_accuracy: 0.3960\n",
            "Epoch 438/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5406 - accuracy: 0.4182 - val_loss: 1.5855 - val_accuracy: 0.4075\n",
            "Epoch 439/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5215 - accuracy: 0.4259 - val_loss: 1.5937 - val_accuracy: 0.4075\n",
            "Epoch 440/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5248 - accuracy: 0.4218 - val_loss: 1.5752 - val_accuracy: 0.4096\n",
            "Epoch 441/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5131 - accuracy: 0.4234 - val_loss: 1.5702 - val_accuracy: 0.4096\n",
            "Epoch 442/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5147 - accuracy: 0.4223 - val_loss: 1.5867 - val_accuracy: 0.4085\n",
            "Epoch 443/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5239 - accuracy: 0.4172 - val_loss: 1.5634 - val_accuracy: 0.4106\n",
            "Epoch 444/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5140 - accuracy: 0.4249 - val_loss: 1.5629 - val_accuracy: 0.4137\n",
            "Epoch 445/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5067 - accuracy: 0.4285 - val_loss: 1.5710 - val_accuracy: 0.4116\n",
            "Epoch 446/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5150 - accuracy: 0.4305 - val_loss: 1.5741 - val_accuracy: 0.4106\n",
            "Epoch 447/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5080 - accuracy: 0.4264 - val_loss: 1.5675 - val_accuracy: 0.4096\n",
            "Epoch 448/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5151 - accuracy: 0.4198 - val_loss: 1.5643 - val_accuracy: 0.4064\n",
            "Epoch 449/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5123 - accuracy: 0.4182 - val_loss: 1.5585 - val_accuracy: 0.4137\n",
            "Epoch 450/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5535 - accuracy: 0.4239 - val_loss: 1.5725 - val_accuracy: 0.4148\n",
            "Epoch 451/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5295 - accuracy: 0.4198 - val_loss: 1.5732 - val_accuracy: 0.4023\n",
            "Epoch 452/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5103 - accuracy: 0.4249 - val_loss: 1.5754 - val_accuracy: 0.4179\n",
            "Epoch 453/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5095 - accuracy: 0.4418 - val_loss: 1.5670 - val_accuracy: 0.4116\n",
            "Epoch 454/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5158 - accuracy: 0.4218 - val_loss: 1.5585 - val_accuracy: 0.4044\n",
            "Epoch 455/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5109 - accuracy: 0.4213 - val_loss: 1.5677 - val_accuracy: 0.4158\n",
            "Epoch 456/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5207 - accuracy: 0.4229 - val_loss: 1.5707 - val_accuracy: 0.4085\n",
            "Epoch 457/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5192 - accuracy: 0.4300 - val_loss: 1.5547 - val_accuracy: 0.4054\n",
            "Epoch 458/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5143 - accuracy: 0.4341 - val_loss: 1.5630 - val_accuracy: 0.4148\n",
            "Epoch 459/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5212 - accuracy: 0.4275 - val_loss: 1.5732 - val_accuracy: 0.4137\n",
            "Epoch 460/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5116 - accuracy: 0.4167 - val_loss: 1.5665 - val_accuracy: 0.4168\n",
            "Epoch 461/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5026 - accuracy: 0.4229 - val_loss: 1.5600 - val_accuracy: 0.4116\n",
            "Epoch 462/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5099 - accuracy: 0.4203 - val_loss: 1.5667 - val_accuracy: 0.4168\n",
            "Epoch 463/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5138 - accuracy: 0.4218 - val_loss: 1.5781 - val_accuracy: 0.4148\n",
            "Epoch 464/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5194 - accuracy: 0.4305 - val_loss: 1.5803 - val_accuracy: 0.4116\n",
            "Epoch 465/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5129 - accuracy: 0.4182 - val_loss: 1.5755 - val_accuracy: 0.4085\n",
            "Epoch 466/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5241 - accuracy: 0.4203 - val_loss: 1.5651 - val_accuracy: 0.4106\n",
            "Epoch 467/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4989 - accuracy: 0.4341 - val_loss: 1.5670 - val_accuracy: 0.4085\n",
            "Epoch 468/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5225 - accuracy: 0.4295 - val_loss: 1.5636 - val_accuracy: 0.4127\n",
            "Epoch 469/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5209 - accuracy: 0.4259 - val_loss: 1.5586 - val_accuracy: 0.4106\n",
            "Epoch 470/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5097 - accuracy: 0.4198 - val_loss: 1.5665 - val_accuracy: 0.4158\n",
            "Epoch 471/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5106 - accuracy: 0.4321 - val_loss: 1.5646 - val_accuracy: 0.4158\n",
            "Epoch 472/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5101 - accuracy: 0.4311 - val_loss: 1.5569 - val_accuracy: 0.4179\n",
            "Epoch 473/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5142 - accuracy: 0.4331 - val_loss: 1.5565 - val_accuracy: 0.4096\n",
            "Epoch 474/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4918 - accuracy: 0.4326 - val_loss: 1.5603 - val_accuracy: 0.4158\n",
            "Epoch 475/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5013 - accuracy: 0.4341 - val_loss: 1.5521 - val_accuracy: 0.4137\n",
            "Epoch 476/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5141 - accuracy: 0.4403 - val_loss: 1.5711 - val_accuracy: 0.4044\n",
            "Epoch 477/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5018 - accuracy: 0.4357 - val_loss: 1.5587 - val_accuracy: 0.4075\n",
            "Epoch 478/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5122 - accuracy: 0.4372 - val_loss: 1.5648 - val_accuracy: 0.4116\n",
            "Epoch 479/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4947 - accuracy: 0.4387 - val_loss: 1.5637 - val_accuracy: 0.4002\n",
            "Epoch 480/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5109 - accuracy: 0.4362 - val_loss: 1.5603 - val_accuracy: 0.4168\n",
            "Epoch 481/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5065 - accuracy: 0.4264 - val_loss: 1.5887 - val_accuracy: 0.4085\n",
            "Epoch 482/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4964 - accuracy: 0.4290 - val_loss: 1.5705 - val_accuracy: 0.4116\n",
            "Epoch 483/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4967 - accuracy: 0.4300 - val_loss: 1.5617 - val_accuracy: 0.4044\n",
            "Epoch 484/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4984 - accuracy: 0.4280 - val_loss: 1.5605 - val_accuracy: 0.4148\n",
            "Epoch 485/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5070 - accuracy: 0.4331 - val_loss: 1.5661 - val_accuracy: 0.4137\n",
            "Epoch 486/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5123 - accuracy: 0.4393 - val_loss: 1.5643 - val_accuracy: 0.4189\n",
            "Epoch 487/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5022 - accuracy: 0.4352 - val_loss: 1.5627 - val_accuracy: 0.4210\n",
            "Epoch 488/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5007 - accuracy: 0.4290 - val_loss: 1.5794 - val_accuracy: 0.4012\n",
            "Epoch 489/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4960 - accuracy: 0.4352 - val_loss: 1.5693 - val_accuracy: 0.4075\n",
            "Epoch 490/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4993 - accuracy: 0.4295 - val_loss: 1.5660 - val_accuracy: 0.4179\n",
            "Epoch 491/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4873 - accuracy: 0.4357 - val_loss: 1.5613 - val_accuracy: 0.4085\n",
            "Epoch 492/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4927 - accuracy: 0.4198 - val_loss: 1.5702 - val_accuracy: 0.4189\n",
            "Epoch 493/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5061 - accuracy: 0.4316 - val_loss: 1.5643 - val_accuracy: 0.4168\n",
            "Epoch 494/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4984 - accuracy: 0.4249 - val_loss: 1.5572 - val_accuracy: 0.4033\n",
            "Epoch 495/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4901 - accuracy: 0.4311 - val_loss: 1.5607 - val_accuracy: 0.4044\n",
            "Epoch 496/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4967 - accuracy: 0.4336 - val_loss: 1.5639 - val_accuracy: 0.4189\n",
            "Epoch 497/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4948 - accuracy: 0.4239 - val_loss: 1.5834 - val_accuracy: 0.4064\n",
            "Epoch 498/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5067 - accuracy: 0.4377 - val_loss: 1.5611 - val_accuracy: 0.4116\n",
            "Epoch 499/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4974 - accuracy: 0.4249 - val_loss: 1.5624 - val_accuracy: 0.4148\n",
            "Epoch 500/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5071 - accuracy: 0.4362 - val_loss: 1.5652 - val_accuracy: 0.4096\n",
            "Epoch 501/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5013 - accuracy: 0.4275 - val_loss: 1.5671 - val_accuracy: 0.4168\n",
            "Epoch 502/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4909 - accuracy: 0.4444 - val_loss: 1.5743 - val_accuracy: 0.4168\n",
            "Epoch 503/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4884 - accuracy: 0.4316 - val_loss: 1.5656 - val_accuracy: 0.4200\n",
            "Epoch 504/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4914 - accuracy: 0.4341 - val_loss: 1.5668 - val_accuracy: 0.4054\n",
            "Epoch 505/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5038 - accuracy: 0.4321 - val_loss: 1.5697 - val_accuracy: 0.4075\n",
            "Epoch 506/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4991 - accuracy: 0.4300 - val_loss: 1.5647 - val_accuracy: 0.4096\n",
            "Epoch 507/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4918 - accuracy: 0.4408 - val_loss: 1.5650 - val_accuracy: 0.4106\n",
            "Epoch 508/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4910 - accuracy: 0.4428 - val_loss: 1.5546 - val_accuracy: 0.4189\n",
            "Epoch 509/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4996 - accuracy: 0.4290 - val_loss: 1.5653 - val_accuracy: 0.4168\n",
            "Epoch 510/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4885 - accuracy: 0.4352 - val_loss: 1.5603 - val_accuracy: 0.4148\n",
            "Epoch 511/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4997 - accuracy: 0.4270 - val_loss: 1.5643 - val_accuracy: 0.4158\n",
            "Epoch 512/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4804 - accuracy: 0.4346 - val_loss: 1.5561 - val_accuracy: 0.4158\n",
            "Epoch 513/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.5023 - accuracy: 0.4444 - val_loss: 1.5683 - val_accuracy: 0.4116\n",
            "Epoch 514/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4987 - accuracy: 0.4362 - val_loss: 1.5687 - val_accuracy: 0.4179\n",
            "Epoch 515/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5109 - accuracy: 0.4372 - val_loss: 1.5588 - val_accuracy: 0.4189\n",
            "Epoch 516/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4870 - accuracy: 0.4346 - val_loss: 1.5614 - val_accuracy: 0.4189\n",
            "Epoch 517/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4930 - accuracy: 0.4362 - val_loss: 1.5552 - val_accuracy: 0.4189\n",
            "Epoch 518/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4869 - accuracy: 0.4516 - val_loss: 1.5698 - val_accuracy: 0.4137\n",
            "Epoch 519/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4859 - accuracy: 0.4352 - val_loss: 1.5669 - val_accuracy: 0.4241\n",
            "Epoch 520/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4889 - accuracy: 0.4326 - val_loss: 1.5575 - val_accuracy: 0.4210\n",
            "Epoch 521/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4705 - accuracy: 0.4259 - val_loss: 1.5532 - val_accuracy: 0.4158\n",
            "Epoch 522/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4850 - accuracy: 0.4444 - val_loss: 1.5579 - val_accuracy: 0.4210\n",
            "Epoch 523/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4864 - accuracy: 0.4413 - val_loss: 1.5403 - val_accuracy: 0.4127\n",
            "Epoch 524/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4808 - accuracy: 0.4295 - val_loss: 1.5454 - val_accuracy: 0.4189\n",
            "Epoch 525/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4901 - accuracy: 0.4439 - val_loss: 1.5660 - val_accuracy: 0.4106\n",
            "Epoch 526/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4929 - accuracy: 0.4382 - val_loss: 1.5541 - val_accuracy: 0.4158\n",
            "Epoch 527/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4918 - accuracy: 0.4413 - val_loss: 1.5639 - val_accuracy: 0.4106\n",
            "Epoch 528/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4864 - accuracy: 0.4454 - val_loss: 1.5607 - val_accuracy: 0.4231\n",
            "Epoch 529/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4900 - accuracy: 0.4444 - val_loss: 1.5628 - val_accuracy: 0.4252\n",
            "Epoch 530/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4928 - accuracy: 0.4326 - val_loss: 1.5588 - val_accuracy: 0.4241\n",
            "Epoch 531/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4944 - accuracy: 0.4346 - val_loss: 1.5615 - val_accuracy: 0.4106\n",
            "Epoch 532/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4823 - accuracy: 0.4505 - val_loss: 1.5635 - val_accuracy: 0.4148\n",
            "Epoch 533/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4957 - accuracy: 0.4362 - val_loss: 1.5594 - val_accuracy: 0.4116\n",
            "Epoch 534/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4963 - accuracy: 0.4403 - val_loss: 1.5683 - val_accuracy: 0.4200\n",
            "Epoch 535/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4815 - accuracy: 0.4377 - val_loss: 1.5590 - val_accuracy: 0.4220\n",
            "Epoch 536/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4815 - accuracy: 0.4413 - val_loss: 1.5734 - val_accuracy: 0.4200\n",
            "Epoch 537/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4831 - accuracy: 0.4341 - val_loss: 1.5542 - val_accuracy: 0.4158\n",
            "Epoch 538/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4867 - accuracy: 0.4423 - val_loss: 1.5599 - val_accuracy: 0.4168\n",
            "Epoch 539/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4800 - accuracy: 0.4449 - val_loss: 1.5551 - val_accuracy: 0.4168\n",
            "Epoch 540/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4821 - accuracy: 0.4408 - val_loss: 1.5554 - val_accuracy: 0.4293\n",
            "Epoch 541/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4845 - accuracy: 0.4352 - val_loss: 1.5580 - val_accuracy: 0.4168\n",
            "Epoch 542/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4746 - accuracy: 0.4387 - val_loss: 1.5541 - val_accuracy: 0.4158\n",
            "Epoch 543/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.5022 - accuracy: 0.4377 - val_loss: 1.5719 - val_accuracy: 0.4220\n",
            "Epoch 544/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4655 - accuracy: 0.4505 - val_loss: 1.5622 - val_accuracy: 0.4231\n",
            "Epoch 545/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4854 - accuracy: 0.4485 - val_loss: 1.5568 - val_accuracy: 0.4189\n",
            "Epoch 546/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4766 - accuracy: 0.4470 - val_loss: 1.5614 - val_accuracy: 0.4189\n",
            "Epoch 547/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4760 - accuracy: 0.4300 - val_loss: 1.5817 - val_accuracy: 0.4064\n",
            "Epoch 548/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4718 - accuracy: 0.4480 - val_loss: 1.5606 - val_accuracy: 0.4179\n",
            "Epoch 549/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4832 - accuracy: 0.4372 - val_loss: 1.5559 - val_accuracy: 0.4241\n",
            "Epoch 550/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4812 - accuracy: 0.4428 - val_loss: 1.5459 - val_accuracy: 0.4189\n",
            "Epoch 551/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4744 - accuracy: 0.4382 - val_loss: 1.5547 - val_accuracy: 0.4179\n",
            "Epoch 552/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4755 - accuracy: 0.4439 - val_loss: 1.5549 - val_accuracy: 0.4231\n",
            "Epoch 553/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4741 - accuracy: 0.4449 - val_loss: 1.5515 - val_accuracy: 0.4168\n",
            "Epoch 554/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4908 - accuracy: 0.4526 - val_loss: 1.5553 - val_accuracy: 0.4168\n",
            "Epoch 555/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4755 - accuracy: 0.4464 - val_loss: 1.5523 - val_accuracy: 0.4137\n",
            "Epoch 556/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4818 - accuracy: 0.4398 - val_loss: 1.5565 - val_accuracy: 0.4231\n",
            "Epoch 557/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4749 - accuracy: 0.4485 - val_loss: 1.5541 - val_accuracy: 0.4210\n",
            "Epoch 558/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4826 - accuracy: 0.4434 - val_loss: 1.5542 - val_accuracy: 0.4220\n",
            "Epoch 559/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4814 - accuracy: 0.4444 - val_loss: 1.5661 - val_accuracy: 0.4168\n",
            "Epoch 560/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4718 - accuracy: 0.4428 - val_loss: 1.5460 - val_accuracy: 0.4148\n",
            "Epoch 561/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4719 - accuracy: 0.4541 - val_loss: 1.5533 - val_accuracy: 0.4231\n",
            "Epoch 562/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4806 - accuracy: 0.4536 - val_loss: 1.5544 - val_accuracy: 0.4168\n",
            "Epoch 563/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4787 - accuracy: 0.4311 - val_loss: 1.5719 - val_accuracy: 0.4231\n",
            "Epoch 564/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4700 - accuracy: 0.4398 - val_loss: 1.5534 - val_accuracy: 0.4189\n",
            "Epoch 565/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4875 - accuracy: 0.4444 - val_loss: 1.5493 - val_accuracy: 0.4148\n",
            "Epoch 566/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4674 - accuracy: 0.4552 - val_loss: 1.5479 - val_accuracy: 0.4168\n",
            "Epoch 567/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4611 - accuracy: 0.4475 - val_loss: 1.5455 - val_accuracy: 0.4137\n",
            "Epoch 568/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4779 - accuracy: 0.4516 - val_loss: 1.5645 - val_accuracy: 0.4252\n",
            "Epoch 569/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4744 - accuracy: 0.4413 - val_loss: 1.5754 - val_accuracy: 0.4241\n",
            "Epoch 570/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4737 - accuracy: 0.4480 - val_loss: 1.5443 - val_accuracy: 0.4168\n",
            "Epoch 571/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4714 - accuracy: 0.4428 - val_loss: 1.5562 - val_accuracy: 0.4189\n",
            "Epoch 572/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4700 - accuracy: 0.4495 - val_loss: 1.5481 - val_accuracy: 0.4262\n",
            "Epoch 573/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4792 - accuracy: 0.4552 - val_loss: 1.5631 - val_accuracy: 0.4189\n",
            "Epoch 574/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4710 - accuracy: 0.4387 - val_loss: 1.5516 - val_accuracy: 0.4096\n",
            "Epoch 575/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4689 - accuracy: 0.4423 - val_loss: 1.5502 - val_accuracy: 0.4168\n",
            "Epoch 576/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4698 - accuracy: 0.4562 - val_loss: 1.5529 - val_accuracy: 0.4231\n",
            "Epoch 577/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4634 - accuracy: 0.4495 - val_loss: 1.5460 - val_accuracy: 0.4272\n",
            "Epoch 578/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4709 - accuracy: 0.4495 - val_loss: 1.5565 - val_accuracy: 0.4168\n",
            "Epoch 579/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4756 - accuracy: 0.4459 - val_loss: 1.5552 - val_accuracy: 0.4200\n",
            "Epoch 580/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4628 - accuracy: 0.4454 - val_loss: 1.5574 - val_accuracy: 0.4106\n",
            "Epoch 581/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4783 - accuracy: 0.4470 - val_loss: 1.5656 - val_accuracy: 0.4272\n",
            "Epoch 582/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4598 - accuracy: 0.4552 - val_loss: 1.5694 - val_accuracy: 0.4293\n",
            "Epoch 583/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4815 - accuracy: 0.4521 - val_loss: 1.5594 - val_accuracy: 0.4231\n",
            "Epoch 584/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4857 - accuracy: 0.4572 - val_loss: 1.5668 - val_accuracy: 0.4220\n",
            "Epoch 585/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4554 - accuracy: 0.4710 - val_loss: 1.5549 - val_accuracy: 0.4231\n",
            "Epoch 586/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4818 - accuracy: 0.4541 - val_loss: 1.5546 - val_accuracy: 0.4189\n",
            "Epoch 587/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4624 - accuracy: 0.4516 - val_loss: 1.5540 - val_accuracy: 0.4262\n",
            "Epoch 588/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4589 - accuracy: 0.4516 - val_loss: 1.5628 - val_accuracy: 0.4210\n",
            "Epoch 589/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4570 - accuracy: 0.4587 - val_loss: 1.5609 - val_accuracy: 0.4304\n",
            "Epoch 590/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4488 - accuracy: 0.4521 - val_loss: 1.5420 - val_accuracy: 0.4252\n",
            "Epoch 591/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4509 - accuracy: 0.4505 - val_loss: 1.5470 - val_accuracy: 0.4262\n",
            "Epoch 592/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4625 - accuracy: 0.4434 - val_loss: 1.5533 - val_accuracy: 0.4210\n",
            "Epoch 593/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4572 - accuracy: 0.4485 - val_loss: 1.5571 - val_accuracy: 0.4200\n",
            "Epoch 594/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4683 - accuracy: 0.4439 - val_loss: 1.5671 - val_accuracy: 0.4241\n",
            "Epoch 595/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4587 - accuracy: 0.4546 - val_loss: 1.5528 - val_accuracy: 0.4293\n",
            "Epoch 596/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4679 - accuracy: 0.4485 - val_loss: 1.5670 - val_accuracy: 0.4293\n",
            "Epoch 597/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4669 - accuracy: 0.4413 - val_loss: 1.5560 - val_accuracy: 0.4252\n",
            "Epoch 598/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4533 - accuracy: 0.4464 - val_loss: 1.5424 - val_accuracy: 0.4220\n",
            "Epoch 599/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4613 - accuracy: 0.4521 - val_loss: 1.5539 - val_accuracy: 0.4210\n",
            "Epoch 600/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4504 - accuracy: 0.4695 - val_loss: 1.5536 - val_accuracy: 0.4137\n",
            "Epoch 601/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4704 - accuracy: 0.4490 - val_loss: 1.5765 - val_accuracy: 0.4200\n",
            "Epoch 602/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4737 - accuracy: 0.4526 - val_loss: 1.5559 - val_accuracy: 0.4272\n",
            "Epoch 603/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4655 - accuracy: 0.4531 - val_loss: 1.5573 - val_accuracy: 0.4189\n",
            "Epoch 604/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4563 - accuracy: 0.4541 - val_loss: 1.5546 - val_accuracy: 0.4293\n",
            "Epoch 605/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4674 - accuracy: 0.4480 - val_loss: 1.5574 - val_accuracy: 0.4262\n",
            "Epoch 606/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4549 - accuracy: 0.4557 - val_loss: 1.5639 - val_accuracy: 0.4200\n",
            "Epoch 607/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4581 - accuracy: 0.4613 - val_loss: 1.5501 - val_accuracy: 0.4272\n",
            "Epoch 608/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4642 - accuracy: 0.4526 - val_loss: 1.5548 - val_accuracy: 0.4262\n",
            "Epoch 609/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4621 - accuracy: 0.4500 - val_loss: 1.5608 - val_accuracy: 0.4335\n",
            "Epoch 610/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4596 - accuracy: 0.4449 - val_loss: 1.5496 - val_accuracy: 0.4293\n",
            "Epoch 611/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4770 - accuracy: 0.4531 - val_loss: 1.5514 - val_accuracy: 0.4252\n",
            "Epoch 612/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4634 - accuracy: 0.4521 - val_loss: 1.5492 - val_accuracy: 0.4241\n",
            "Epoch 613/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4704 - accuracy: 0.4562 - val_loss: 1.5376 - val_accuracy: 0.4241\n",
            "Epoch 614/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4509 - accuracy: 0.4582 - val_loss: 1.5417 - val_accuracy: 0.4241\n",
            "Epoch 615/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4766 - accuracy: 0.4639 - val_loss: 1.5389 - val_accuracy: 0.4262\n",
            "Epoch 616/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4398 - accuracy: 0.4608 - val_loss: 1.5463 - val_accuracy: 0.4220\n",
            "Epoch 617/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4553 - accuracy: 0.4639 - val_loss: 1.5512 - val_accuracy: 0.4272\n",
            "Epoch 618/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4631 - accuracy: 0.4521 - val_loss: 1.5525 - val_accuracy: 0.4220\n",
            "Epoch 619/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4548 - accuracy: 0.4572 - val_loss: 1.5580 - val_accuracy: 0.4293\n",
            "Epoch 620/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4509 - accuracy: 0.4582 - val_loss: 1.5480 - val_accuracy: 0.4272\n",
            "Epoch 621/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4563 - accuracy: 0.4582 - val_loss: 1.5614 - val_accuracy: 0.4314\n",
            "Epoch 622/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4596 - accuracy: 0.4490 - val_loss: 1.5584 - val_accuracy: 0.4168\n",
            "Epoch 623/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4444 - accuracy: 0.4485 - val_loss: 1.5536 - val_accuracy: 0.4407\n",
            "Epoch 624/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4479 - accuracy: 0.4495 - val_loss: 1.5400 - val_accuracy: 0.4189\n",
            "Epoch 625/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4541 - accuracy: 0.4490 - val_loss: 1.5522 - val_accuracy: 0.4293\n",
            "Epoch 626/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4838 - accuracy: 0.4516 - val_loss: 1.5510 - val_accuracy: 0.4376\n",
            "Epoch 627/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4524 - accuracy: 0.4572 - val_loss: 1.5528 - val_accuracy: 0.4366\n",
            "Epoch 628/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4503 - accuracy: 0.4459 - val_loss: 1.5594 - val_accuracy: 0.4252\n",
            "Epoch 629/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4514 - accuracy: 0.4521 - val_loss: 1.5488 - val_accuracy: 0.4252\n",
            "Epoch 630/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4379 - accuracy: 0.4526 - val_loss: 1.5407 - val_accuracy: 0.4148\n",
            "Epoch 631/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4549 - accuracy: 0.4675 - val_loss: 1.5493 - val_accuracy: 0.4168\n",
            "Epoch 632/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4682 - accuracy: 0.4577 - val_loss: 1.5495 - val_accuracy: 0.4283\n",
            "Epoch 633/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4502 - accuracy: 0.4572 - val_loss: 1.5481 - val_accuracy: 0.4283\n",
            "Epoch 634/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4488 - accuracy: 0.4598 - val_loss: 1.5567 - val_accuracy: 0.4148\n",
            "Epoch 635/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4794 - accuracy: 0.4341 - val_loss: 1.5569 - val_accuracy: 0.4324\n",
            "Epoch 636/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4517 - accuracy: 0.4480 - val_loss: 1.5438 - val_accuracy: 0.4304\n",
            "Epoch 637/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4421 - accuracy: 0.4628 - val_loss: 1.5419 - val_accuracy: 0.4210\n",
            "Epoch 638/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4571 - accuracy: 0.4546 - val_loss: 1.5531 - val_accuracy: 0.4252\n",
            "Epoch 639/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4423 - accuracy: 0.4552 - val_loss: 1.5468 - val_accuracy: 0.4272\n",
            "Epoch 640/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4549 - accuracy: 0.4603 - val_loss: 1.5431 - val_accuracy: 0.4272\n",
            "Epoch 641/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4432 - accuracy: 0.4603 - val_loss: 1.5530 - val_accuracy: 0.4283\n",
            "Epoch 642/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4610 - accuracy: 0.4500 - val_loss: 1.5423 - val_accuracy: 0.4241\n",
            "Epoch 643/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4625 - accuracy: 0.4608 - val_loss: 1.5490 - val_accuracy: 0.4241\n",
            "Epoch 644/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4434 - accuracy: 0.4557 - val_loss: 1.5433 - val_accuracy: 0.4231\n",
            "Epoch 645/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4527 - accuracy: 0.4685 - val_loss: 1.5633 - val_accuracy: 0.4345\n",
            "Epoch 646/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4457 - accuracy: 0.4546 - val_loss: 1.5460 - val_accuracy: 0.4189\n",
            "Epoch 647/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4377 - accuracy: 0.4577 - val_loss: 1.5568 - val_accuracy: 0.4158\n",
            "Epoch 648/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4389 - accuracy: 0.4562 - val_loss: 1.5412 - val_accuracy: 0.4189\n",
            "Epoch 649/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4379 - accuracy: 0.4557 - val_loss: 1.5357 - val_accuracy: 0.4272\n",
            "Epoch 650/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4409 - accuracy: 0.4654 - val_loss: 1.5312 - val_accuracy: 0.4231\n",
            "Epoch 651/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4286 - accuracy: 0.4577 - val_loss: 1.5443 - val_accuracy: 0.4335\n",
            "Epoch 652/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4427 - accuracy: 0.4577 - val_loss: 1.5427 - val_accuracy: 0.4345\n",
            "Epoch 653/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4605 - accuracy: 0.4654 - val_loss: 1.5298 - val_accuracy: 0.4231\n",
            "Epoch 654/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4507 - accuracy: 0.4552 - val_loss: 1.5393 - val_accuracy: 0.4231\n",
            "Epoch 655/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4416 - accuracy: 0.4582 - val_loss: 1.5323 - val_accuracy: 0.4272\n",
            "Epoch 656/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4640 - accuracy: 0.4593 - val_loss: 1.5399 - val_accuracy: 0.4272\n",
            "Epoch 657/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4533 - accuracy: 0.4552 - val_loss: 1.5394 - val_accuracy: 0.4262\n",
            "Epoch 658/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4346 - accuracy: 0.4634 - val_loss: 1.5398 - val_accuracy: 0.4356\n",
            "Epoch 659/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4372 - accuracy: 0.4639 - val_loss: 1.5539 - val_accuracy: 0.4252\n",
            "Epoch 660/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4523 - accuracy: 0.4598 - val_loss: 1.5463 - val_accuracy: 0.4314\n",
            "Epoch 661/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4541 - accuracy: 0.4659 - val_loss: 1.5373 - val_accuracy: 0.4272\n",
            "Epoch 662/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4482 - accuracy: 0.4562 - val_loss: 1.5455 - val_accuracy: 0.4283\n",
            "Epoch 663/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4419 - accuracy: 0.4680 - val_loss: 1.5499 - val_accuracy: 0.4314\n",
            "Epoch 664/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4380 - accuracy: 0.4639 - val_loss: 1.5615 - val_accuracy: 0.4366\n",
            "Epoch 665/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4427 - accuracy: 0.4613 - val_loss: 1.5564 - val_accuracy: 0.4335\n",
            "Epoch 666/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4303 - accuracy: 0.4700 - val_loss: 1.5503 - val_accuracy: 0.4304\n",
            "Epoch 667/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4455 - accuracy: 0.4541 - val_loss: 1.5439 - val_accuracy: 0.4397\n",
            "Epoch 668/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4245 - accuracy: 0.4562 - val_loss: 1.5375 - val_accuracy: 0.4345\n",
            "Epoch 669/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4267 - accuracy: 0.4634 - val_loss: 1.5478 - val_accuracy: 0.4304\n",
            "Epoch 670/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4385 - accuracy: 0.4659 - val_loss: 1.5598 - val_accuracy: 0.4314\n",
            "Epoch 671/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4432 - accuracy: 0.4572 - val_loss: 1.5486 - val_accuracy: 0.4272\n",
            "Epoch 672/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4306 - accuracy: 0.4654 - val_loss: 1.5632 - val_accuracy: 0.4335\n",
            "Epoch 673/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4502 - accuracy: 0.4577 - val_loss: 1.5608 - val_accuracy: 0.4376\n",
            "Epoch 674/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4295 - accuracy: 0.4675 - val_loss: 1.5413 - val_accuracy: 0.4200\n",
            "Epoch 675/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4430 - accuracy: 0.4521 - val_loss: 1.5392 - val_accuracy: 0.4335\n",
            "Epoch 676/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4494 - accuracy: 0.4587 - val_loss: 1.5416 - val_accuracy: 0.4356\n",
            "Epoch 677/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4424 - accuracy: 0.4541 - val_loss: 1.5328 - val_accuracy: 0.4304\n",
            "Epoch 678/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4320 - accuracy: 0.4746 - val_loss: 1.5411 - val_accuracy: 0.4356\n",
            "Epoch 679/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4253 - accuracy: 0.4628 - val_loss: 1.5422 - val_accuracy: 0.4272\n",
            "Epoch 680/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4330 - accuracy: 0.4623 - val_loss: 1.5344 - val_accuracy: 0.4262\n",
            "Epoch 681/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4373 - accuracy: 0.4552 - val_loss: 1.5340 - val_accuracy: 0.4366\n",
            "Epoch 682/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4516 - accuracy: 0.4603 - val_loss: 1.5412 - val_accuracy: 0.4376\n",
            "Epoch 683/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4350 - accuracy: 0.4577 - val_loss: 1.5353 - val_accuracy: 0.4293\n",
            "Epoch 684/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4500 - accuracy: 0.4618 - val_loss: 1.5534 - val_accuracy: 0.4272\n",
            "Epoch 685/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4306 - accuracy: 0.4572 - val_loss: 1.5600 - val_accuracy: 0.4304\n",
            "Epoch 686/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4268 - accuracy: 0.4818 - val_loss: 1.5448 - val_accuracy: 0.4314\n",
            "Epoch 687/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.4208 - accuracy: 0.4654 - val_loss: 1.5384 - val_accuracy: 0.4262\n",
            "Epoch 688/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4306 - accuracy: 0.4649 - val_loss: 1.5261 - val_accuracy: 0.4314\n",
            "Epoch 689/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4544 - accuracy: 0.4593 - val_loss: 1.5528 - val_accuracy: 0.4304\n",
            "Epoch 690/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4269 - accuracy: 0.4762 - val_loss: 1.5539 - val_accuracy: 0.4304\n",
            "Epoch 691/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4402 - accuracy: 0.4675 - val_loss: 1.5456 - val_accuracy: 0.4241\n",
            "Epoch 692/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4289 - accuracy: 0.4644 - val_loss: 1.5475 - val_accuracy: 0.4387\n",
            "Epoch 693/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4533 - accuracy: 0.4546 - val_loss: 1.5370 - val_accuracy: 0.4356\n",
            "Epoch 694/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4331 - accuracy: 0.4623 - val_loss: 1.5441 - val_accuracy: 0.4366\n",
            "Epoch 695/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4341 - accuracy: 0.4675 - val_loss: 1.5508 - val_accuracy: 0.4324\n",
            "Epoch 696/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4268 - accuracy: 0.4654 - val_loss: 1.5384 - val_accuracy: 0.4324\n",
            "Epoch 697/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4282 - accuracy: 0.4710 - val_loss: 1.5406 - val_accuracy: 0.4366\n",
            "Epoch 698/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4422 - accuracy: 0.4623 - val_loss: 1.5561 - val_accuracy: 0.4293\n",
            "Epoch 699/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4235 - accuracy: 0.4685 - val_loss: 1.5438 - val_accuracy: 0.4335\n",
            "Epoch 700/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4204 - accuracy: 0.4680 - val_loss: 1.5459 - val_accuracy: 0.4272\n",
            "Epoch 701/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4443 - accuracy: 0.4526 - val_loss: 1.5503 - val_accuracy: 0.4376\n",
            "Epoch 702/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4352 - accuracy: 0.4736 - val_loss: 1.5467 - val_accuracy: 0.4345\n",
            "Epoch 703/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4350 - accuracy: 0.4623 - val_loss: 1.5482 - val_accuracy: 0.4304\n",
            "Epoch 704/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4482 - accuracy: 0.4664 - val_loss: 1.5617 - val_accuracy: 0.4345\n",
            "Epoch 705/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4342 - accuracy: 0.4587 - val_loss: 1.5719 - val_accuracy: 0.4293\n",
            "Epoch 706/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4172 - accuracy: 0.4705 - val_loss: 1.5444 - val_accuracy: 0.4356\n",
            "Epoch 707/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4472 - accuracy: 0.4690 - val_loss: 1.5516 - val_accuracy: 0.4272\n",
            "Epoch 708/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4243 - accuracy: 0.4649 - val_loss: 1.5318 - val_accuracy: 0.4262\n",
            "Epoch 709/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4224 - accuracy: 0.4669 - val_loss: 1.5247 - val_accuracy: 0.4272\n",
            "Epoch 710/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4322 - accuracy: 0.4736 - val_loss: 1.5488 - val_accuracy: 0.4314\n",
            "Epoch 711/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4461 - accuracy: 0.4695 - val_loss: 1.5515 - val_accuracy: 0.4345\n",
            "Epoch 712/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4349 - accuracy: 0.4675 - val_loss: 1.5412 - val_accuracy: 0.4356\n",
            "Epoch 713/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4329 - accuracy: 0.4726 - val_loss: 1.5352 - val_accuracy: 0.4293\n",
            "Epoch 714/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4154 - accuracy: 0.4813 - val_loss: 1.5339 - val_accuracy: 0.4335\n",
            "Epoch 715/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4218 - accuracy: 0.4741 - val_loss: 1.5430 - val_accuracy: 0.4304\n",
            "Epoch 716/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4356 - accuracy: 0.4675 - val_loss: 1.5362 - val_accuracy: 0.4356\n",
            "Epoch 717/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4237 - accuracy: 0.4751 - val_loss: 1.5348 - val_accuracy: 0.4314\n",
            "Epoch 718/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4195 - accuracy: 0.4705 - val_loss: 1.5356 - val_accuracy: 0.4293\n",
            "Epoch 719/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4172 - accuracy: 0.4772 - val_loss: 1.5586 - val_accuracy: 0.4283\n",
            "Epoch 720/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4185 - accuracy: 0.4762 - val_loss: 1.5376 - val_accuracy: 0.4272\n",
            "Epoch 721/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4159 - accuracy: 0.4593 - val_loss: 1.5367 - val_accuracy: 0.4304\n",
            "Epoch 722/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4162 - accuracy: 0.4731 - val_loss: 1.5248 - val_accuracy: 0.4168\n",
            "Epoch 723/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4309 - accuracy: 0.4654 - val_loss: 1.5382 - val_accuracy: 0.4345\n",
            "Epoch 724/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4185 - accuracy: 0.4700 - val_loss: 1.5268 - val_accuracy: 0.4345\n",
            "Epoch 725/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4308 - accuracy: 0.4664 - val_loss: 1.5424 - val_accuracy: 0.4324\n",
            "Epoch 726/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4261 - accuracy: 0.4623 - val_loss: 1.5374 - val_accuracy: 0.4335\n",
            "Epoch 727/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4232 - accuracy: 0.4526 - val_loss: 1.5608 - val_accuracy: 0.4345\n",
            "Epoch 728/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4092 - accuracy: 0.4798 - val_loss: 1.5481 - val_accuracy: 0.4283\n",
            "Epoch 729/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4299 - accuracy: 0.4685 - val_loss: 1.5420 - val_accuracy: 0.4376\n",
            "Epoch 730/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4224 - accuracy: 0.4731 - val_loss: 1.5595 - val_accuracy: 0.4283\n",
            "Epoch 731/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4164 - accuracy: 0.4736 - val_loss: 1.5498 - val_accuracy: 0.4439\n",
            "Epoch 732/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4278 - accuracy: 0.4710 - val_loss: 1.5479 - val_accuracy: 0.4304\n",
            "Epoch 733/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4111 - accuracy: 0.4736 - val_loss: 1.5477 - val_accuracy: 0.4335\n",
            "Epoch 734/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4222 - accuracy: 0.4690 - val_loss: 1.5587 - val_accuracy: 0.4335\n",
            "Epoch 735/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4107 - accuracy: 0.4767 - val_loss: 1.5533 - val_accuracy: 0.4293\n",
            "Epoch 736/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4233 - accuracy: 0.4746 - val_loss: 1.5413 - val_accuracy: 0.4356\n",
            "Epoch 737/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4113 - accuracy: 0.4762 - val_loss: 1.5339 - val_accuracy: 0.4366\n",
            "Epoch 738/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4071 - accuracy: 0.4808 - val_loss: 1.5256 - val_accuracy: 0.4304\n",
            "Epoch 739/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4338 - accuracy: 0.4757 - val_loss: 1.5280 - val_accuracy: 0.4345\n",
            "Epoch 740/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4437 - accuracy: 0.4731 - val_loss: 1.5415 - val_accuracy: 0.4376\n",
            "Epoch 741/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4249 - accuracy: 0.4664 - val_loss: 1.5356 - val_accuracy: 0.4335\n",
            "Epoch 742/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4286 - accuracy: 0.4587 - val_loss: 1.5282 - val_accuracy: 0.4304\n",
            "Epoch 743/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4089 - accuracy: 0.4741 - val_loss: 1.5162 - val_accuracy: 0.4335\n",
            "Epoch 744/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4237 - accuracy: 0.4669 - val_loss: 1.5208 - val_accuracy: 0.4356\n",
            "Epoch 745/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4218 - accuracy: 0.4721 - val_loss: 1.5373 - val_accuracy: 0.4293\n",
            "Epoch 746/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4187 - accuracy: 0.4762 - val_loss: 1.5348 - val_accuracy: 0.4356\n",
            "Epoch 747/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3973 - accuracy: 0.4874 - val_loss: 1.5364 - val_accuracy: 0.4449\n",
            "Epoch 748/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4135 - accuracy: 0.4710 - val_loss: 1.5314 - val_accuracy: 0.4283\n",
            "Epoch 749/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4339 - accuracy: 0.4721 - val_loss: 1.5376 - val_accuracy: 0.4366\n",
            "Epoch 750/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4292 - accuracy: 0.4685 - val_loss: 1.5359 - val_accuracy: 0.4387\n",
            "Epoch 751/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4107 - accuracy: 0.4721 - val_loss: 1.5274 - val_accuracy: 0.4345\n",
            "Epoch 752/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4272 - accuracy: 0.4639 - val_loss: 1.5292 - val_accuracy: 0.4418\n",
            "Epoch 753/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4174 - accuracy: 0.4777 - val_loss: 1.5444 - val_accuracy: 0.4304\n",
            "Epoch 754/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4064 - accuracy: 0.4803 - val_loss: 1.5204 - val_accuracy: 0.4241\n",
            "Epoch 755/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4161 - accuracy: 0.4736 - val_loss: 1.5373 - val_accuracy: 0.4387\n",
            "Epoch 756/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4180 - accuracy: 0.4680 - val_loss: 1.5156 - val_accuracy: 0.4387\n",
            "Epoch 757/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4192 - accuracy: 0.4669 - val_loss: 1.5286 - val_accuracy: 0.4397\n",
            "Epoch 758/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4157 - accuracy: 0.4792 - val_loss: 1.5265 - val_accuracy: 0.4356\n",
            "Epoch 759/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4090 - accuracy: 0.4803 - val_loss: 1.5210 - val_accuracy: 0.4335\n",
            "Epoch 760/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4067 - accuracy: 0.4854 - val_loss: 1.5170 - val_accuracy: 0.4366\n",
            "Epoch 761/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4170 - accuracy: 0.4726 - val_loss: 1.5317 - val_accuracy: 0.4356\n",
            "Epoch 762/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4052 - accuracy: 0.4731 - val_loss: 1.5350 - val_accuracy: 0.4439\n",
            "Epoch 763/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4037 - accuracy: 0.4716 - val_loss: 1.5133 - val_accuracy: 0.4324\n",
            "Epoch 764/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4036 - accuracy: 0.4741 - val_loss: 1.5180 - val_accuracy: 0.4241\n",
            "Epoch 765/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4054 - accuracy: 0.4772 - val_loss: 1.5482 - val_accuracy: 0.4387\n",
            "Epoch 766/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4113 - accuracy: 0.4654 - val_loss: 1.5286 - val_accuracy: 0.4407\n",
            "Epoch 767/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4102 - accuracy: 0.4818 - val_loss: 1.5387 - val_accuracy: 0.4283\n",
            "Epoch 768/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4166 - accuracy: 0.4792 - val_loss: 1.5329 - val_accuracy: 0.4387\n",
            "Epoch 769/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4137 - accuracy: 0.4649 - val_loss: 1.5205 - val_accuracy: 0.4324\n",
            "Epoch 770/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4052 - accuracy: 0.4823 - val_loss: 1.5184 - val_accuracy: 0.4314\n",
            "Epoch 771/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4162 - accuracy: 0.4787 - val_loss: 1.5208 - val_accuracy: 0.4480\n",
            "Epoch 772/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4102 - accuracy: 0.4844 - val_loss: 1.5264 - val_accuracy: 0.4366\n",
            "Epoch 773/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3926 - accuracy: 0.4874 - val_loss: 1.5092 - val_accuracy: 0.4407\n",
            "Epoch 774/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3992 - accuracy: 0.4885 - val_loss: 1.5223 - val_accuracy: 0.4262\n",
            "Epoch 775/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4080 - accuracy: 0.4700 - val_loss: 1.5211 - val_accuracy: 0.4439\n",
            "Epoch 776/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4076 - accuracy: 0.4803 - val_loss: 1.5303 - val_accuracy: 0.4376\n",
            "Epoch 777/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4106 - accuracy: 0.4736 - val_loss: 1.5202 - val_accuracy: 0.4366\n",
            "Epoch 778/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4002 - accuracy: 0.4839 - val_loss: 1.5290 - val_accuracy: 0.4459\n",
            "Epoch 779/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4257 - accuracy: 0.4849 - val_loss: 1.5303 - val_accuracy: 0.4356\n",
            "Epoch 780/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4180 - accuracy: 0.4772 - val_loss: 1.5392 - val_accuracy: 0.4428\n",
            "Epoch 781/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4138 - accuracy: 0.4608 - val_loss: 1.5259 - val_accuracy: 0.4304\n",
            "Epoch 782/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4010 - accuracy: 0.4844 - val_loss: 1.5310 - val_accuracy: 0.4397\n",
            "Epoch 783/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4197 - accuracy: 0.4736 - val_loss: 1.5347 - val_accuracy: 0.4366\n",
            "Epoch 784/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4085 - accuracy: 0.4716 - val_loss: 1.5211 - val_accuracy: 0.4387\n",
            "Epoch 785/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.4131 - accuracy: 0.4818 - val_loss: 1.5349 - val_accuracy: 0.4418\n",
            "Epoch 786/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4095 - accuracy: 0.4731 - val_loss: 1.5364 - val_accuracy: 0.4356\n",
            "Epoch 787/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3967 - accuracy: 0.4782 - val_loss: 1.5322 - val_accuracy: 0.4439\n",
            "Epoch 788/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4271 - accuracy: 0.4823 - val_loss: 1.5281 - val_accuracy: 0.4314\n",
            "Epoch 789/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4099 - accuracy: 0.4864 - val_loss: 1.5433 - val_accuracy: 0.4335\n",
            "Epoch 790/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4031 - accuracy: 0.4772 - val_loss: 1.5335 - val_accuracy: 0.4418\n",
            "Epoch 791/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3974 - accuracy: 0.4772 - val_loss: 1.5147 - val_accuracy: 0.4470\n",
            "Epoch 792/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4021 - accuracy: 0.4813 - val_loss: 1.5378 - val_accuracy: 0.4356\n",
            "Epoch 793/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4221 - accuracy: 0.4751 - val_loss: 1.5219 - val_accuracy: 0.4439\n",
            "Epoch 794/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4120 - accuracy: 0.4721 - val_loss: 1.5153 - val_accuracy: 0.4491\n",
            "Epoch 795/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4011 - accuracy: 0.4721 - val_loss: 1.5170 - val_accuracy: 0.4459\n",
            "Epoch 796/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4375 - accuracy: 0.4587 - val_loss: 1.5350 - val_accuracy: 0.4387\n",
            "Epoch 797/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3934 - accuracy: 0.4654 - val_loss: 1.5237 - val_accuracy: 0.4470\n",
            "Epoch 798/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4242 - accuracy: 0.4828 - val_loss: 1.5367 - val_accuracy: 0.4501\n",
            "Epoch 799/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4202 - accuracy: 0.4813 - val_loss: 1.5393 - val_accuracy: 0.4428\n",
            "Epoch 800/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4147 - accuracy: 0.4844 - val_loss: 1.5303 - val_accuracy: 0.4449\n",
            "Epoch 801/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3893 - accuracy: 0.4905 - val_loss: 1.5273 - val_accuracy: 0.4459\n",
            "Epoch 802/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4077 - accuracy: 0.4736 - val_loss: 1.5199 - val_accuracy: 0.4397\n",
            "Epoch 803/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.4470 - accuracy: 0.4654 - val_loss: 1.5174 - val_accuracy: 0.4324\n",
            "Epoch 804/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4061 - accuracy: 0.4854 - val_loss: 1.5235 - val_accuracy: 0.4376\n",
            "Epoch 805/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4030 - accuracy: 0.4880 - val_loss: 1.5388 - val_accuracy: 0.4439\n",
            "Epoch 806/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3845 - accuracy: 0.4844 - val_loss: 1.5344 - val_accuracy: 0.4345\n",
            "Epoch 807/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4023 - accuracy: 0.4946 - val_loss: 1.5219 - val_accuracy: 0.4418\n",
            "Epoch 808/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3990 - accuracy: 0.4798 - val_loss: 1.5247 - val_accuracy: 0.4314\n",
            "Epoch 809/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.4115 - accuracy: 0.4746 - val_loss: 1.5435 - val_accuracy: 0.4356\n",
            "Epoch 810/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4034 - accuracy: 0.4803 - val_loss: 1.5338 - val_accuracy: 0.4366\n",
            "Epoch 811/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3900 - accuracy: 0.4926 - val_loss: 1.5429 - val_accuracy: 0.4449\n",
            "Epoch 812/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3905 - accuracy: 0.4895 - val_loss: 1.5337 - val_accuracy: 0.4418\n",
            "Epoch 813/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.4063 - accuracy: 0.4731 - val_loss: 1.5015 - val_accuracy: 0.4324\n",
            "Epoch 814/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4116 - accuracy: 0.4823 - val_loss: 1.5179 - val_accuracy: 0.4418\n",
            "Epoch 815/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4039 - accuracy: 0.4803 - val_loss: 1.5116 - val_accuracy: 0.4397\n",
            "Epoch 816/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3908 - accuracy: 0.4828 - val_loss: 1.5214 - val_accuracy: 0.4470\n",
            "Epoch 817/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3962 - accuracy: 0.4859 - val_loss: 1.5408 - val_accuracy: 0.4459\n",
            "Epoch 818/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4142 - accuracy: 0.4818 - val_loss: 1.5250 - val_accuracy: 0.4439\n",
            "Epoch 819/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4007 - accuracy: 0.4818 - val_loss: 1.5341 - val_accuracy: 0.4428\n",
            "Epoch 820/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3970 - accuracy: 0.4833 - val_loss: 1.5231 - val_accuracy: 0.4428\n",
            "Epoch 821/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3809 - accuracy: 0.4869 - val_loss: 1.5154 - val_accuracy: 0.4376\n",
            "Epoch 822/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3916 - accuracy: 0.4864 - val_loss: 1.5270 - val_accuracy: 0.4459\n",
            "Epoch 823/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3909 - accuracy: 0.4798 - val_loss: 1.5330 - val_accuracy: 0.4418\n",
            "Epoch 824/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3969 - accuracy: 0.4716 - val_loss: 1.5335 - val_accuracy: 0.4335\n",
            "Epoch 825/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3750 - accuracy: 0.4767 - val_loss: 1.5303 - val_accuracy: 0.4387\n",
            "Epoch 826/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4036 - accuracy: 0.4828 - val_loss: 1.5195 - val_accuracy: 0.4324\n",
            "Epoch 827/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4194 - accuracy: 0.4833 - val_loss: 1.5289 - val_accuracy: 0.4376\n",
            "Epoch 828/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3905 - accuracy: 0.4798 - val_loss: 1.5304 - val_accuracy: 0.4459\n",
            "Epoch 829/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3932 - accuracy: 0.4900 - val_loss: 1.5256 - val_accuracy: 0.4356\n",
            "Epoch 830/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4027 - accuracy: 0.4833 - val_loss: 1.5211 - val_accuracy: 0.4407\n",
            "Epoch 831/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3902 - accuracy: 0.4915 - val_loss: 1.5314 - val_accuracy: 0.4418\n",
            "Epoch 832/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3920 - accuracy: 0.4844 - val_loss: 1.5220 - val_accuracy: 0.4439\n",
            "Epoch 833/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3937 - accuracy: 0.4833 - val_loss: 1.5101 - val_accuracy: 0.4397\n",
            "Epoch 834/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4240 - accuracy: 0.4905 - val_loss: 1.5124 - val_accuracy: 0.4439\n",
            "Epoch 835/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3977 - accuracy: 0.4828 - val_loss: 1.5409 - val_accuracy: 0.4418\n",
            "Epoch 836/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3807 - accuracy: 0.4803 - val_loss: 1.5173 - val_accuracy: 0.4428\n",
            "Epoch 837/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3996 - accuracy: 0.4874 - val_loss: 1.5130 - val_accuracy: 0.4356\n",
            "Epoch 838/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3752 - accuracy: 0.4915 - val_loss: 1.5327 - val_accuracy: 0.4356\n",
            "Epoch 839/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3872 - accuracy: 0.4874 - val_loss: 1.5283 - val_accuracy: 0.4553\n",
            "Epoch 840/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3848 - accuracy: 0.4844 - val_loss: 1.5173 - val_accuracy: 0.4376\n",
            "Epoch 841/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3901 - accuracy: 0.4818 - val_loss: 1.5136 - val_accuracy: 0.4376\n",
            "Epoch 842/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3983 - accuracy: 0.4828 - val_loss: 1.5304 - val_accuracy: 0.4522\n",
            "Epoch 843/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3817 - accuracy: 0.4905 - val_loss: 1.5158 - val_accuracy: 0.4470\n",
            "Epoch 844/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3978 - accuracy: 0.4700 - val_loss: 1.5247 - val_accuracy: 0.4407\n",
            "Epoch 845/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4149 - accuracy: 0.4905 - val_loss: 1.5193 - val_accuracy: 0.4407\n",
            "Epoch 846/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3950 - accuracy: 0.4833 - val_loss: 1.5310 - val_accuracy: 0.4428\n",
            "Epoch 847/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3824 - accuracy: 0.4931 - val_loss: 1.5221 - val_accuracy: 0.4407\n",
            "Epoch 848/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3904 - accuracy: 0.4849 - val_loss: 1.5331 - val_accuracy: 0.4397\n",
            "Epoch 849/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4061 - accuracy: 0.4915 - val_loss: 1.5224 - val_accuracy: 0.4439\n",
            "Epoch 850/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3853 - accuracy: 0.4874 - val_loss: 1.5153 - val_accuracy: 0.4376\n",
            "Epoch 851/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3730 - accuracy: 0.4880 - val_loss: 1.5110 - val_accuracy: 0.4439\n",
            "Epoch 852/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4013 - accuracy: 0.4787 - val_loss: 1.5224 - val_accuracy: 0.4397\n",
            "Epoch 853/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3917 - accuracy: 0.4849 - val_loss: 1.5315 - val_accuracy: 0.4480\n",
            "Epoch 854/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3925 - accuracy: 0.4833 - val_loss: 1.5112 - val_accuracy: 0.4387\n",
            "Epoch 855/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4080 - accuracy: 0.4721 - val_loss: 1.5091 - val_accuracy: 0.4376\n",
            "Epoch 856/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3796 - accuracy: 0.4818 - val_loss: 1.5228 - val_accuracy: 0.4511\n",
            "Epoch 857/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3901 - accuracy: 0.4880 - val_loss: 1.5147 - val_accuracy: 0.4553\n",
            "Epoch 858/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3877 - accuracy: 0.4885 - val_loss: 1.5267 - val_accuracy: 0.4449\n",
            "Epoch 859/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3970 - accuracy: 0.4890 - val_loss: 1.5334 - val_accuracy: 0.4439\n",
            "Epoch 860/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3930 - accuracy: 0.4982 - val_loss: 1.5100 - val_accuracy: 0.4397\n",
            "Epoch 861/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4026 - accuracy: 0.4751 - val_loss: 1.5287 - val_accuracy: 0.4511\n",
            "Epoch 862/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3959 - accuracy: 0.4813 - val_loss: 1.5175 - val_accuracy: 0.4459\n",
            "Epoch 863/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3954 - accuracy: 0.4777 - val_loss: 1.5130 - val_accuracy: 0.4345\n",
            "Epoch 864/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3963 - accuracy: 0.4839 - val_loss: 1.5327 - val_accuracy: 0.4387\n",
            "Epoch 865/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3854 - accuracy: 0.4910 - val_loss: 1.5113 - val_accuracy: 0.4397\n",
            "Epoch 866/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3857 - accuracy: 0.4921 - val_loss: 1.5118 - val_accuracy: 0.4439\n",
            "Epoch 867/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3868 - accuracy: 0.4926 - val_loss: 1.5015 - val_accuracy: 0.4439\n",
            "Epoch 868/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3969 - accuracy: 0.4962 - val_loss: 1.5085 - val_accuracy: 0.4449\n",
            "Epoch 869/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3955 - accuracy: 0.4885 - val_loss: 1.5208 - val_accuracy: 0.4501\n",
            "Epoch 870/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3773 - accuracy: 0.4910 - val_loss: 1.5028 - val_accuracy: 0.4397\n",
            "Epoch 871/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3885 - accuracy: 0.4972 - val_loss: 1.5135 - val_accuracy: 0.4428\n",
            "Epoch 872/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3833 - accuracy: 0.4849 - val_loss: 1.4992 - val_accuracy: 0.4387\n",
            "Epoch 873/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3773 - accuracy: 0.4859 - val_loss: 1.4956 - val_accuracy: 0.4459\n",
            "Epoch 874/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3984 - accuracy: 0.4915 - val_loss: 1.4914 - val_accuracy: 0.4491\n",
            "Epoch 875/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3908 - accuracy: 0.4890 - val_loss: 1.4936 - val_accuracy: 0.4428\n",
            "Epoch 876/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3711 - accuracy: 0.4946 - val_loss: 1.5063 - val_accuracy: 0.4459\n",
            "Epoch 877/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3826 - accuracy: 0.4854 - val_loss: 1.5106 - val_accuracy: 0.4449\n",
            "Epoch 878/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3895 - accuracy: 0.4941 - val_loss: 1.5055 - val_accuracy: 0.4480\n",
            "Epoch 879/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3854 - accuracy: 0.4869 - val_loss: 1.5095 - val_accuracy: 0.4407\n",
            "Epoch 880/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3657 - accuracy: 0.4910 - val_loss: 1.5150 - val_accuracy: 0.4376\n",
            "Epoch 881/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3741 - accuracy: 0.4900 - val_loss: 1.5280 - val_accuracy: 0.4439\n",
            "Epoch 882/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3868 - accuracy: 0.4721 - val_loss: 1.5056 - val_accuracy: 0.4376\n",
            "Epoch 883/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3978 - accuracy: 0.4792 - val_loss: 1.5279 - val_accuracy: 0.4397\n",
            "Epoch 884/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3790 - accuracy: 0.4818 - val_loss: 1.5156 - val_accuracy: 0.4449\n",
            "Epoch 885/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3648 - accuracy: 0.4890 - val_loss: 1.5136 - val_accuracy: 0.4439\n",
            "Epoch 886/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3726 - accuracy: 0.4849 - val_loss: 1.5082 - val_accuracy: 0.4439\n",
            "Epoch 887/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3926 - accuracy: 0.4874 - val_loss: 1.5165 - val_accuracy: 0.4532\n",
            "Epoch 888/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3954 - accuracy: 0.4890 - val_loss: 1.5182 - val_accuracy: 0.4459\n",
            "Epoch 889/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3618 - accuracy: 0.4972 - val_loss: 1.5267 - val_accuracy: 0.4407\n",
            "Epoch 890/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3776 - accuracy: 0.4972 - val_loss: 1.5200 - val_accuracy: 0.4397\n",
            "Epoch 891/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3610 - accuracy: 0.4977 - val_loss: 1.5329 - val_accuracy: 0.4439\n",
            "Epoch 892/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3783 - accuracy: 0.4931 - val_loss: 1.5083 - val_accuracy: 0.4439\n",
            "Epoch 893/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3755 - accuracy: 0.4885 - val_loss: 1.5060 - val_accuracy: 0.4459\n",
            "Epoch 894/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3956 - accuracy: 0.4828 - val_loss: 1.5168 - val_accuracy: 0.4418\n",
            "Epoch 895/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3820 - accuracy: 0.4782 - val_loss: 1.5188 - val_accuracy: 0.4449\n",
            "Epoch 896/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3762 - accuracy: 0.5003 - val_loss: 1.5117 - val_accuracy: 0.4449\n",
            "Epoch 897/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3713 - accuracy: 0.4946 - val_loss: 1.5204 - val_accuracy: 0.4397\n",
            "Epoch 898/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3711 - accuracy: 0.4941 - val_loss: 1.5242 - val_accuracy: 0.4470\n",
            "Epoch 899/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3843 - accuracy: 0.4946 - val_loss: 1.5322 - val_accuracy: 0.4501\n",
            "Epoch 900/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4036 - accuracy: 0.4844 - val_loss: 1.5204 - val_accuracy: 0.4459\n",
            "Epoch 901/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3728 - accuracy: 0.4941 - val_loss: 1.5192 - val_accuracy: 0.4428\n",
            "Epoch 902/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3545 - accuracy: 0.5018 - val_loss: 1.5086 - val_accuracy: 0.4522\n",
            "Epoch 903/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3712 - accuracy: 0.4992 - val_loss: 1.4990 - val_accuracy: 0.4387\n",
            "Epoch 904/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3704 - accuracy: 0.4977 - val_loss: 1.5111 - val_accuracy: 0.4397\n",
            "Epoch 905/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3806 - accuracy: 0.4982 - val_loss: 1.4985 - val_accuracy: 0.4511\n",
            "Epoch 906/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3853 - accuracy: 0.4890 - val_loss: 1.5013 - val_accuracy: 0.4407\n",
            "Epoch 907/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3693 - accuracy: 0.4992 - val_loss: 1.5092 - val_accuracy: 0.4470\n",
            "Epoch 908/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3615 - accuracy: 0.4997 - val_loss: 1.5096 - val_accuracy: 0.4522\n",
            "Epoch 909/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3871 - accuracy: 0.4833 - val_loss: 1.5083 - val_accuracy: 0.4449\n",
            "Epoch 910/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3885 - accuracy: 0.4762 - val_loss: 1.5110 - val_accuracy: 0.4491\n",
            "Epoch 911/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4200 - accuracy: 0.4818 - val_loss: 1.5292 - val_accuracy: 0.4439\n",
            "Epoch 912/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3706 - accuracy: 0.4962 - val_loss: 1.5054 - val_accuracy: 0.4501\n",
            "Epoch 913/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3581 - accuracy: 0.4900 - val_loss: 1.5152 - val_accuracy: 0.4480\n",
            "Epoch 914/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3757 - accuracy: 0.4895 - val_loss: 1.5003 - val_accuracy: 0.4470\n",
            "Epoch 915/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3700 - accuracy: 0.4895 - val_loss: 1.5054 - val_accuracy: 0.4511\n",
            "Epoch 916/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3668 - accuracy: 0.4880 - val_loss: 1.5034 - val_accuracy: 0.4491\n",
            "Epoch 917/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3701 - accuracy: 0.4828 - val_loss: 1.5059 - val_accuracy: 0.4553\n",
            "Epoch 918/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3901 - accuracy: 0.4905 - val_loss: 1.5214 - val_accuracy: 0.4397\n",
            "Epoch 919/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3757 - accuracy: 0.4926 - val_loss: 1.5146 - val_accuracy: 0.4553\n",
            "Epoch 920/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4029 - accuracy: 0.5003 - val_loss: 1.5141 - val_accuracy: 0.4459\n",
            "Epoch 921/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3744 - accuracy: 0.4895 - val_loss: 1.5145 - val_accuracy: 0.4418\n",
            "Epoch 922/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4084 - accuracy: 0.4890 - val_loss: 1.5260 - val_accuracy: 0.4459\n",
            "Epoch 923/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3648 - accuracy: 0.4951 - val_loss: 1.5148 - val_accuracy: 0.4449\n",
            "Epoch 924/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3730 - accuracy: 0.4905 - val_loss: 1.5095 - val_accuracy: 0.4407\n",
            "Epoch 925/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3783 - accuracy: 0.4962 - val_loss: 1.5058 - val_accuracy: 0.4459\n",
            "Epoch 926/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3897 - accuracy: 0.4951 - val_loss: 1.5097 - val_accuracy: 0.4449\n",
            "Epoch 927/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3521 - accuracy: 0.4926 - val_loss: 1.4954 - val_accuracy: 0.4491\n",
            "Epoch 928/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3783 - accuracy: 0.4905 - val_loss: 1.4991 - val_accuracy: 0.4501\n",
            "Epoch 929/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3947 - accuracy: 0.4900 - val_loss: 1.5025 - val_accuracy: 0.4459\n",
            "Epoch 930/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3769 - accuracy: 0.4885 - val_loss: 1.5046 - val_accuracy: 0.4511\n",
            "Epoch 931/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3909 - accuracy: 0.4895 - val_loss: 1.5163 - val_accuracy: 0.4470\n",
            "Epoch 932/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3745 - accuracy: 0.4849 - val_loss: 1.5158 - val_accuracy: 0.4387\n",
            "Epoch 933/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3815 - accuracy: 0.4946 - val_loss: 1.5183 - val_accuracy: 0.4532\n",
            "Epoch 934/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3798 - accuracy: 0.4962 - val_loss: 1.5050 - val_accuracy: 0.4532\n",
            "Epoch 935/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3632 - accuracy: 0.4900 - val_loss: 1.5002 - val_accuracy: 0.4532\n",
            "Epoch 936/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3970 - accuracy: 0.4880 - val_loss: 1.5139 - val_accuracy: 0.4480\n",
            "Epoch 937/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3711 - accuracy: 0.4839 - val_loss: 1.5012 - val_accuracy: 0.4501\n",
            "Epoch 938/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3948 - accuracy: 0.4885 - val_loss: 1.5027 - val_accuracy: 0.4480\n",
            "Epoch 939/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3966 - accuracy: 0.4828 - val_loss: 1.5031 - val_accuracy: 0.4407\n",
            "Epoch 940/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3716 - accuracy: 0.5013 - val_loss: 1.5093 - val_accuracy: 0.4522\n",
            "Epoch 941/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3667 - accuracy: 0.4992 - val_loss: 1.5069 - val_accuracy: 0.4491\n",
            "Epoch 942/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3727 - accuracy: 0.4962 - val_loss: 1.5192 - val_accuracy: 0.4532\n",
            "Epoch 943/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3649 - accuracy: 0.4828 - val_loss: 1.5065 - val_accuracy: 0.4470\n",
            "Epoch 944/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3636 - accuracy: 0.4977 - val_loss: 1.5146 - val_accuracy: 0.4543\n",
            "Epoch 945/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3631 - accuracy: 0.5003 - val_loss: 1.5092 - val_accuracy: 0.4563\n",
            "Epoch 946/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3554 - accuracy: 0.4936 - val_loss: 1.5212 - val_accuracy: 0.4480\n",
            "Epoch 947/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3604 - accuracy: 0.4828 - val_loss: 1.5022 - val_accuracy: 0.4511\n",
            "Epoch 948/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3629 - accuracy: 0.4941 - val_loss: 1.5099 - val_accuracy: 0.4449\n",
            "Epoch 949/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3499 - accuracy: 0.5028 - val_loss: 1.4954 - val_accuracy: 0.4470\n",
            "Epoch 950/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3667 - accuracy: 0.4900 - val_loss: 1.5237 - val_accuracy: 0.4553\n",
            "Epoch 951/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3693 - accuracy: 0.4992 - val_loss: 1.5276 - val_accuracy: 0.4480\n",
            "Epoch 952/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3560 - accuracy: 0.4972 - val_loss: 1.5185 - val_accuracy: 0.4532\n",
            "Epoch 953/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3476 - accuracy: 0.4880 - val_loss: 1.5017 - val_accuracy: 0.4574\n",
            "Epoch 954/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3888 - accuracy: 0.4798 - val_loss: 1.5051 - val_accuracy: 0.4439\n",
            "Epoch 955/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3737 - accuracy: 0.4921 - val_loss: 1.5085 - val_accuracy: 0.4480\n",
            "Epoch 956/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3753 - accuracy: 0.4962 - val_loss: 1.5311 - val_accuracy: 0.4376\n",
            "Epoch 957/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3761 - accuracy: 0.4941 - val_loss: 1.5102 - val_accuracy: 0.4532\n",
            "Epoch 958/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3509 - accuracy: 0.5003 - val_loss: 1.5097 - val_accuracy: 0.4522\n",
            "Epoch 959/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3853 - accuracy: 0.4941 - val_loss: 1.4905 - val_accuracy: 0.4532\n",
            "Epoch 960/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3463 - accuracy: 0.5059 - val_loss: 1.5017 - val_accuracy: 0.4532\n",
            "Epoch 961/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3752 - accuracy: 0.4921 - val_loss: 1.5084 - val_accuracy: 0.4522\n",
            "Epoch 962/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3768 - accuracy: 0.4977 - val_loss: 1.5136 - val_accuracy: 0.4418\n",
            "Epoch 963/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3424 - accuracy: 0.4946 - val_loss: 1.5001 - val_accuracy: 0.4563\n",
            "Epoch 964/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3647 - accuracy: 0.5069 - val_loss: 1.5171 - val_accuracy: 0.4522\n",
            "Epoch 965/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3910 - accuracy: 0.4910 - val_loss: 1.5237 - val_accuracy: 0.4366\n",
            "Epoch 966/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3603 - accuracy: 0.5008 - val_loss: 1.5118 - val_accuracy: 0.4366\n",
            "Epoch 967/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3751 - accuracy: 0.4977 - val_loss: 1.5299 - val_accuracy: 0.4491\n",
            "Epoch 968/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3563 - accuracy: 0.4987 - val_loss: 1.5158 - val_accuracy: 0.4595\n",
            "Epoch 969/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3392 - accuracy: 0.5028 - val_loss: 1.4878 - val_accuracy: 0.4532\n",
            "Epoch 970/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3648 - accuracy: 0.4982 - val_loss: 1.4938 - val_accuracy: 0.4574\n",
            "Epoch 971/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3474 - accuracy: 0.5044 - val_loss: 1.5026 - val_accuracy: 0.4522\n",
            "Epoch 972/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3827 - accuracy: 0.4951 - val_loss: 1.4955 - val_accuracy: 0.4522\n",
            "Epoch 973/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3700 - accuracy: 0.4951 - val_loss: 1.4879 - val_accuracy: 0.4428\n",
            "Epoch 974/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3665 - accuracy: 0.4905 - val_loss: 1.4950 - val_accuracy: 0.4574\n",
            "Epoch 975/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3642 - accuracy: 0.4951 - val_loss: 1.4979 - val_accuracy: 0.4532\n",
            "Epoch 976/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3545 - accuracy: 0.4936 - val_loss: 1.5122 - val_accuracy: 0.4501\n",
            "Epoch 977/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3483 - accuracy: 0.4992 - val_loss: 1.4999 - val_accuracy: 0.4543\n",
            "Epoch 978/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.4079 - accuracy: 0.5003 - val_loss: 1.5034 - val_accuracy: 0.4584\n",
            "Epoch 979/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3990 - accuracy: 0.4890 - val_loss: 1.5100 - val_accuracy: 0.4449\n",
            "Epoch 980/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3366 - accuracy: 0.5095 - val_loss: 1.4962 - val_accuracy: 0.4563\n",
            "Epoch 981/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3431 - accuracy: 0.4915 - val_loss: 1.5097 - val_accuracy: 0.4563\n",
            "Epoch 982/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3482 - accuracy: 0.4987 - val_loss: 1.5075 - val_accuracy: 0.4511\n",
            "Epoch 983/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3486 - accuracy: 0.4900 - val_loss: 1.5182 - val_accuracy: 0.4501\n",
            "Epoch 984/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3459 - accuracy: 0.4915 - val_loss: 1.4970 - val_accuracy: 0.4449\n",
            "Epoch 985/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3941 - accuracy: 0.4900 - val_loss: 1.5172 - val_accuracy: 0.4480\n",
            "Epoch 986/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3414 - accuracy: 0.4956 - val_loss: 1.4916 - val_accuracy: 0.4491\n",
            "Epoch 987/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3507 - accuracy: 0.5003 - val_loss: 1.4834 - val_accuracy: 0.4480\n",
            "Epoch 988/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3738 - accuracy: 0.4839 - val_loss: 1.4840 - val_accuracy: 0.4574\n",
            "Epoch 989/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3616 - accuracy: 0.5003 - val_loss: 1.4797 - val_accuracy: 0.4605\n",
            "Epoch 990/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3743 - accuracy: 0.4951 - val_loss: 1.4914 - val_accuracy: 0.4584\n",
            "Epoch 991/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3655 - accuracy: 0.4905 - val_loss: 1.4983 - val_accuracy: 0.4511\n",
            "Epoch 992/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3630 - accuracy: 0.4962 - val_loss: 1.5180 - val_accuracy: 0.4574\n",
            "Epoch 993/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3566 - accuracy: 0.5028 - val_loss: 1.4995 - val_accuracy: 0.4522\n",
            "Epoch 994/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3480 - accuracy: 0.4941 - val_loss: 1.4952 - val_accuracy: 0.4459\n",
            "Epoch 995/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3624 - accuracy: 0.4977 - val_loss: 1.4836 - val_accuracy: 0.4532\n",
            "Epoch 996/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3994 - accuracy: 0.4905 - val_loss: 1.4992 - val_accuracy: 0.4501\n",
            "Epoch 997/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3450 - accuracy: 0.4992 - val_loss: 1.5100 - val_accuracy: 0.4407\n",
            "Epoch 998/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3507 - accuracy: 0.5028 - val_loss: 1.5181 - val_accuracy: 0.4543\n",
            "Epoch 999/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3637 - accuracy: 0.5008 - val_loss: 1.5398 - val_accuracy: 0.4553\n",
            "Epoch 1000/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3615 - accuracy: 0.4895 - val_loss: 1.5150 - val_accuracy: 0.4605\n",
            "Epoch 1001/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3850 - accuracy: 0.5074 - val_loss: 1.4963 - val_accuracy: 0.4522\n",
            "Epoch 1002/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3392 - accuracy: 0.5085 - val_loss: 1.5063 - val_accuracy: 0.4511\n",
            "Epoch 1003/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3429 - accuracy: 0.4885 - val_loss: 1.5078 - val_accuracy: 0.4543\n",
            "Epoch 1004/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3781 - accuracy: 0.4967 - val_loss: 1.5065 - val_accuracy: 0.4543\n",
            "Epoch 1005/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3869 - accuracy: 0.4967 - val_loss: 1.4995 - val_accuracy: 0.4543\n",
            "Epoch 1006/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3538 - accuracy: 0.5044 - val_loss: 1.4902 - val_accuracy: 0.4605\n",
            "Epoch 1007/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3645 - accuracy: 0.4977 - val_loss: 1.4827 - val_accuracy: 0.4584\n",
            "Epoch 1008/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3555 - accuracy: 0.4956 - val_loss: 1.5077 - val_accuracy: 0.4428\n",
            "Epoch 1009/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3552 - accuracy: 0.5008 - val_loss: 1.4914 - val_accuracy: 0.4532\n",
            "Epoch 1010/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3369 - accuracy: 0.5136 - val_loss: 1.4999 - val_accuracy: 0.4511\n",
            "Epoch 1011/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3812 - accuracy: 0.4936 - val_loss: 1.4985 - val_accuracy: 0.4574\n",
            "Epoch 1012/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3417 - accuracy: 0.5033 - val_loss: 1.5208 - val_accuracy: 0.4491\n",
            "Epoch 1013/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3722 - accuracy: 0.4972 - val_loss: 1.5170 - val_accuracy: 0.4511\n",
            "Epoch 1014/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3365 - accuracy: 0.4987 - val_loss: 1.4878 - val_accuracy: 0.4470\n",
            "Epoch 1015/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3365 - accuracy: 0.5069 - val_loss: 1.5025 - val_accuracy: 0.4532\n",
            "Epoch 1016/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3593 - accuracy: 0.5090 - val_loss: 1.5128 - val_accuracy: 0.4480\n",
            "Epoch 1017/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3645 - accuracy: 0.5054 - val_loss: 1.4945 - val_accuracy: 0.4511\n",
            "Epoch 1018/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3569 - accuracy: 0.4977 - val_loss: 1.5040 - val_accuracy: 0.4543\n",
            "Epoch 1019/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3927 - accuracy: 0.5115 - val_loss: 1.4864 - val_accuracy: 0.4522\n",
            "Epoch 1020/1100\n",
            "122/122 [==============================] - 1s 7ms/step - loss: 1.3484 - accuracy: 0.4931 - val_loss: 1.4909 - val_accuracy: 0.4543\n",
            "Epoch 1021/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3356 - accuracy: 0.4982 - val_loss: 1.5039 - val_accuracy: 0.4563\n",
            "Epoch 1022/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3450 - accuracy: 0.4951 - val_loss: 1.4997 - val_accuracy: 0.4636\n",
            "Epoch 1023/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3461 - accuracy: 0.5018 - val_loss: 1.5280 - val_accuracy: 0.4501\n",
            "Epoch 1024/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3598 - accuracy: 0.5049 - val_loss: 1.5055 - val_accuracy: 0.4543\n",
            "Epoch 1025/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3515 - accuracy: 0.5054 - val_loss: 1.4971 - val_accuracy: 0.4501\n",
            "Epoch 1026/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3514 - accuracy: 0.5003 - val_loss: 1.4914 - val_accuracy: 0.4501\n",
            "Epoch 1027/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3588 - accuracy: 0.5059 - val_loss: 1.5098 - val_accuracy: 0.4522\n",
            "Epoch 1028/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3656 - accuracy: 0.5013 - val_loss: 1.5107 - val_accuracy: 0.4584\n",
            "Epoch 1029/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3463 - accuracy: 0.4992 - val_loss: 1.4940 - val_accuracy: 0.4511\n",
            "Epoch 1030/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3547 - accuracy: 0.4890 - val_loss: 1.5189 - val_accuracy: 0.4501\n",
            "Epoch 1031/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3630 - accuracy: 0.4987 - val_loss: 1.4866 - val_accuracy: 0.4522\n",
            "Epoch 1032/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3372 - accuracy: 0.5028 - val_loss: 1.5082 - val_accuracy: 0.4491\n",
            "Epoch 1033/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3535 - accuracy: 0.4941 - val_loss: 1.4912 - val_accuracy: 0.4459\n",
            "Epoch 1034/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3528 - accuracy: 0.4977 - val_loss: 1.5036 - val_accuracy: 0.4501\n",
            "Epoch 1035/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3429 - accuracy: 0.5023 - val_loss: 1.5058 - val_accuracy: 0.4563\n",
            "Epoch 1036/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3275 - accuracy: 0.4997 - val_loss: 1.4960 - val_accuracy: 0.4480\n",
            "Epoch 1037/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3367 - accuracy: 0.4951 - val_loss: 1.5196 - val_accuracy: 0.4501\n",
            "Epoch 1038/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3326 - accuracy: 0.4992 - val_loss: 1.4985 - val_accuracy: 0.4532\n",
            "Epoch 1039/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3709 - accuracy: 0.5023 - val_loss: 1.5120 - val_accuracy: 0.4439\n",
            "Epoch 1040/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3425 - accuracy: 0.4946 - val_loss: 1.5000 - val_accuracy: 0.4553\n",
            "Epoch 1041/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3256 - accuracy: 0.5044 - val_loss: 1.4943 - val_accuracy: 0.4501\n",
            "Epoch 1042/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3466 - accuracy: 0.5023 - val_loss: 1.4952 - val_accuracy: 0.4501\n",
            "Epoch 1043/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3730 - accuracy: 0.4915 - val_loss: 1.5040 - val_accuracy: 0.4563\n",
            "Epoch 1044/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3506 - accuracy: 0.5023 - val_loss: 1.5153 - val_accuracy: 0.4491\n",
            "Epoch 1045/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3573 - accuracy: 0.5028 - val_loss: 1.5058 - val_accuracy: 0.4553\n",
            "Epoch 1046/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3353 - accuracy: 0.5008 - val_loss: 1.5023 - val_accuracy: 0.4522\n",
            "Epoch 1047/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3630 - accuracy: 0.4987 - val_loss: 1.5134 - val_accuracy: 0.4543\n",
            "Epoch 1048/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3428 - accuracy: 0.4997 - val_loss: 1.5095 - val_accuracy: 0.4522\n",
            "Epoch 1049/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3420 - accuracy: 0.5008 - val_loss: 1.5110 - val_accuracy: 0.4439\n",
            "Epoch 1050/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3548 - accuracy: 0.5013 - val_loss: 1.5047 - val_accuracy: 0.4491\n",
            "Epoch 1051/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3478 - accuracy: 0.5095 - val_loss: 1.5115 - val_accuracy: 0.4532\n",
            "Epoch 1052/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3473 - accuracy: 0.5038 - val_loss: 1.4972 - val_accuracy: 0.4470\n",
            "Epoch 1053/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3406 - accuracy: 0.5090 - val_loss: 1.4842 - val_accuracy: 0.4459\n",
            "Epoch 1054/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3848 - accuracy: 0.4972 - val_loss: 1.4950 - val_accuracy: 0.4543\n",
            "Epoch 1055/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3509 - accuracy: 0.4890 - val_loss: 1.4939 - val_accuracy: 0.4595\n",
            "Epoch 1056/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3268 - accuracy: 0.5023 - val_loss: 1.4893 - val_accuracy: 0.4480\n",
            "Epoch 1057/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3382 - accuracy: 0.5003 - val_loss: 1.4937 - val_accuracy: 0.4501\n",
            "Epoch 1058/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3209 - accuracy: 0.5161 - val_loss: 1.4776 - val_accuracy: 0.4511\n",
            "Epoch 1059/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3416 - accuracy: 0.4962 - val_loss: 1.4999 - val_accuracy: 0.4543\n",
            "Epoch 1060/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3191 - accuracy: 0.5100 - val_loss: 1.4959 - val_accuracy: 0.4605\n",
            "Epoch 1061/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3284 - accuracy: 0.5064 - val_loss: 1.4972 - val_accuracy: 0.4709\n",
            "Epoch 1062/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3602 - accuracy: 0.5172 - val_loss: 1.5074 - val_accuracy: 0.4449\n",
            "Epoch 1063/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3612 - accuracy: 0.4941 - val_loss: 1.5031 - val_accuracy: 0.4522\n",
            "Epoch 1064/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3165 - accuracy: 0.4982 - val_loss: 1.4858 - val_accuracy: 0.4563\n",
            "Epoch 1065/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3452 - accuracy: 0.5085 - val_loss: 1.4768 - val_accuracy: 0.4532\n",
            "Epoch 1066/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3438 - accuracy: 0.5069 - val_loss: 1.4847 - val_accuracy: 0.4595\n",
            "Epoch 1067/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3352 - accuracy: 0.4854 - val_loss: 1.5072 - val_accuracy: 0.4574\n",
            "Epoch 1068/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3173 - accuracy: 0.5095 - val_loss: 1.5137 - val_accuracy: 0.4626\n",
            "Epoch 1069/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3253 - accuracy: 0.5085 - val_loss: 1.5119 - val_accuracy: 0.4553\n",
            "Epoch 1070/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3394 - accuracy: 0.5100 - val_loss: 1.5099 - val_accuracy: 0.4605\n",
            "Epoch 1071/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3386 - accuracy: 0.5069 - val_loss: 1.5068 - val_accuracy: 0.4615\n",
            "Epoch 1072/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3312 - accuracy: 0.5105 - val_loss: 1.4895 - val_accuracy: 0.4595\n",
            "Epoch 1073/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3339 - accuracy: 0.5105 - val_loss: 1.5064 - val_accuracy: 0.4574\n",
            "Epoch 1074/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3396 - accuracy: 0.5161 - val_loss: 1.4937 - val_accuracy: 0.4647\n",
            "Epoch 1075/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3323 - accuracy: 0.5054 - val_loss: 1.5151 - val_accuracy: 0.4543\n",
            "Epoch 1076/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3687 - accuracy: 0.4962 - val_loss: 1.4907 - val_accuracy: 0.4595\n",
            "Epoch 1077/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3338 - accuracy: 0.5059 - val_loss: 1.4896 - val_accuracy: 0.4595\n",
            "Epoch 1078/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3556 - accuracy: 0.5033 - val_loss: 1.5157 - val_accuracy: 0.4563\n",
            "Epoch 1079/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3287 - accuracy: 0.5085 - val_loss: 1.5090 - val_accuracy: 0.4563\n",
            "Epoch 1080/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3367 - accuracy: 0.5074 - val_loss: 1.5278 - val_accuracy: 0.4553\n",
            "Epoch 1081/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3259 - accuracy: 0.5131 - val_loss: 1.5436 - val_accuracy: 0.4563\n",
            "Epoch 1082/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3377 - accuracy: 0.5115 - val_loss: 1.4977 - val_accuracy: 0.4615\n",
            "Epoch 1083/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3285 - accuracy: 0.4956 - val_loss: 1.5006 - val_accuracy: 0.4532\n",
            "Epoch 1084/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3140 - accuracy: 0.5069 - val_loss: 1.4829 - val_accuracy: 0.4574\n",
            "Epoch 1085/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3256 - accuracy: 0.5136 - val_loss: 1.4946 - val_accuracy: 0.4543\n",
            "Epoch 1086/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3554 - accuracy: 0.5054 - val_loss: 1.4940 - val_accuracy: 0.4543\n",
            "Epoch 1087/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3331 - accuracy: 0.5018 - val_loss: 1.4810 - val_accuracy: 0.4563\n",
            "Epoch 1088/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3523 - accuracy: 0.4900 - val_loss: 1.5008 - val_accuracy: 0.4667\n",
            "Epoch 1089/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3173 - accuracy: 0.5044 - val_loss: 1.4829 - val_accuracy: 0.4511\n",
            "Epoch 1090/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3219 - accuracy: 0.5115 - val_loss: 1.5177 - val_accuracy: 0.4501\n",
            "Epoch 1091/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3556 - accuracy: 0.4967 - val_loss: 1.4819 - val_accuracy: 0.4563\n",
            "Epoch 1092/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3462 - accuracy: 0.5038 - val_loss: 1.5010 - val_accuracy: 0.4522\n",
            "Epoch 1093/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3545 - accuracy: 0.4956 - val_loss: 1.5031 - val_accuracy: 0.4636\n",
            "Epoch 1094/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3308 - accuracy: 0.5064 - val_loss: 1.5038 - val_accuracy: 0.4605\n",
            "Epoch 1095/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3389 - accuracy: 0.4951 - val_loss: 1.5152 - val_accuracy: 0.4543\n",
            "Epoch 1096/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3355 - accuracy: 0.5023 - val_loss: 1.5053 - val_accuracy: 0.4563\n",
            "Epoch 1097/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3206 - accuracy: 0.5115 - val_loss: 1.4955 - val_accuracy: 0.4615\n",
            "Epoch 1098/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3353 - accuracy: 0.5079 - val_loss: 1.5046 - val_accuracy: 0.4626\n",
            "Epoch 1099/1100\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3630 - accuracy: 0.5028 - val_loss: 1.4881 - val_accuracy: 0.4532\n",
            "Epoch 1100/1100\n",
            "122/122 [==============================] - 1s 8ms/step - loss: 1.3233 - accuracy: 0.5079 - val_loss: 1.4765 - val_accuracy: 0.4553\n"
          ]
        }
      ],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1100, validation_data=(x_testcnn, y_test)) \n",
        "#Turns out I was supposed to make the labels integers and then have those integers correspond to a string label. \n",
        "#With this I have 0 to 13 for my integers and then an array of size 13 with string values.\n",
        "#aaround 400 80%acc, 1000 -> 96% acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "e9BiGdCNrKUu",
        "outputId": "a02a848e-fa46-4fa9-d52b-32a01de8d733"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZEAN-V0rTbx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "76e84a9d-a131-447e-8be6-b45ee05ec168"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#accuracy plot\n",
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoCKhqWFrx3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728aae33-cecf-46a6-b774-fd918c6c4128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ]
        }
      ],
      "source": [
        "#Save model\n",
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPU34-Tnrz2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b3f275-bdc0-4565-f789-ec9c38524c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 5, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 5, 70)             44870     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 5, 70)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 70)             0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 350)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                3510      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,148\n",
            "Trainable params: 49,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#reload\n",
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGoCSPWor3mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c1cc88-34a4-4df1-8db7-0f2560e83b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.07      0.02      0.03        56\n",
            "           2       0.42      0.73      0.53       130\n",
            "           3       0.51      0.43      0.47       137\n",
            "           4       0.31      0.26      0.29       121\n",
            "           5       0.76      0.66      0.70       125\n",
            "           6       0.48      0.48      0.48       120\n",
            "           7       0.32      0.27      0.29       136\n",
            "           8       0.47      0.54      0.50       137\n",
            "\n",
            "    accuracy                           0.46       962\n",
            "   macro avg       0.42      0.42      0.41       962\n",
            "weighted avg       0.44      0.46      0.44       962\n",
            "\n",
            "[[ 1 22  8 13  0  2  9  1]\n",
            " [ 2 95  3 21  0  0  9  0]\n",
            " [ 0  5 59  9  6 22 16 20]\n",
            " [ 3 51  8 32  0  9  4 14]\n",
            " [ 0  4 11  0 82  5 11 12]\n",
            " [ 1 18  9  7  8 58  9 10]\n",
            " [ 3 25  7 14 10 13 37 27]\n",
            " [ 4  8 11  6  2 12 20 74]]\n"
          ]
        }
      ],
      "source": [
        "#check accuracy\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "pred = model.predict(x_testcnn) \n",
        "pred = np.argmax(pred, axis = 1)\n",
        "y_test = np.array([int(x) for x in y_test])\n",
        "\n",
        "\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test, pred))\n",
        "#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With melspectograms the same CNN model doesn't seem to perform better, the first CNN seem to be the best model by looking at the final results, also the random forest seem a robust classifier and doesn't require so much traning, meanwhile the CNN required cross-validation to solve the overfitting problem.\n",
        "The decision tree might also be considered the best one, with an 80% accuracy and little computation/training behind it, might be one of the best if the model needs to be constantly trained, but also the performance of the tree might not increase much, meanwhile for the CNN with more data and more hyperparameters tuning the model might outperform by far the others, all these considerations must be taken into account when we need to use our models in an application or in a real-world context.\n",
        "For example for an app on mobile if I keep needing to update the model and use it with little computation, than the decision tree should be the best for this puropose; if I will not need to train in again and just use it than the CNN wuold be the best, it all depends on the contexts"
      ],
      "metadata": {
        "id": "7xu-nkZQJEmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mfccs' CNN got little overfitting at first, to try to fix this I added some dropout and tried to decrease the number of epochs for the model to try to learn only the datas needed for generalization"
      ],
      "metadata": {
        "id": "D3Q2vp01Ne4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameters tuning was performed only with the random forest approach, but could be also be performed with the CNN to try to find a better combination of hyperparameters. The scope was to demonstrate a possible approach to try to gain better performances"
      ],
      "metadata": {
        "id": "sUevkviCQk5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also the different models performed differently on the different classes, even if the overall accuracy of the calssifiers seems to be near 80, some models got better results with the class 'neutral' and others with the class angry and so on, so if I'll need to use a model that recoginze only angry people I might want to use a specific classifier over another that performs poorly on angry datas but better overall."
      ],
      "metadata": {
        "id": "cdcwXx6VWcv9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}